<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Make Python Faster: a Practical Guide to Accelerating your Code</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="introduction.html"><strong aria-hidden="true">1.</strong> üìñ Introduction and ToC</a></li><li class="chapter-item "><a href="when_to_optimise.html"><strong aria-hidden="true">2.</strong> ‚ùì When to Optimise</a></li><li class="chapter-item "><a href="pythons_execution_model.html"><strong aria-hidden="true">3.</strong> üêç Python's Execution Model</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="the_global_interpreter_lock.html"><strong aria-hidden="true">3.1.</strong> üîí The Global Interpreter Lock</a></li></ol></li><li class="chapter-item "><a href="alternative_python_interpreters.html"><strong aria-hidden="true">4.</strong> üöÄ Alternative Python Interpreters</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="alternative_python_interpreters/pypy.html"><strong aria-hidden="true">4.1.</strong> üèéÔ∏è PyPy</a></li><li class="chapter-item "><a href="alternative_python_interpreters/ironpython.html"><strong aria-hidden="true">4.2.</strong> üîó IronPython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/jython.html"><strong aria-hidden="true">4.3.</strong> ‚òï Jython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/graalpython.html"><strong aria-hidden="true">4.4.</strong> üåå GraalPython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/cinder.html"><strong aria-hidden="true">4.5.</strong> üî• Cinder</a></li><li class="chapter-item "><a href="alternative_python_interpreters/micropython.html"><strong aria-hidden="true">4.6.</strong> ü§ñ MicroPython</a></li></ol></li><li class="chapter-item "><a href="profiling_tools.html"><strong aria-hidden="true">5.</strong> üîé Profiling Tools</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="profiling_tools/time_time.html"><strong aria-hidden="true">5.1.</strong> üïí time.time()</a></li><li class="chapter-item "><a href="profiling_tools/timeit.html"><strong aria-hidden="true">5.2.</strong> ‚è±Ô∏è timeit</a></li><li class="chapter-item "><a href="profiling_tools/unixs_time_command.html"><strong aria-hidden="true">5.3.</strong> ‚è≤Ô∏è Unix's time Command</a></li><li class="chapter-item "><a href="profiling_tools/cprofile.html"><strong aria-hidden="true">5.4.</strong> üìä cProfile</a></li><li class="chapter-item "><a href="profiling_tools/snakeviz.html"><strong aria-hidden="true">5.5.</strong> üê≤ Snakeviz</a></li><li class="chapter-item "><a href="profiling_tools/line_profiler.html"><strong aria-hidden="true">5.6.</strong> üìà line_profiler</a></li><li class="chapter-item "><a href="profiling_tools/memory_profiler.html"><strong aria-hidden="true">5.7.</strong> üß† memory_profiler</a></li><li class="chapter-item "><a href="profiling_tools/py_spy.html"><strong aria-hidden="true">5.8.</strong> üïµÔ∏è‚Äç‚ôÇÔ∏è Py-Spy</a></li></ol></li><li class="chapter-item "><a href="built_in_data_structures.html"><strong aria-hidden="true">6.</strong> üèóÔ∏è Built-in Data Structures</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="built_in_data_structures/lists.html"><strong aria-hidden="true">6.1.</strong> üìù Lists</a></li><li class="chapter-item "><a href="built_in_data_structures/tuples.html"><strong aria-hidden="true">6.2.</strong> üéÅ Tuples</a></li><li class="chapter-item "><a href="built_in_data_structures/dictionaries.html"><strong aria-hidden="true">6.3.</strong> üóùÔ∏è Dictionaries</a></li><li class="chapter-item "><a href="built_in_data_structures/sets.html"><strong aria-hidden="true">6.4.</strong> üéØ Sets</a></li><li class="chapter-item "><a href="built_in_data_structures/general_tips.html"><strong aria-hidden="true">6.5.</strong> üí° General Tips</a></li></ol></li><li class="chapter-item "><a href="the_collections_module.html"><strong aria-hidden="true">7.</strong> The Collections Module</a></li><li class="chapter-item "><a href="algorithm_complexity.html"><strong aria-hidden="true">8.</strong> Algorithm Complexity</a></li><li class="chapter-item "><a href="optimising_common_algorithms.html"><strong aria-hidden="true">9.</strong> Optimising Common Algorithms</a></li><li class="chapter-item "><a href="numpy_and_pandas.html"><strong aria-hidden="true">10.</strong> NumPy and Pandas</a></li><li class="chapter-item "><a href="efficient_numerical_computations.html"><strong aria-hidden="true">11.</strong> Efficient Numerical Computations</a></li><li class="chapter-item "><a href="optimising_data_manipulation.html"><strong aria-hidden="true">12.</strong> Optimising Data Manipulation</a></li><li class="chapter-item "><a href="dask_and_polars.html"><strong aria-hidden="true">13.</strong> Dask and Polars</a></li><li class="chapter-item "><a href="threading_and_multiprocessing.html"><strong aria-hidden="true">14.</strong> Threading and Multiprocessing</a></li><li class="chapter-item "><a href="asynchronous_programming.html"><strong aria-hidden="true">15.</strong> Asynchronous Programming</a></li><li class="chapter-item "><a href="concurrent_futures.html"><strong aria-hidden="true">16.</strong> concurrent.futures</a></li><li class="chapter-item "><a href="c_extensions_for_python.html"><strong aria-hidden="true">17.</strong> C Extensions for Python</a></li><li class="chapter-item "><a href="compiling_with_cython.html"><strong aria-hidden="true">18.</strong> Compiling with Cython</a></li><li class="chapter-item "><a href="just_in_time_compilation.html"><strong aria-hidden="true">19.</strong> Just-in-Time Compilation</a></li><li class="chapter-item "><a href="using_numba.html"><strong aria-hidden="true">20.</strong> Using Numba</a></li><li class="chapter-item "><a href="memory_management.html"><strong aria-hidden="true">21.</strong> Memory Management</a></li><li class="chapter-item "><a href="reducing_footprint_and_avoiding_leaks.html"><strong aria-hidden="true">22.</strong> Reducing Footprint and Avoiding Leaks</a></li><li class="chapter-item "><a href="performant_web_applications.html"><strong aria-hidden="true">23.</strong> Performant Web Applications</a></li><li class="chapter-item "><a href="caching_strategies.html"><strong aria-hidden="true">24.</strong> Caching Strategies</a></li><li class="chapter-item "><a href="general_best_practices.html"><strong aria-hidden="true">25.</strong> General Best Practices</a></li><li class="chapter-item "><a href="common_performance_anti_patterns.html"><strong aria-hidden="true">26.</strong> Common Performance Anti-Patterns</a></li><li class="chapter-item "><a href="profiling.html"><strong aria-hidden="true">27.</strong> üîé Profiling</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="profiling/the_simplest_method.html"><strong aria-hidden="true">27.1.</strong> time.time() + print</a></li><li class="chapter-item "><a href="profiling/unix_time_command.html"><strong aria-hidden="true">27.2.</strong> The Unix time Command</a></li><li class="chapter-item "><a href="profiling/the_timeit_module.html"><strong aria-hidden="true">27.3.</strong> The timeit Module</a></li><li class="chapter-item "><a href="profiling/function_calls_with_cprofile.html"><strong aria-hidden="true">27.4.</strong> Splitting Function Calls with cProfile</a></li><li class="chapter-item "><a href="profiling/adding_decorator_function.html"><strong aria-hidden="true">27.5.</strong> Adding a Decorator Function</a></li><li class="chapter-item "><a href="profiling/getting_granular_with_line_profiler.html"><strong aria-hidden="true">27.6.</strong> Getting Granular with line-profiler</a></li><li class="chapter-item "><a href="profiling/profiling_memory_usage.html"><strong aria-hidden="true">27.7.</strong> Profiling Memory Usage</a></li><li class="chapter-item "><a href="profiling/profiling_on_the_fly.html"><strong aria-hidden="true">27.8.</strong> Profiling on the Fly with Py-Spy</a></li></ol></li><li class="chapter-item "><a href="data_structures_and_algorithms.html"><strong aria-hidden="true">28.</strong> üèõÔ∏è Data Structures and Algorithms</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="data_structures_and_algorithms/arrays_lists_vs_tuples.html"><strong aria-hidden="true">28.1.</strong> Arrays (Lists vs Tuples)</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/sets_and_dictionaries.html"><strong aria-hidden="true">28.2.</strong> Ready: Sets, Dictionaries</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/generator_comprehension_iteration_and_evaluation.html"><strong aria-hidden="true">28.3.</strong> Let's Get Generating</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/sensible_loop_design.html"><strong aria-hidden="true">28.4.</strong> Sensible Loop Design</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/numpy_the_solution_to_lists.html"><strong aria-hidden="true">28.5.</strong> Numpy: the Solution to Lists</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/accelerating_pandas_with_cpus_and_gpus.html"><strong aria-hidden="true">28.6.</strong> Accelerating Pandas with CPUs and GPUs</a></li><li class="chapter-item "><a href="data_structures_and_algorithms/dask_and_polars.html"><strong aria-hidden="true">28.7.</strong> Dask and Polars</a></li></ol></li><li class="chapter-item "><a href="async_methods.html"><strong aria-hidden="true">29.</strong> ‚è≥ Async Methods</a></li><li class="chapter-item "><a href="multiprocessing.html"><strong aria-hidden="true">30.</strong> üè≠ Multiprocessing</a></li><li class="chapter-item "><a href="compiling_python.html"><strong aria-hidden="true">31.</strong> ‚öôÔ∏è Compiling Python</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="compiling/cython_pure_c_based_compiling.html"><strong aria-hidden="true">31.1.</strong> Cython: Pure C-based Compiling</a></li><li class="chapter-item "><a href="compiling/numba_llvm_based_compiling.html"><strong aria-hidden="true">31.2.</strong> Numba: LLVM-based Compiling</a></li><li class="chapter-item "><a href="compiling/pypy_replacement_virtual_machine.html"><strong aria-hidden="true">31.3.</strong> PyPy: Replacement Virtual Machine</a></li><li class="chapter-item "><a href="compiling/foreign_function_interfaces.html"><strong aria-hidden="true">31.4.</strong> Foreign Function Interfaces</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Make Python Faster: a Practical Guide to Accelerating your Code</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="-introduction"><a class="header" href="#-introduction">üìñ Introduction</a></h1>
<p>These are some notes and code examples about profiling Python; using sensible data structures and algorithms; compiling to machine code; using async methods; and multiprocessing.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents:</a></h2>
<ul>
<li><a href="./when_to_optimise.html">‚ùì When to Optimise</a></li>
<li><a href="./profiling.html">üîé Profiling</a></li>
<li><a href="./data_structures_and_types.html">üèõÔ∏è Data Structures and Algorithms</a></li>
<li><a href="./async_methods.html">‚è≥ Async Methods</a></li>
<li><a href="./multiprocessing.html">üè≠ Multiprocessing in Python</a></li>
<li><a href="./compiling_python.html">‚öôÔ∏è Compiling Python</a></li>
</ul>
<p>Writing a book about making Python faster is a great idea, as performance optimization is crucial in many Python applications, ranging from data analysis to web development. Here are several important topics you might consider covering:</p>
<p>Understanding Python's Execution Model:
Overview of the Python interpreter (CPython) and its execution model.
The Global Interpreter Lock (GIL) and its implications for multi-threaded applications.
Alternative Python interpreters that might offer performance benefits (e.g., PyPy, Jython).</p>
<p>Profiling and Benchmarking:
Techniques and tools for profiling Python code to identify bottlenecks (e.g., cProfile, line_profiler).
Benchmarking best practices to ensure accurate performance measurements.</p>
<p>Efficient Use of Data Structures:</p>
<p>Understanding built-in data structures (lists, tuples, dictionaries, sets) and their performance characteristics.
When and how to use collections from the collections module for efficiency.
Algorithm Optimization:
Algorithm complexity and its impact on performance.
Examples of optimizing common algorithms and data processing patterns.
Using NumPy and Pandas for Efficient Data Processing:
Leveraging NumPy for efficient numerical computations.
Optimizing data manipulation and analysis with Pandas.
Concurrency and Parallelism:
Threading and multiprocessing in Python: when and how to use them.
AsyncIO for asynchronous programming: concepts, event loops, and practical applications.
Using concurrent.futures for easy concurrency and parallelism.
C Extensions and Cython:
Writing C extensions for Python for critical performance paths.
Using Cython to compile Python to C for performance gains.
Just-In-Time Compilation (JIT):
An introduction to JIT compilation and how it can improve Python performance.
Practical examples using Numba for JIT compilation in Python.
Memory Management and Optimization:
Understanding Python's memory management: reference counting and garbage collection.
Techniques for reducing memory footprint and avoiding memory leaks.
Performance Tips for Web Applications:
Optimizing Django or Flask applications for better performance.
Caching strategies and when to use them.
Tools and Libraries for Performance Enhancement:
Overview of libraries and tools specifically designed to improve Python performance (e.g., PyPy, NumPy, Numba).
Best Practices and Patterns:
General best practices for writing efficient Python code.
Common performance anti-patterns and how to avoid them.
Each chapter could include real-world examples, case studies, and practical tips that readers can apply to their own Python projects. Additionally, emphasizing the importance of measuring performance improvements and the trade-offs between readability, maintainability, and speed could provide valuable insights for Python developers of all levels.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-when-to-optimise-your-code"><a class="header" href="#-when-to-optimise-your-code">‚ùì When to Optimise your Code</a></h1>
<p>In most organisations, programmers live under the paradigm: <code>overall team velocity &gt; individual code optimisation</code></p>
<p>Some of the tips in this book are general best practice (e.g. avoid pointless iterations in your loops); others require more of a time investment. So before considering any implementation of the latter, ask yourself the following three questions;</p>
<ol>
<li><strong>Does the code achieve its objectives?</strong></li>
</ol>
<p>Can you build a prototype to demonstrate proof-of-concept? Is the code useful? Does it do what it's meant to do? If not, figure this bit out first!</p>
<ol start="2">
<li><strong>Is the code robust?</strong></li>
</ol>
<p>Have you documented what you're doing? Does your code conform to organisational standards? Can other developers easily build on top of it?</p>
<ol start="3">
<li><strong>Is it worth further development?</strong></li>
</ol>
<p>Is this a mission-critical piece of code? Or is it only being run occasionally? Remember the famous quote from Tony Hoare/Donald Knuth: <code>Premature optimisation is the root of all evil</code></p>
<p>We code to make the world more efficient. But consult this xkcd graphic if in doubt:</p>
<p><img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png" alt="A matrix about time saved vs time invested" /></p>
<p>Some problems just aren't worth worrying about!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-pythons-execution-model"><a class="header" href="#-pythons-execution-model">üêç Python's Execution Model</a></h1>
<p>Before we start considering how to make our code faster, it's worth recapping how Python programs typically work.</p>
<p>CPython is the default and most widely used implementation of the Python programming language. It is written in C and Python, providing a foundation for executing Python programs. CPython compiles Python code into bytecode, which is then executed by the Python virtual machine. The execution model of CPython involves several key components and processes, including parsing, compilation, and interpretation.</p>
<h2 id="parsing"><a class="header" href="#parsing">Parsing</a></h2>
<p>The Python interpreter first reads your Python code. This stage involves parsing the code into an Abstract Syntax Tree (AST). The AST represents the code's structure in a tree-like form, making it easier for the compiler to understand and manipulate.</p>
<p>Try running the following code, which uses the ast module to parse a simple function into an AST, illustrating what happens at the beginning of the Python code execution process:</p>
<pre><code class="language-python">import ast

code = """
def greet(name):
    return f'Hello, {name}!'
"""

tree = ast.parse(code)
print(ast.dump(tree, indent=4))
</code></pre>
<h2 id="compilation"><a class="header" href="#compilation">Compilation</a></h2>
<p>After parsing, the AST is compiled into bytecode. Bytecode is a low-level, platform-independent representation of your code that can be executed by the Python virtual machine (PVM).</p>
<p>The following code compiles a string of Python code into bytecode and then disassembles it to inspect the bytecode instructions that the Python virtual machine will execute:</p>
<pre><code class="language-python">import dis

def greet(name):
    return f'Hello, {name}!'

# Compile the function into bytecode
compiled_code = compile('greet("World")', '&lt;string&gt;', 'exec')

# Disassemble to see the bytecode
dis.dis(compiled_code)
</code></pre>
<h2 id="python-virtual-machine-pvm"><a class="header" href="#python-virtual-machine-pvm">Python Virtual Machine (PVM)</a></h2>
<p>The Python virtual machine is the runtime engine of CPython. It executes the bytecode produced during the compilation stage. The PVM is an interpreter for the bytecode, going through the instructions one by one and performing the specified operations.</p>
<h3 id="in-conclusion"><a class="header" href="#in-conclusion">In Conclusion</a></h3>
<p>The execution model of CPython can be summarized as follows:</p>
<ol>
<li>Source Code: The Python source code (.py files) is written by the programmer.</li>
<li>AST: The source code is parsed into an Abstract Syntax Tree, representing the syntactical structure.</li>
<li>Bytecode Compilation: The AST is compiled into bytecode, a lower-level, platform-independent code.</li>
<li>PVM Execution: The Python virtual machine executes the bytecode, performing operations as specified.</li>
</ol>
<p>The efficiency and performance of CPython can be affected by factors such as the complexity of the Python code, the use of built-in functions (which are typically optimized C functions), and the interaction with external modules and libraries.</p>
<p>It's also worth noting that while CPython is the standard and most commonly used Python interpreter, other implementations exist, such as PyPy (which focuses on performance through Just-In-Time compilation) and Jython (which runs on the Java platform). Each implementation has its execution model, optimized for different use cases and performance characteristics.</p>
<p>We'll explore more of these things in subsequent chapters.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-the-global-interpreter-lock"><a class="header" href="#-the-global-interpreter-lock">üîí The Global Interpreter Lock</a></h1>
<p>The Global Interpreter Lock (GIL) is a mechanism used in computer languages that have memory management, notably in CPython, the standard Python implementation. The GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary because CPython's memory management is not thread-safe. The GIL ensures that only one thread runs in the interpreter at any given time, which simplifies the implementation of CPython and prevents potential conflicts between threads. But we'll see how this can have performance implications.</p>
<h2 id="implications-for-multi-threaded-applications"><a class="header" href="#implications-for-multi-threaded-applications">Implications for Multi-threaded Applications</a></h2>
<p>The existence of the GIL has significant implications for developers writing multi-threaded applications in Python:</p>
<ol>
<li>Concurrency vs. Parallelism: While the GIL allows for concurrency (multiple threads can be created and managed), it does not allow for true parallelism (multiple threads executing simultaneously) in a single Python process. This means that multi-threaded programs that are CPU-bound may not see a performance improvement; in fact, they might run slower than if they were executed in a single thread due to overhead.</li>
<li>I/O-bound Applications: For I/O-bound applications (waiting for input/output operations to complete), the GIL has less impact. The GIL is released while waiting for I/O, allowing other threads to run. Therefore, Python's threading module can be an excellent choice for I/O-bound applications.</li>
<li>Alternative Approaches: To achieve true parallelism, Python developers often use multiprocessing instead of multithreading. The multiprocessing module creates separate Python processes for each task, each with its own Python interpreter and, by extension, its own GIL. This allows tasks to run in parallel on multiple cores.</li>
</ol>
<h2 id="some-examples-to-try"><a class="header" href="#some-examples-to-try">Some Examples to Try</a></h2>
<p><strong>Example 1: Demonstrating the GIL's Impact on CPU-bound Operations</strong></p>
<p>This example demonstrates how the GIL can limit the performance of CPU-bound multi-threaded programs.</p>
<pre><code class="language-python">import threading
import time

# A simple CPU-bound function
def cpu_bound_task():
    count = 0
    while count &lt; 10000000:
        count += 1

start_time = time.time()
threads = []
for _ in range(2):  # Create 2 threads
    thread = threading.Thread(target=cpu_bound_task)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end_time = time.time()
print(f"Duration with threads: {end_time - start_time} seconds")
</code></pre>
<p><strong>Example 2: I/O-bound Multi-threading</strong></p>
<p>The next example demonstrates how multi-threading can be beneficial for I/O-bound tasks, as the GIL is released during I/O operations, allowing other threads to run.</p>
<pre><code class="language-python">import threading
import time

# A simple I/O-bound function that waits for some time, simulating an I/O operation
def io_bound_task():
    print("Task start")
    time.sleep(2)  # Simulate an I/O operation
    print("Task complete")

start_time = time.time()
threads = []
for _ in range(2):  # Create 2 threads
    thread = threading.Thread(target=io_bound_task)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end_time = time.time()
print(f"Duration with threads: {end_time - start_time} seconds")
</code></pre>
<p>In the CPU-bound example, you might not notice a significant performance improvement from using threads due to the GIL. In contrast, the I/O-bound example can benefit from multi-threading, as threads waiting for I/O allow others to run, potentially improving overall efficiency.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-alternative-python-interpreters"><a class="header" href="#-alternative-python-interpreters">üöÄ Alternative Python Interpreters</a></h1>
<p>The standard Python interpreter (CPython) sometimes faces criticism for performance issues, especially in comparison to compiled languages. This has led to the development of alternative Python interpreters, each aiming to address specific performance bottlenecks or to integrate Python more seamlessly with different environments. Let's check some of them out!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-pypy"><a class="header" href="#-pypy">üèéÔ∏è PyPy</a></h1>
<p>PyPy stands out as the most popular alternative Python interpreter, known for its impressive speed improvements over CPython. PyPy achieves its performance through Just-In-Time (JIT) compilation, which compiles Python code into machine code at runtime. This can lead to significant performance gains, especially in long-running applications. However, if you have short but frequently run scripts, the compilation overhead can be a concern.</p>
<p><strong>Features:</strong></p>
<ul>
<li>JIT compilation for faster execution.</li>
<li>Compatibility with Python 2.7 and 3.10 (at time of writing).</li>
<li>Supports most of the Python standard library and many third-party modules.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>PyPy is best suited for long-running applications where the overhead of JIT compilation can be amortized over time. It's particularly beneficial for applications with heavy numerical computations or extensive use of loops (but you may not need it if using NumPy!).</p>
<p><strong>Code Example:</strong></p>
<p>The usage of PyPy is straightforward because it's a drop-in replacement for CPython. First, download it from <a href="https://www.pypy.org/">https://www.pypy.org/</a></p>
<p>You can run your Python script using PyPy just by using the <code>pypy</code> command instead of <code>python</code>:</p>
<pre><code class="language-shell">pypy script.py
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-ironpython"><a class="header" href="#-ironpython">üîó IronPython</a></h1>
<p>IronPython is an open-source implementation of Python that runs on the .NET Framework and Mono. It is designed to seamlessly integrate with .NET, allowing Python developers to make use of .NET libraries and frameworks. IronPython aims to be a true implementation of Python, while also providing the additional performance and integration capabilities of .NET.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Full integration with the .NET Framework, enabling access to a vast library of .NET functionality.</li>
<li>Allows Python code to interoperate with .NET languages like C# and VB.NET.</li>
<li>Supports dynamic compilation to .NET bytecode, potentially offering performance benefits on the .NET runtime.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>IronPython is ideal for Python developers working in a .NET environment or needing to integrate Python code with .NET applications. It's particularly useful for projects that can benefit from the .NET framework's features, such as Windows-based desktop applications or web services.</p>
<p><strong>Code Example:</strong></p>
<p>Running a Python script with IronPython is similar to using the standard Python interpreter, but you use the <code>ipy</code> command instead.</p>
<pre><code class="language-shell">ipy script.py
</code></pre>
<p>To integrate Python code within a C# application using IronPython, you can do something like the following:</p>
<pre><code class="language-csharp">var engine = Python.CreateEngine();
var scope = engine.CreateScope();
engine.ExecuteFile("script.py", scope);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-jython"><a class="header" href="#-jython">‚òï Jython</a></h1>
<p>Jython is an implementation of Python designed to run on the Java platform. It compiles Python code to Java bytecode, allowing Python programs to seamlessly integrate with Java modules and libraries. This offers a performance boost in environments where the Java Virtual Machine (JVM) is optimized.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Runs on the JVM, allowing integration with Java libraries.</li>
<li>Access to Java's concurrency features and large ecosystem.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>Jython is a great choice when you need to integrate Python code with Java applications or take advantage of Java's rich ecosystem of libraries.</p>
<p><strong>Code Example:</strong></p>
<p>Using Jython typically involves invoking the Jython interpreter to run Python scripts or to integrate Python with Java code.</p>
<pre><code class="language-shell">jython script.py
</code></pre>
<p>In Java, you can embed Jython as follows:</p>
<pre><code class="language-java">PythonInterpreter interpreter = new PythonInterpreter();
interpreter.exec("print('Hello from Jython')");
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-graalpython"><a class="header" href="#-graalpython">üåå GraalPython</a></h1>
<p>GraalPython is part of the GraalVM ecosystem, offering a high-performance Python 3 interpreter. It's designed to execute Python code efficiently by leveraging the GraalVM's advanced JIT compiler. GraalPython aims to support Python 3.8 language features and a wide range of Python libraries, making it an attractive option for running existing Python code at higher speeds.</p>
<p><strong>Features:</strong></p>
<ul>
<li>High-performance execution through GraalVM's JIT compilation.</li>
<li>Compatibility with Python 3.8 features and a broad set of third-party libraries.</li>
<li>Interoperability with other languages supported by GraalVM, such as JavaScript, Ruby, and R, enabling polyglot applications.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>GraalPython is a good choice for projects that require high performance and are running in environments where GraalVM can be used. It's also beneficial for applications that need to interoperate with other programming languages supported by GraalVM, making it ideal for complex, multi-language systems.</p>
<p><strong>Code Example:</strong></p>
<p>Get started here: <a href="https://github.com/oracle/graalpython">https://github.com/oracle/graalpython</a></p>
<p>To run a Python script using GraalPython, you typically use the <code>graalpython</code> command provided by GraalVM.</p>
<pre><code class="language-shell">graalpython script.py
</code></pre>
<p>For polyglot applications, you can access Python code from Java like this:</p>
<pre><code class="language-java">Context context = Context.newBuilder().allowAllAccess(true).build();
context.eval("python", "print('Hello from GraalPython')");
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-cinder"><a class="header" href="#-cinder">üî• Cinder</a></h1>
<p>Cinder is Meta's (formerly Facebook's) performance-oriented fork of CPython 3.8. It incorporates several enhancements aimed at improving the performance of Python code, notably in highly concurrent server environments. Cinder is not a standalone interpreter but has contributed some of its features back to the main Python branch, showcasing its influence on the Python ecosystem.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Static Python: An experimental, opt-in type system that allows for compiling Python to more efficient code, improving performance.</li>
<li>Strict Modules: Provides a way to declare modules with stricter performance characteristics, allowing Cinder to optimize these modules more aggressively.</li>
<li>Performance Enhancements: Includes various optimizations for faster execution of Python code, particularly in the context of web and server applications.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>Cinder is tailored for large-scale, performance-sensitive Python applications, especially those running in server environments. It's most beneficial for organizations that can invest in managing a custom Python interpreter to gain execution speed.</p>
<p><strong>Code Example:</strong></p>
<p>You can find installation instructions here: <a href="https://github.com/facebookincubator/cinder">https://github.com/facebookincubator/cinder</a></p>
<p>While Cinder's usage is similar to standard Python, benefiting from its features often requires adopting specific patterns or annotations in your code, such as using static types:</p>
<pre><code class="language-python">from static import int64

def fib(n: int64) -&gt; int64:
    if n &lt;= 1:
        return n
    return fib(n-1) + fib(n-2)
</code></pre>
<p>Cinder is used similarly to CPython, but with an emphasis on its performance-enhancing features.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-micropython"><a class="header" href="#-micropython">ü§ñ MicroPython</a></h1>
<p>MicroPython is a lean and efficient implementation of Python 3, designed to run on microcontrollers and in constrained environments. Its goal is to be as compatible with standard Python as possible given the hardware limitations, making Python programming accessible for embedded systems development.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Compact: Requires minimal resources, running in as little as 256KB of code space and 16KB of RAM.</li>
<li>Peripheral Access: Includes libraries to access low-level hardware, such as digital and analog I/O, SPI, I2C, and more.</li>
<li>Interactive Prompt: Offers a Python command line (REPL) on the device for interactive development and debugging.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>MicroPython shines in embedded systems and IoT applications where resources are limited. It's ideal for developers looking to leverage Python's ease of use and readability in hardware projects, from hobbyist level to professional embedded systems.</p>
<p><strong>Code Example:</strong></p>
<p>Find download instructions here: <a href="https://micropython.org">https://micropython.org</a></p>
<p>A simple MicroPython script to blink an LED might look like this:</p>
<pre><code class="language-python">from machine import Pin
import time

led = Pin(2, Pin.OUT) # Pin 2 is an LED on many boards

while True:
    led.on()
    time.sleep(0.5)
    led.off()
    time.sleep(0.5)
</code></pre>
<p>This script toggles an LED on and off on a board like the ESP8266 or ESP32, showcasing the simplicity of using Python for hardware programming.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-profiling-tools"><a class="header" href="#-profiling-tools">üîé Profiling Tools</a></h1>
<p>Profiling Python code is crucial for identifying performance bottlenecks and optimizing the execution time of your applications.</p>
<p>Good benchmarking involves measuring the execution time of your code under specific conditions. Here are some best practices:</p>
<ol>
<li><strong>Isolate the Code You Want to Benchmark</strong> - Make sure that the code you're benchmarking is isolated from setup and teardown operations that you don't intend to measure.</li>
<li><strong>Choose the Right Tool for the Job</strong> - As we'll see, <code>timeit</code> is great for micro-benchmarks, while <code>cProfile</code> and <code>line_profiler</code> can help with more detailed profiling.</li>
<li><strong>Warm-up the Python Runtime</strong> - Before running your benchmarks, "warm up" the Python interpreter by running your code a few times without measuring it. This process can help mitigate the impact of caching and other optimizations that the interpreter might perform.</li>
<li><strong>Run Benchmarks Multiple Times</strong> - To get a more accurate measure, run your benchmarks multiple times and consider using the average time. This approach helps smooth out any irregularities caused by background processes or other anomalies.</li>
<li><strong>Consider Systematic Variations</strong> - Be aware of external factors that can affect benchmark results, such as other running processes, system load, and hardware differences. Try to minimize these variations when benchmarking.</li>
<li><strong>Benchmark with Realistic Data</strong> - Test your code with data that closely resembles what you expect in production. The performance can greatly differ based on the type, size, and complexity of the input data.</li>
</ol>
<p>There are several tools and techniques available for profiling, each with its own strengths and use cases. In the next few pages we'll check out some of the most commonly used profiling tools and methods, along with code examples.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-timetime"><a class="header" href="#-timetime">üïí time.time()</a></h1>
<p>The <code>time.time()</code> function from the Python standard library is a straightforward way to measure the elapsed time during code execution. It returns the current time in seconds since the Epoch (January 1, 1970, 00:00:00 UTC). You can use it to calculate how long a piece of code takes to execute by recording the time before and after the execution and then finding the difference.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import time

start_time = time.time()

# Place the code you want to time here
time.sleep(2)  # Example: simple sleep for 2 seconds

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time} seconds")
</code></pre>
<p>This method is very basic and is useful for quick-and-dirty timing without needing to install or use more complex profiling tools.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-timeit"><a class="header" href="#-timeit">‚è±Ô∏è timeit</a></h1>
<p><code>timeit</code> is a Python module designed to allow Python developers to time small bits of Python code with a minimal influence from the timing mechanism itself. It provides a more accurate timing mechanism than time.time() for small code snippets by taking into account setup code and running the code multiple times to calculate an average time.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import timeit

code_to_test = """
a = [1, 2, 3]
b = [4, 5, 6]
c = a + b
"""

elapsed_time = timeit.timeit(stmt=code_to_test, number=100000)
print(f"Elapsed time: {elapsed_time} seconds")
</code></pre>
<p>The function runs the code snippet specified by the stmt parameter number times and returns the total time taken.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-unixs-time-command"><a class="header" href="#-unixs-time-command">‚è≤Ô∏è Unix's time Command</a></h1>
<p>The Unix <code>time</code> command is used to measure the time taken by a program to execute, providing a simple way to time the execution duration of command-line programs and scripts. This is not a Python-specific tool but can be used with Python scripts or any other executable program.</p>
<p><strong>Usage Example:</strong></p>
<p>To time a Python script named script.py, you would use the time command like so:</p>
<pre><code class="language-shell">time python script.py
</code></pre>
<p>This will output something similar to:</p>
<pre><code class="language-shell">real    0m0.123s
user    0m0.084s
sys     0m0.036s
</code></pre>
<ul>
<li><code>real</code> indicates the total elapsed time (wall clock).</li>
<li><code>user</code> shows the total time spent in user mode.</li>
<li><code>sys</code> represents the total time spent in kernel mode.</li>
</ul>
<p>Using the time command is beneficial for getting a quick overview of the time taken by an entire program or script, including any Python scripts you might be running.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-cprofile"><a class="header" href="#-cprofile">üìä cProfile</a></h1>
<p>cProfile is a built-in profiler that provides a detailed breakdown of how much time your program spends in each function. It's great for getting an overview of which functions are the most time-consuming.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import cProfile
import re

def example_function():
    return re.compile("foo|bar")

if __name__ == "__main__":
    cProfile.run('example_function()')
</code></pre>
<p>This will output statistics about the time spent in each function, allowing you to identify which parts of your code are the slowest.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-snakeviz"><a class="header" href="#-snakeviz">üê≤ Snakeviz</a></h1>
<p>Snakeviz is a browser-based graphical viewer for the output of Python‚Äôs cProfile module.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install snakeviz</code></p>
<p>First, generate a profile file using cProfile:</p>
<pre><code class="language-python">import cProfile
cProfile.run('example_function()', 'profile_output')
</code></pre>
<p>Then, visualize it with Snakeviz:</p>
<pre><code class="language-shell">snakeviz profile_output
</code></pre>
<p>This will open up a browser tab with an interactive visualization of your profiling data.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-line_profiler"><a class="header" href="#-line_profiler">üìà line_profiler</a></h1>
<p><code>line_profiler</code> is an external tool that goes into more detail than cProfile by showing how much time is spent on each line of your code. This is especially useful for fine-tuning performance by identifying slow lines in functions.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install line_profiler</code></p>
<pre><code class="language-python">from line_profiler import LineProfiler

def do_some_operations():
    [x**2 for x in range(10000)]  # Example operation

if __name__ == '__main__':
    lp = LineProfiler()
    lp_wrapper = lp(do_some_operations)
    lp_wrapper()
    lp.print_stats()
</code></pre>
<p>You need to wrap the function you want to profile and then print the statistics.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-memory_profiler"><a class="header" href="#-memory_profiler">üß† memory_profiler</a></h1>
<p><code>memory_profiler</code> monitors the memory usage of your application, which can be crucial for identifying memory leaks or functions that use more memory than expected.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install memory_profiler</code></p>
<pre><code class="language-python">from memory_profiler import profile

@profile
def my_func():
    a = [1] * (10**6)
    b = [2] * (2 * 10**7)
    del b
    return a

if __name__ == '__main__':
    my_func()
</code></pre>
<p>This decorates a function to profile its memory usage line by line.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-py-spy"><a class="header" href="#-py-spy">üïµÔ∏è‚Äç‚ôÇÔ∏è Py-Spy</a></h1>
<p>Py-Spy is a sampling profiler for Python programs that can profile running Python processes without modifying them or needing program restarts.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install py-spy</code></p>
<p>Run it in your terminal:</p>
<pre><code class="language-shell">py-spy top --pid &lt;pid of your python program&gt;
</code></pre>
<p>Or to generate a flame graph:</p>
<pre><code class="language-shell">py-spy record --pid &lt;pid of your python program&gt; --output profile.svg
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-built-in-data-structures"><a class="header" href="#-built-in-data-structures">üèóÔ∏è Built-in Data Structures</a></h1>
<p>Python provides several built-in data structures that are highly versatile and powerful, enabling developers to store and manipulate data efficiently. These data structures include lists, tuples, dictionaries, and sets, each with its unique features and performance characteristics. They are, in general, lower-level and hence faster than custom data structures.</p>
<p>Choosing the right data structure depends on the specific requirements of your application, including the types of operations you need to perform and their frequency.</p>
<p>Let's check them out!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lists"><a class="header" href="#-lists">üìù Lists</a></h1>
<p>Lists are ordered collections of items (elements) that can be of different types. They are mutable, meaning that their elements can be changed, added, or removed.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Accessing an element by index is O(1).</li>
<li>Adding/removing elements at the end is O(1), but inserting/removing elements elsewhere can be O(n) because it may require shifting elements.</li>
<li>Searching for an element is O(n) because it requires a linear search.</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Lists are dynamic arrays and thus have some overhead for memory allocation to support their mutability and variable size. Each item in a list holds a reference to an object (which could be anything), and there's additional memory overhead for maintaining the size of the list and the pointers to each item.</p>
<p>So the memory usage of a list grows with the number of elements. However, because Python preallocates memory in chunks (to avoid frequent resizing), a list might use more memory than the actual data it stores.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_list = [1, 2, 3]
my_list.append(4)  # Add an element
my_list[1] = 20    # Modify an element
del my_list[0]     # Remove an element
print(my_list)     # Output: [20, 3, 4]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-tuples"><a class="header" href="#-tuples">üéÅ Tuples</a></h1>
<p>Tuples are similar to lists but are immutable, meaning that once a tuple is created, it cannot be modified. This makes tuples a good choice for representing fixed collections of items.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Accessing an element by index is O(1).</li>
<li>Since tuples are immutable, operations like adding or removing elements are not applicable.</li>
<li>Searching for an element is O(n).</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Tuples are immutable and hence can be optimized by Python's runtime. Since they cannot change in size, Python knows exactly how much memory to allocate at creation time.</p>
<p>Therefore, generally, tuples are more memory-efficient than lists with the same elements because of their immutability and the absence of overhead associated with variable size. However, like lists, each element is a reference to another object, so the overall memory usage depends on what is stored in the tuple.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_tuple = (1, 2, 3)
print(my_tuple[1])  # Access an element, Output: 2
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-dictionaries"><a class="header" href="#-dictionaries">üóùÔ∏è Dictionaries</a></h1>
<p>Dictionaries are unordered collections of key-value pairs. They allow for fast data lookup by key and are mutable.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Access, insertion, and deletion operations are O(1) on average because dictionaries are implemented using hash tables.</li>
<li>However, in the worst-case scenario (e.g., many key collisions), these operations can degrade to O(n).</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Dictionaries in Python are implemented using hash tables. This means they use a sparse array to provide fast access paths to values based on unique keys. Each entry in the hash table holds the key, the value, and a hash of the key for fast comparison.</p>
<p>This overhead allows for fast access but means that, byte for byte, a dictionary will use more memory than a list or tuple storing the same data. The memory usage becomes more efficient as the dictionary grows larger, but sparse usage (many empty entries) can lead to wasted space.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_dict = {'apple': 5, 'banana': 3}
my_dict['cherry'] = 7  # Add a new key-value pair
my_dict['apple'] = 10  # Update an existing key-value pair
del my_dict['banana']  # Remove a key-value pair
print(my_dict)         # Output: {'apple': 10, 'cherry': 7}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-sets"><a class="header" href="#-sets">üéØ Sets</a></h1>
<p>Sets are unordered collections of unique elements. They are mutable and provide efficient ways to perform common set operations like union, intersection, and difference.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Like dictionaries, set operations such as adding, removing, and checking for membership are O(1) on average.</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Sets are conceptually similar to dictionaries with only keys and no values. They are also backed by a hash table, providing fast operations for checking membership, adding, and removing elements. They are memory-efficient for operations involving large numbers of elements but might use more memory than lists or tuples for the same number of elements, due to the hash table mechanism.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_set = {1, 2, 3}
my_set.add(4)    # Add an element
my_set.remove(2) # Remove an element
print(my_set)    # Output: {1, 3, 4}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-general-tips"><a class="header" href="#-general-tips">üí° General Tips</a></h1>
<ul>
<li>Lists and Tuples: Good for ordered collections of items. Lists are mutable, whereas tuples are immutable.</li>
<li>Dictionaries: Ideal for associative arrays where key-value pair mappings are needed.</li>
<li>Sets: Useful for storing unique elements and performing set operations.</li>
</ul>
<h2 id="general-memory-efficiency-tips"><a class="header" href="#general-memory-efficiency-tips">General Memory Efficiency Tips</a></h2>
<ul>
<li>Use tuples instead of lists for fixed-size collections because tuples have a smaller memory overhead.</li>
<li>Consider using <strong>slots</strong> for classes if you're creating many instances of a class. This can significantly reduce the memory overhead by preventing the creation of a <strong>dict</strong> for each instance, at the cost of flexibility.</li>
<li>Be mindful of container size and cleanup. Large collections can consume a lot of memory. Removing references to unneeded objects or using data structures with smaller overheads can help manage memory usage.</li>
<li>Consider using specialized libraries or data structures for large datasets, such as arrays from the array module for homogeneous data or third-party libraries like NumPy, which can be more memory-efficient for certain tasks.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-collections-module"><a class="header" href="#the-collections-module">The Collections Module</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="algorithm-complexity"><a class="header" href="#algorithm-complexity">Algorithm Complexity</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimising-common-algorithms"><a class="header" href="#optimising-common-algorithms">Optimising Common Algorithms</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="numpy-and-pandas"><a class="header" href="#numpy-and-pandas">NumPy and Pandas</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="efficient-numerical-computations"><a class="header" href="#efficient-numerical-computations">Efficient Numerical Computations</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimising-data-manipulation"><a class="header" href="#optimising-data-manipulation">Optimising Data Manipulation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dask-and-polars"><a class="header" href="#dask-and-polars">Dask and Polars</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="threading-and-multiprocessing"><a class="header" href="#threading-and-multiprocessing">Threading and Multiprocessing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="asynchronous-programming"><a class="header" href="#asynchronous-programming">Asynchronous Programming</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrentfutures"><a class="header" href="#concurrentfutures">concurrent.futures</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="c-extensions-for-python"><a class="header" href="#c-extensions-for-python">C Extensions for Python</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compiling-with-cython"><a class="header" href="#compiling-with-cython">Compiling with Cython</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="just-in-time-compilation"><a class="header" href="#just-in-time-compilation">Just-in-Time Compilation</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-numba"><a class="header" href="#using-numba">Using Numba</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-management"><a class="header" href="#memory-management">Memory Management</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reducing-footprint-and-avoiding-leaks"><a class="header" href="#reducing-footprint-and-avoiding-leaks">Reducing Footprint and Avoiding Leaks</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="performant-web-applications"><a class="header" href="#performant-web-applications">Performant Web Applications</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="caching-strategies"><a class="header" href="#caching-strategies">Caching Strategies</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="general-best-practices"><a class="header" href="#general-best-practices">General Best Practices</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-performance-anti-patterns"><a class="header" href="#common-performance-anti-patterns">Common Performance Anti-Patterns</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-profiling"><a class="header" href="#-profiling">üîé Profiling</a></h1>
<p>It's tempting to just dive into your code and start refactoring at once! But you might end up spending lots of time eeking out tiny performance gains on parts of your code that are already pretty efficient - and completely missing the real bottlenecks.</p>
<p>That's why we start with profiling. Figure out where the problems are - and just how bad they really are. That way, you can make an evidence-based request for more time from management to spend on improving the code.</p>
<p>Here are resources you might want to profile in terms of usage:</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Network bandwidth</li>
<li>Disk IO</li>
</ul>
<p>One other thing to always remember is that profiling can add to the computer workload, and slow things down. But hopefully this is only by a very tiny amount.</p>
<p>In this chapter, we'll explore a variety of profiling methods. There's no universal panacea - it depends on how much time you have and how much granularity you need!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-simplest-method-timetime--print"><a class="header" href="#the-simplest-method-timetime--print">The Simplest Method: time.time() + print</a></h1>
<p>Let's create a simple Python function with a time complexity of O(n<sup>2</sup>). This is a "CPU-bound" problem.</p>
<p>The function <code>print_all_pairs</code> takes a list of numbers as input and prints out all pairs of numbers from the list. The function <code>generate_random_numbers</code> returns a list of numbers as long as its input, in which all the numbers are between 1 and a 1000. Feel free to run it first, to make sure everything's working.</p>
<pre><code class="language-python">import random

def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]

random_numbers = generate_random_numbers(1000)
print_all_pairs(random_numbers)
</code></pre>
<p>Now let's use Python's built-in time module to add some super-simple profiling! This involves adding a few lines of code, commented below</p>
<pre><code class="language-python">import random
import time # import the time module

... # leave our functions as they are

random_numbers = generate_random_numbers(1000)

start_time = time.time() # start the clock
print_all_pairs(random_numbers)
end_time = time.time() # stop the clock

time_taken = end_time - start_time # calculate the time taken
print(time_taken) # print the result
</code></pre>
<p>Be aware that the <code>time_taken</code> will always vary - it's an approximation. Your computer might be doing other more- or less-intensive things, at any given time.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-unix-time-command"><a class="header" href="#the-unix-time-command">The Unix time Command</a></h1>
<p>If you're on a Unix-like system, you can use the <code>time</code> command! This has several benefits, which we'll explore below.</p>
<p>Now there are actually two possible commands: a shell version (accessed through <code>time</code>) and a system command (accessed through <code>/usr/bin/time</code>). We want the latter.</p>
<p>Again, let's take our original, simplest script, saved as <code>test.py</code>:</p>
<pre><code class="language-python">import random

def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]

random_numbers = generate_random_numbers(1000)
print_all_pairs(random_numbers)
</code></pre>
<p>Run it from the command line like this: <code>/usr/bin/time -p --verbose python test.py</code></p>
<p>The <code>-p</code> flag puts our results on separate lines, which is prettier. You'll get something like this:</p>
<pre><code class="language-shell">real 9.86 # this represents the total time taken
user 2.64 # this is how much time the CPU spent outside kernel functions
sys 1.09 # this is how much time spent on kernel functions
</code></pre>
<p><code>real</code> - <code>user</code> - <code>sys</code> = time spent on IO tasks + any other system tasks.</p>
<p>We can see the advantage of using the Unix <code>time</code> command now: it allows us to strip away "background noise"; it also includes the time taken to load the Python executable, which may be relevant if you're profiling code that spawns lots of processes.</p>
<p>Try adding <code>-l</code> on MacOS or <code>--verbose</code> on Linux for more info. <code>Page faults</code> are worth keeping an eye on - they suggest that you're using RAM and the kernel is resorting to disk access.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-timeit-module-profiling-from-the-terminal"><a class="header" href="#the-timeit-module-profiling-from-the-terminal">The timeit Module: Profiling from the Terminal</a></h1>
<p>We can also use Python's built-in <code>timeit</code> module to test our script from the command line. This helps solve for any CPU fluctuations in our profiling time, by running several loops and iterations.</p>
<p>Let's go back to our original Python script:</p>
<pre><code class="language-python">import random

def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]
</code></pre>
<p>Then, in your terminal, you'll want to run the following:</p>
<pre><code class="language-shell">python -m timeit -v -s "import test; random_numbers = test.generate_random_numbers(1000)" "test.print_all_pairs(random_numbers)"
</code></pre>
<p>I've included the <code>-v</code> (i.e. <code>--verbose</code>) flag, because I want to see the cumulative time spent, from which I can calculate an average variability. This will output something like:</p>
<pre><code class="language-shell">raw times: 10.8 sec, 9.85 sec, 8.05 sec, 9.03 sec, 10 sec

1 loop, best of 5: 8.05 sec per loop
</code></pre>
<p><strong>Pretty consistent! üòé</strong>
<strong>But timeit seems to have slowed me down vs the decoractor method üòû</strong></p>
<p>Other flags you might want to include are <code>-n</code> (number of loops) and <code>-r</code> (number of repetitions). If you leave this out, timeit will use its defaults.</p>
<h2 id="the-timeit-magic-profiling-in-jupyter"><a class="header" href="#the-timeit-magic-profiling-in-jupyter">The %timeit Magic: Profiling in Jupyter</a></h2>
<p>If you're using a Jupyter Notebook, you can do something similar with the <code>%timeit</code> magic. Stick it before what you want to profile:</p>
<pre><code class="language-python">%timeit print_all_pairs(random_numbers)
</code></pre>
<p>N.B. the methodology behind the Python <code>timeit</code> module and the Jupyter <code>%timeit</code> magic is a little different: the former picks the quickest time; the latter gives the mean and standard deviation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="splitting-function-calls-with-cprofile"><a class="header" href="#splitting-function-calls-with-cprofile">Splitting Function Calls with cProfile</a></h1>
<p>Perhaps you don't want to manage a system of decorator functions, and you just want to see the breakout times for all the functions in your code. <code>cProfile</code> is a built-in tool to help.</p>
<p>It adds some computational overhead, but it helps you profile which parts of your code are being called the most and quantify what time penalty they are incurring.</p>
<p>To demonstrate this utility, let's break our nested loop script into two functions, and remove our use of <code>random</code> to avoid any <code>import</code> overhead, as below:</p>
<pre><code class="language-python">def inner_loop(numbers, i):
    n = len(numbers)
    for j in range(n):
        print(numbers[i], numbers[j])

def outer_loop(numbers):
    n = len(numbers)
    for i in range(n):
        inner_loop(numbers, i)

numbers = list(range(1, 1001))  # List of numbers from 1 to 1000
outer_loop(numbers)
</code></pre>
<p>Now let's run it from the command line, with <code>cProfile</code>:</p>
<pre><code class="language-shell">python -m cProfile -s cumulative test.py
</code></pre>
<p>The <code>-s cumulative</code> flag sorts the output by cumulative time spent. It should look something like this:</p>
<pre><code class="language-shell">1002005 function calls in 7.515 seconds

Ordered by: cumulative time

  ncalls  tottime  percall  cumtime  percall filename:lineno(function)
      1    0.000    0.000    7.515    7.515 {built-in method builtins.exec}
      1    0.000    0.000    7.515    7.515 test.py:1(&lt;module&gt;)
      1    0.002    0.002    7.515    7.515 test.py:6(outer_loop)
   1000    0.437    0.000    7.513    0.008 test.py:1(inner_loop)
1000000    7.075    0.000    7.075    0.000 {built-in method builtins.print}
   1001    0.001    0.000    0.001    0.000 {built-in method builtins.len}
      1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}
</code></pre>
<p>Makes sense! What's really costing us time is not the loops per se, but all the <code>print</code> statements.</p>
<h2 id="plotting-cprofile-with-snakeviz"><a class="header" href="#plotting-cprofile-with-snakeviz">Plotting cProfile with SnakeViz</a></h2>
<p>Everyone loves some pretty pictures! SnakeViz is 'a viewer for Python profiling data that runs as a web application in your browser'. Let's get it from PyPI: <code>pip install snakeviz</code></p>
<p>To use SnakeViz, you need to have generated an output file (<code>-o</code>) using <code>cProfile</code>. So let's try something like this:</p>
<pre><code class="language-shell">python -m cProfile -o program.prof test.py
snakeviz program.prof
</code></pre>
<p>... which will open up the SnakeViz page in your browser. At the top is a waterfall diagram. Here's my hovering over the middle bar, displaying information about the <code>outer_loop</code> function:</p>
<p><img src="profiling/../assets/SnakeVizTop.png" alt="SnakeViz visualisation of Python script" /></p>
<p>At the bottom is a filterable table, allowing you to interact with the conventional cProfile output:</p>
<p><img src="profiling/../assets/SnakeVizBottom.png" alt="SnakeViz tabular representation of Python script" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="adding-a-decorator-function"><a class="header" href="#adding-a-decorator-function">Adding a Decorator Function</a></h1>
<p>Using <code>time.time()</code> and some <code>print</code> statements is a quick-and-dirty way to do some initial profiling. But it can get messy! Using a decorator is both neater and more extendable.</p>
<p>Let's reuse our code from before, but add a decorator method!</p>
<pre><code class="language-python">import random
import time
from functools import wraps # add this import, so that we can access the decorated functions

# here's our profiler function
def time_profiler(function): 
    @wraps(function)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = function(*args, **kwargs)
        end_time = time.time()
        execution_time = end_time - start_time
        print(f"Function '{function.__name__}' took {execution_time:.6f} seconds to execute.")
        return result
    return wrapper

@time_profiler # add our decoractor for profiling
def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]

random_numbers = generate_random_numbers(1000)
print_all_pairs(random_numbers)
</code></pre>
<p>Run this from your terminal and you'll get something like: <code>Function 'print_all_pairs' took 7.186900 seconds to execute.</code></p>
<p>If we're wanting to profile multiple functions in a more complex piece of code, we can just add the <code>@time_profiler</code> decorator above any of the function names.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-granular-with-line-profiler"><a class="header" href="#getting-granular-with-line-profiler">Getting Granular with line-profiler</a></h1>
<p>Now we're going to get even more granular, going from <code>script -&gt; function-by-function -&gt; line-by-line</code>! We can use the awesome little tool, <code>line-profiler</code>. It provides way more detail, at the cost of some overhead. We can get it from PyPI: <code>pip install line-profiler</code>.</p>
<p>Let's go back to our original script, with the nested loops:</p>
<pre><code class="language-python">import random

def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]

random_numbers = generate_random_numbers(1000)
print_all_pairs(random_numbers)
</code></pre>
<p>We need to add a decorator above our function:</p>
<pre><code class="language-python">import random 

@profile
def print_all_pairs(numbers):
    ... # same as before
</code></pre>
<p>To use <code>line-profiler</code>, we'll run the bundled <code>kernprof</code> CLI script. We include two flags: <code>-l</code> for "line-by-line" and <code>-v</code> for printing the output to the console.</p>
<pre><code class="language-shell">python -m kernprof -lv test.py
</code></pre>
<p>And kernprof says...!</p>
<pre><code class="language-shell">Wrote profile results to test.py.lprof
Timer unit: 1e-06 s

Total time: 6.6159 s
File: test.py
Function: print_all_pairs at line 3

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
     3                                           @profile
     4                                           def print_all_pairs(numbers):
     5         1          2.0      2.0      0.0      n = len(numbers)
     6      1001        423.0      0.4      0.0      for i in range(n):
     7   1001000     423059.0      0.4      6.4          for j in range(n):
     8   1000000    6192419.0      6.2     93.6              print(numbers[i], numbers[j])
</code></pre>
<p>I'm starting to think our problem might be all this <code>print</code>-ing üòÖüòÖ</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="profiling-memory-usage-with-memory-profiler"><a class="header" href="#profiling-memory-usage-with-memory-profiler">Profiling Memory Usage with memory-profiler</a></h1>
<p>No surprises as to what this chapter is about! We'll be using the RAM equivalent of <code>line-profiler</code>, called <code>memory-profiler</code>. But what we want from memory usage is less clear than CPU usage, and a bit of a Goldilock's scenario:</p>
<ul>
<li>we may want to reduce RAM usage for greater efficiency</li>
<li>we may want to increase RAM usage, to save the number of CPU cycles required</li>
</ul>
<p>I won't go into too much detail here, but in essence: memory allocation is expensive, so sometimes overallocating is better.</p>
<p>Examining your code with <code>memory-profiler</code> adds quite a lot of overhead - to make it run faster, you'll also want to install <code>psutil</code>. For graphs, you'll need <code>matplotlib</code>. So, let's get them all from PyPI: <code>pip install memory-profiler psutil matplotlib</code></p>
<p>We decorate our script as with <code>line-profiler</code>:</p>
<pre><code class="language-python">import random

@profile
def print_all_pairs(numbers):
    n = len(numbers)
    for i in range(n):        
        for j in range(n):    
            print(numbers[i], numbers[j])

def generate_random_numbers(length):
    return [random.randint(1, 1000) for _ in range(length)]

random_numbers = generate_random_numbers(1000)
print_all_pairs(random_numbers)
</code></pre>
<p>To run it, we have two options:</p>
<ol>
<li><strong>Getting tabular output from the terminal</strong></li>
</ol>
<p>To do this, enter this (rather slow...) command:</p>
<pre><code class="language-shell">python -m memory_profiler test.py
</code></pre>
<p>This will return a table showing how much RAM is added during the running of the script:</p>
<pre><code class="language-shell">Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
     3   17.977 MiB   17.977 MiB           1   @profile
     4                                         def print_all_pairs(numbers):
     5   17.980 MiB    0.004 MiB           1       n = len(numbers)
     6   17.984 MiB    0.000 MiB        1001       for i in range(n):
     7   17.984 MiB    0.000 MiB     1001000           for j in range(n):
     8   17.984 MiB    0.004 MiB     1000000               print(numbers[i], numbers[j])
</code></pre>
<ol start="2">
<li><strong>Plotting a graph over time</strong></li>
</ol>
<p>You can use the bundled utility, <code>mprof</code>, for this:</p>
<pre><code class="language-shell">mprof run test.py
mprof plot
</code></pre>
<p>Which will plot a graph similar to this:</p>
<p><img src="profiling/../assets/mprof.png" alt="memory_profiler graph" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="profiling-on-the-fly-with-py-spy"><a class="header" href="#profiling-on-the-fly-with-py-spy">Profiling on the Fly with Py-Spy</a></h1>
<p><code>py-spy</code> is another neat little profiling tool to have in your kit. It's used for inspecting pre-spawned processes - so there's no need for running a special workflow. Instead, it's useful for profiling things <em>in production</em>, and it can do this safely because it runs in a separate process.</p>
<p>There various ways to install <code>py-spy</code>. If you're on macOS, you can use Homebrew: <code>brew install py-spy</code>. If you have Rust (which <code>py-spy</code> is programmed in) already installed, you can do: <code>cargo install py-spy</code>. But for now, let's go with our classic: <code>pip install py-spy</code>.</p>
<div class="warning">By the way, you need root access to use this profiling method. That means some "sudo" commands!</div>
<p>Okay, let's give it a spin! <code>py-spy</code> has three subcommands: <code>record</code>, <code>top</code>, and <code>dump</code>. Let's explore the first two. We're going to go for our split loop function script again:</p>
<pre><code class="language-python">def inner_loop(numbers, i):
    n = len(numbers)
    for j in range(n):
        print(numbers[i], numbers[j])

def outer_loop(numbers):
    n = len(numbers)
    for i in range(n):
        inner_loop(numbers, i)

numbers = list(range(1, 1001))  # List of numbers from 1 to 1000
outer_loop(numbers)
</code></pre>
<p>If we're firing our program up from scratch, you can write something like the following:</p>
<pre><code class="language-shell">py-spy record -o profile.svg -- python test.py
</code></pre>
<p>If you're dealing with a process that's in production, you'll need to figure out the <code>Process ID</code>. If you're on a Unix-based system, I recommend <code>pgrep python</code>. Otherwise check Task Manager. Alternatively, use <code>psutil</code> and include this snippet of code at the start of your file:</p>
<pre><code class="language-python">import psutil

for proc in psutil.process_iter(['pid', 'name']):
    if 'python' in proc.info['name']:
        print(proc.info['pid'])
</code></pre>
<p>This means you can capture the PID when the process first spawns. Then you'll want to use the <code>--pid</code> flag, as follows:</p>
<pre><code class="language-shell">py-spy record -o profile.svg --pid 12345
</code></pre>
<p>These will create a üî• flame graph üî• showing CPU usage!</p>
<p><img src="profiling/../assets/py-spy-flame-graph.png" alt="py-spy record output" /></p>
<p>If you use <code>top</code>, you'll get a tabular output of how the process is doing: <code>py-spy top -- python test.py</code>:</p>
<p><img src="profiling/../assets/py-spy-top.png" alt="py-spy top output" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-structures-and-types"><a class="header" href="#data-structures-and-types">Data Structures and Types</a></h1>
<p>Python comes with several ways of creating, storing, moving, and transforming data:</p>
<ul>
<li>Lists</li>
<li>Tuples</li>
<li>Sets</li>
<li>Dictionaries</li>
</ul>
<p>In addition, people have created libraries with their own data structures, typically for tackling maths/stats/physics data problems, e.g.</p>
<ul>
<li>Numpy</li>
<li>Pandas</li>
<li>Dask</li>
<li>Polars</li>
</ul>
<p>In this section, we'll look at all their various advantages and disadvantages. Picking appropriate data structures for what you are trying to do is a major factor in writing high-performance code. A rule of thumb to remember throughout is that "generic" structures containing multiple data types will tend to incur greater overhead when being manipulated than those of a single type.</p>
<p>We'll also consider how data structures interact with algorithm selection.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arrays-tuples-static-and-lists-dynamic"><a class="header" href="#arrays-tuples-static-and-lists-dynamic">Arrays: Tuples (Static) and Lists (Dynamic)</a></h1>
<p>An array is a class of data structure which stores elements in order and contiguously in memory, and provides constant-time access them. This means that, if we know an elements position in the index, we can find it super-quick in O(1). Python has two flavours of array: <code>tuples</code> and <code>lists</code>. We can recognise them by <code>[]</code> vs <code>()</code> respectively.</p>
<p>When an array is created, the computer has to allocate a block of system memory to store it (hence, in some programming languages, arrays are fixed-size). This is "metaphorically" the <code>tuple</code>: a static array with immutable elements. But because <code>tuples</code> are static and immutable, Python can cache them, meaning it can skip out on <em>actually</em> speaking to the kernel for system memory. Python gives us the <code>list</code> as a helpful abstraction to make modifying arrays easier ... at a cost. We call them dynamic arrays.</p>
<p>When to use a <code>tuple</code> vs a <code>list</code> is often dictated by the nature of your data: does it <em>need</em> to change, or is it <em>fixed</em>? But if performance is a concern, it's worth asking the question: can I use a <code>tuple</code> instead of a <code>list</code>? This is because the former require less memory and CPU overhead. Try the <a href="data_structures_and_algorithms/../profiling/the_timeit_module.html">timeit</a> profiling method on the initialisation examples below!</p>
<pre><code class="language-python"># Initializing a list
example_list = [1, 2, 3, 4, 5]
print("Example list:", example_list)

# Initializing a tuple
example_tuple = (1, 2, 3, 4, 5)
print("Example tuple:", example_tuple)
</code></pre>
<h2 id="extending-arrays"><a class="header" href="#extending-arrays">Extending Arrays</a></h2>
<p>Consider this operation:</p>
<pre><code class="language-python">example_list = [1, 2, 3, 4, 5]
example_list.append(6)
</code></pre>
<p>Under the hood, Python has to create a new array to hold the original elements of <code>example_list</code> PLUS the new element. Doing this often would be very expensive: allocating memory is <strong>expensive</strong>.</p>
<p>In reality, Python sort-of knows this. Rather than creating an array of <code>len(example_list)+1</code>, CPython will assume that some more <code>.append</code> method calls are coming, so it will <em>overallocate</em> approximately 115% percent (following a growth pattern of 0, 4, 8, 16, 25, 35, 46, 58, 72, 88, ...), and then fill it up until it needs to resize again. Still, if you are constantly appending to a list, expect an ongoing memory cost. On the flipside, it's an O(1) operation.</p>
<p>The <code>tuple</code> doesn't let you have this problem, by design! There's no <code>.append(x)</code> ... -ish. You can hack your way around it by <em>adding</em> two tuples together, to create a new tuple. Which is not a in-place operation, and also a slower O(n). ü§¶ü§¶ü§¶</p>
<h2 id="finding-things-sorting-and-searching"><a class="header" href="#finding-things-sorting-and-searching">Finding Things (Sorting and Searching)</a></h2>
<p>This book has an interest in straightforward performant programming. We're not going to go into the depths of comparing sorting algorithms and time complexities. Instead, my recommendation is to make use of Python's built-in methods for lists. The Python team has spent time optimising these.</p>
<p>For instance, <code>list.sort()</code> uses "Timsort", invented by Tim Peters (who also wrote the famous <a href="https://en.wikipedia.org/wiki/Zen_of_Python">Zen of Python</a>).</p>
<pre><code class="language-python">import random

random_numbers = [random.randint(1, 1000) for _ in range(100)]

random_numbers.sort()

print("Sorted list using sorted():", sorted_numbers)
print("Sorted list using sort():", random_numbers)
</code></pre>
<p>Similarly, take the case of searching for an item, with the aim of returning the index. In Python, you can use the <code>.index()</code> method, which is highly optimised and has a worst case time complexity of O(n). It's only worth implementing something like binary search, which is O(log n), if the list is <em>already</em> sorted, as otherwise you'll be incurring the sorting cost too.</p>
<pre><code class="language-python">import random

random_numbers = [random.randint(1, 100) for _ in range(100)]

try:
    index = random_numbers.index(42)
    print("There is a 42 at index", index)
except ValueError:
    print("42 is not in the list.")
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ready-sets-dictionaries"><a class="header" href="#ready-sets-dictionaries">Ready: Sets, Dictionaries</a></h1>
<p>The order of elements was important in <code>lists</code> and <code>tuples</code>, but what if we don't care about that? As long as every element is unique (or at least the keys are), we can use another two native data structures in Python: <code>sets</code> and <code>dictionaries</code>. The relationship between them can be summarised as follows:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>Sets</th><th>Dictionaries</th></tr></thead><tbody>
<tr><td><strong>Keys</strong></td><td>‚úÖ</td><td>‚úÖ</td></tr>
<tr><td><strong>Values</strong></td><td>‚ùå</td><td>‚úÖ</td></tr>
</tbody></table>
</div>
<p>I.e. a <code>set</code> is a data structure containing unique elements; a <code>dictionary</code> is a data structure containing unique elements (it's keys), each of which have some associated data (their values).</p>
<p>Let's summarise the pros and cons of <code>sets</code>/<code>dictionaries</code> vs <code>lists/tuples</code>, on average time complexities for operations and needs:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>Sets/Dictionaries</th><th>Lists/Tuples</th><th>Who Wins?</th></tr></thead><tbody>
<tr><td><strong>Search</strong></td><td>O(1)</td><td>O(n)</td><td>Sets/Dictionaries</td></tr>
<tr><td><strong>Membership (x in y)</strong></td><td>O(1)</td><td>O(n)</td><td>Sets/Dictionaries</td></tr>
<tr><td><strong>Insertion</strong></td><td>O(1)</td><td>O(1)</td><td>Lists/Tuples (just)</td></tr>
<tr><td><strong>Iteration</strong></td><td>O(n)</td><td>O(n)</td><td>Lists/Tuples (just)</td></tr>
<tr><td><strong>Sorting</strong></td><td>N/A</td><td>O(n log n)</td><td>Lists/Tuples (obvs)</td></tr>
<tr><td><strong>Deletion</strong></td><td>O(1)</td><td>O(n)</td><td>Sets/Dictionaries</td></tr>
<tr><td><strong>Memory</strong></td><td>More (Hashing)</td><td>Less</td><td>Lists/Tuples</td></tr>
</tbody></table>
</div>
<p>Why do sets/dictionaries use more memory? In simple terms, creating one requires an allocation of a block of memory. A hash function then enables the key to be used as an index, allowing for O(1) look-up - just as <code>list[index]</code> is O(1) too. Python does some further optimising under the hood by putting the keys/values into their own array. But still, hash tables are bigger because by nature they contain empty buckets. Also, when a hash table becomes more than 2/3rds full, there's a compute cost of expanding the table (to <code>3 * len(set)</code>).</p>
<p>One other thing to note is that the O(1)s in the Sets/Dictionaries column can also disguise a potential constant factor - how quick the hashing algorithm is. But in general, sets and dictionaries outperform if you're just wanting to add, check, and delete unique elements from groups. Try running the code below to see what I mean!</p>
<pre><code class="language-python">import time

# Create our list and set
unique_list = list(range(100000))
unique_set = set(unique_list)

# Let's iterate through the list and perform membership tests
start_time = time.time()
for i in unique_list:
    _ = i in unique_list
list_time = time.time() - start_time
print("List lookup time:", list_time)

# Compare this with the set
start_time = time.time()
for i in unique_set:
    _ = i in unique_set
set_time = time.time() - start_time
print("Set lookup time:", set_time)
</code></pre>
<p>Here were my results üò≤</p>
<pre><code class="language-shell">List lookup time: 49.48573088645935
Set lookup time: 0.010624885559082031
</code></pre>
<h2 id="a-note-about-imports-and-namespace-dictionary-look-ups"><a class="header" href="#a-note-about-imports-and-namespace-dictionary-look-ups">A Note about Imports and Namespace Dictionary Look-Ups</a></h2>
<p>It's obvious to avoid unneccesary <code>import</code> statements in your script. It's also faster to be explicit with your imports, due to how Python finds things from its namespaces.</p>
<p>It does dictionary look-ups to find things in this order: <code>locals() -&gt; globals() -&gt; __builtin__</code> - and stops when it finds what it's looking for.</p>
<p>Hence the results of this code:</p>
<pre><code class="language-python">import random
from random import randint

def func1(n):
    result = 0
    for _ in range(n):
        result += random.randint(1, 100)
    return result

def func2(n):
    result = 0
    for _ in range(n):
        result += randint(1, 100)
    return result

def func3(n, randint=random.randint):
    result = 0
    for _ in range(n):
        result += randint(1, 100)
    return result

# Testing the functions
print(func1(1000000))
print(func2(1000000))
print(func3(1000000))
</code></pre>
<p>Let's use our trusty friend <a href="data_structures_and_algorithms/../profiling/function_calls_with_cprofile.html">cProfile</a>:</p>
<pre><code class="language-python">python -m cProfile test.py
</code></pre>
<p>Here's the bit of the output we're interested in:</p>
<div class="table-wrapper"><table><thead><tr><th>tottime</th><th>function</th></tr></thead><tbody>
<tr><td>0.381</td><td>test.py:4(func1)</td></tr>
<tr><td>0.358</td><td>test.py:10(func2)</td></tr>
<tr><td>0.353</td><td>test.py:16(func3)</td></tr>
</tbody></table>
</div>
<p>Why's this happened? Let's look at the code more closely:</p>
<ul>
<li>In func1, we explicitly ask to look at the <code>random</code> library, so Python has to go all the way to the <strong>builtin</strong> map to find what it needs. That requires the most dictionary look-ups, and takes the longest time.</li>
<li>func2 leverages the <code>from random import randint</code> line so that it can now find the <code>randint</code> function in <code>globals()</code>. But it's still traversing the <code>locals()</code> map and doing a dictionary look-up.</li>
<li>func3 brings <code>randint</code> into the <code>locals()</code> namespace, minimising Python's search. It looks clunky, but if you're writing code where every millisecond counts...</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lets-get-generating"><a class="header" href="#lets-get-generating">Let's Get Generating</a></h1>
<p>In this chapter, we'll look at the comprehension, iteration, and evaluation of generators, as a way to lazily evluate and poll loops.</p>
<p>Imagine we want to write a function that takes a integer <code>n</code> as input, and returns a <code>n</code> length list of square numbers, starting from 1<sup>2</sup>.</p>
<p>Here's a naive implementation:</p>
<pre><code class="language-python">def naive_builder(n):
    square_numbers = []
    for _ in range(n):
        square_numbers.append((_+1)**2)
    return square_numbers
</code></pre>
<p>The issue is that when <code>n</code> gets big, so will the list <code>square_numbers</code>. We've seen how <code>list.append()</code> has compute overhead as an operation. We're also then going to have a memory-hogging array at the end of it. Great...</p>
<p>But if all we're going to do with this list is subsequently iterate over it, there's a better approach: using a generator function. In the following code, let's implement a naive builder and iterate over its list, and then a generator version. We'll do some time-profiling. Then we'll inspect more closely how the iterator version works.</p>
<pre><code class="language-python">import math
import time

n = int(input("How many iterations? "))

# This is the naive implementation again
def naive_builder(n):
    square_numbers = []
    for _ in range(n):
        square_numbers.append((_+1)**2)
    return square_numbers

# What's changed in our generator function?
def generator_builder(n):
    for _ in range(n):
        yield (_+1)**2 
        
# Let's iterate over each and time it!
naive_start_time = time.time()
for number in naive_builder(n):
    pass # do something
naive_end_time = time.time() 

gen_start_time = time.time() 
for number in generator_builder(n):
    pass # do something
gen_end_time = time.time()

# Calculate time taken
naive_time_taken = naive_end_time - naive_start_time
print("Naive: ", naive_time_taken) 

gen_time_taken = gen_end_time - gen_start_time 
print("Generator: ", gen_time_taken)
</code></pre>
<p>Here's what I got on my machine for running over 100m items - you'll see why generators are so good now!</p>
<pre><code class="language-shell">How many iterations? 100000000
Naive:  26.604732751846313
Generator:  16.16110110282898
</code></pre>
<p>That's about a 40% speed improvement. Rather than building a list and then iterating over it to do the actual work, we've just done the actual work by iteratively polling the generator. Such an algorithmic design is called <em>single pass</em> / <em>online</em>.</p>
<h2 id="when-to-use--not-use-generators"><a class="header" href="#when-to-use--not-use-generators">When to use / not use generators</a></h2>
<p>On the face of it, generators seem great: more speed, less memory usage. But what if you wanted to access the list data more than once? In that case, accepting the one-off cost of creating a list is the better option. Otherwise, you'll be forced to continually re-run the generator, which takes time.</p>
<p>Of course, if you're in a memory-constrained environment, then you might want to disregard this!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sensible-loop-design"><a class="header" href="#sensible-loop-design">Sensible Loop Design</a></h1>
<p>In a second, we'll go on to vector computation, and see why Python's built-in lists aren't great for handling these.</p>
<p>But before that, it's worth looking more closely at the loop construct, and considering two ways in which we can easily optimise our programs. The first of these involves a built-in <code>generator</code>!</p>
<pre><code class="language-python">def slow_sum_of_squares(numbers):
    result = 0
    for num in numbers:
        result += num ** 2
    return result

def fast_sum_of_squares(numbers):
    result = sum(num ** 2 for num in numbers)
    return result

# Example usage:
numbers = list(range(1, 1000000))
slow_result = slow_sum_of_squares(numbers)
fast_result = fast_sum_of_squares(numbers)
</code></pre>
<p>Let's run it from the command line, with our trusty friend <code>cProfile</code> (described back <a href="data_structures_and_algorithms/../profiling/function_calls_with_cprofile.html">here</a>):</p>
<pre><code class="language-shell">python -m cProfile -s cumulative test.py
</code></pre>
<p>You'll see that the <code>fast_sum_of_squares</code> number is astonishingly quick!</p>
<h2 id="avoiding-unnecessary-calculations"><a class="header" href="#avoiding-unnecessary-calculations">Avoiding Unnecessary Calculations</a></h2>
<p>Let's look at an even more explicit and simple example.</p>
<p>Imagine we're collecting money for a charity, and a company has promised to match what's raised. Let's go and collect a million donations, where everyone gives up to $1m (aren't people wonderful). Now we want to know the final amount!</p>
<p>We could:</p>
<ul>
<li>multiple every donation by two before adding it to the running total</li>
<li>add every donation to the running total, and then double it at the end</li>
</ul>
<p>Obviously the below is a stuipdly inefficient way of calculating, but I was trying to come up with an artificial example to make a point!</p>
<pre><code class="language-python">import random

def slow_match(donations):
    total = 0
    for donation in donations:
        total += donation * 2
    return total

def fast_match(n):
    total = 0
    for donation in donations:
        total += donation
    return total * 2

donations = list(random.randint(1, 1000000) for _ in range(1000000))

slow_method = slow_match(donations)
fast_method = fast_match(donations)
</code></pre>
<p>Again, run it with <code>cProfile</code>. On my machine, I knock off 30% by using fast_match. Computers are fast, but operations obviously still take time! So you might as well avoid repetitive code when possible.</p>
<p>Above was a trite example. But you may have more complex calculations being reused. There's no need to keep computing and then reallocating the result to a place in memory if it's unchanging.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="numpy-the-solution-to-lists"><a class="header" href="#numpy-the-solution-to-lists">Numpy: the Solution to Lists</a></h1>
<p>Python's native list structure has a problem for vector operations and matrix manipulations. Lists don't actually hold the data - instead, they hold <strong>pointers</strong> to the data location. The advantage of this is that the contents of a list can be heterogenous. However, it introduces a fetch/look-up overhead, and if you are doing lots of operations, this fragmentation adds up.</p>
<p>What does this look like in hardware terms? Data needs to be moved from memory to the CPU. Modern computers can have tiered architectures, with DRAM, SRAM, external caches, and on-chip caches. CPUs can also do things like branch prediction, speculation, overlapped instruction fetching, pipelining, and superscalar execution. If you want to get into the weeds, you can use the Linux <code>perf</code> tool to do some profiling, looking for <code>cache-misses</code> (memory bound) and <code>page-faults</code> (disk/network bound). But we might as well be sensible with how we do things.</p>
<h3 id="the-array-module"><a class="header" href="#the-array-module">The Array Module?</a></h3>
<p>Python offers an <code>array</code> module, that overcomes the memory fragementation issue by storing items sequentially. Iterating through an array therefore doesn't require multiple look-ups, as data can be cached (i.e. closer in terms of spatial and temporal locality to the CPU). But then we run into a different issue: Python, as a high-level interpreted language, isn't optimised for vector operations, and isn't good at dealing with the low-level implementation of <code>array</code>.</p>
<h2 id="enter-numpy"><a class="header" href="#enter-numpy">Enter NumPy</a></h2>
<p>NumPy stores items sequentially in memory AND offers optimised vector operations. It requires orders of magnitude less instructions, and gets more data onto the cache, closer to the CPU. It also has a relatively concise syntax, which can make code cleaner too.</p>
<p>Let's compare generating two million-item lists and doing multitiplication. First, in native Python:</p>
<pre><code class="language-python">import random
# Generating two vectors of a million items each
vector_a = [random.random() for _ in range(1_000_000)]
vector_b = [random.random() for _ in range(1_000_000)]

# Calculate dot product using a loop
result = 0
for i in range(len(vector_a)):
    result += vector_a[i] * vector_b[i]
</code></pre>
<p>Here's the equivalent in <code>NumPy</code>:</p>
<pre><code class="language-python">import numpy as np

# Generating two vectors of a million items each
vector_a = np.random.rand(1_000_000)
vector_b = np.random.rand(1_000_000)

# Perform dot product
result = np.dot(vector_a, vector_b)
</code></pre>
<p>On my local machine, the second block of code took 0.025 seconds, versus 0.443 for the native Python. That's almost a 17x speed-up! And the NumPy syntax is much cleaner than a foreloop with array indexing.</p>
<h2 id="using-numexpr"><a class="header" href="#using-numexpr">Using <code>numexpr</code></a></h2>
<p>An issue you might face with <code>NumPy</code> is that it processes operations one by one, and stores intermediate results in temporary arrays. <code>numexpr</code> lets you compile a multi-step vector expression into something more efficient. It even supports parallelism.</p>
<p>Let's imagine we want to calculate the following: <code>(a * b) + sqrt(a + b)</code>. Here's how we'd do it in NumPy:</p>
<pre><code class="language-python">import numpy as np
import time

vector_a = np.random.rand(1_000_000)
vector_b = np.random.rand(1_000_000)

result_np = (vector_a * vector_b) + np.sqrt(vector_a + vector_b)
</code></pre>
<p>This took a little longer on my machine: 0.028 seconds. To use <code>numexpr</code>, we have to enter the calculation as a string:</p>
<pre><code class="language-python">import numpy as np
import numexpr as ne

# Generating two vectors of a million items each
vector_a = np.random.rand(1_000_000)
vector_b = np.random.rand(1_000_000)

result_ne = ne.evaluate("(vector_a * vector_b) + sqrt(vector_a + vector_b)")
</code></pre>
<p>This was a blazing fast 0.006 seconds, a further 4.7x speedup...</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="accelerating-pandas-with-cpus-and-gpus"><a class="header" href="#accelerating-pandas-with-cpus-and-gpus">Accelerating Pandas with CPUs and GPUs</a></h1>
<p>If you've done any data work with Python, you'll probably have come across the Pandas DataFrame. It's built on top of NumPy, and is friendly for datasets up to 10GB or so. Beyond this, you need to have lots of RAM!</p>
<ol>
<li>Beware of datatypes in Pandas. Numeric columns reference the NumPy data types; strings reference the Python implementation (slower).</li>
<li>Avoid repeated <code>.concat</code> methods, because this requires building lots of new objects.</li>
<li>Filter data before doing compute intensive activities; drop things you don't need.</li>
</ol>
<h2 id="modin"><a class="header" href="#modin">Modin</a></h2>
<p>This is a drop-in replacement for Pandas which scales to use all your cores:</p>
<pre><code class="language-python">import pandas as pd
import modin.pandas as pd
</code></pre>
<h2 id="cudf"><a class="header" href="#cudf">cuDF</a></h2>
<p>This enables you to easily augment your code with GPUs (provided they're NVIDIA!). This is <code>Rapids cuDF pandas Accelerator Mode</code>.</p>
<p>If you're using a Jupyter notebook, check out the magic extension: <code>%load_ext cudf.pandas</code>. You can get access to an NVIDIA GPU for free on Colab. It attempts to offload whatever it can onto a GPU, falling back to the CPU with minimal cost if it can't.</p>
<p>Check out an example here: <a href="https://colab.research.google.com/drive/1XTKHiIcvyL5nuldx0HSL_dUa8yopzy_Y">https://colab.research.google.com/drive/1XTKHiIcvyL5nuldx0HSL_dUa8yopzy_Y</a></p>
<h2 id="vaex"><a class="header" href="#vaex">Vaex</a></h2>
<p>This lets you tackle bigger datasets (than your RAM can handle) by using lazy evaluation: <a href="https://github.com/vaexio/vaex">https://github.com/vaexio/vaex</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dask-and-polars-1"><a class="header" href="#dask-and-polars-1">Dask and Polars</a></h1>
<p>These are two libraries designed to address the shortcomings of Pandas.</p>
<h2 id="dask"><a class="header" href="#dask">Dask</a></h2>
<p>Dask is a flexible parallel computing library for analytics, enabling you to scale up to clusters or down to your laptop. It's particularly well-suited for working with large datasets that don't fit into memory, as it breaks down complex computations into manageable tasks, which are executed in parallel. Dask provides dynamic task scheduling optimized for computation. It's designed to integrate seamlessly with existing Python libraries like NumPy, Pandas, and Scikit-Learn, allowing you to scale those libraries' functionality across multiple cores or machines.</p>
<p>Here's a simple example that demonstrates how to use Dask Array to perform a computation that is automatically parallelized:</p>
<pre><code class="language-python">import dask.array as da

# Create a large random dask array
x = da.random.random((10000, 10000), chunks=(1000, 1000))

# Compute the mean of the array
mean_result = x.mean().compute()

print(mean_result)
</code></pre>
<h2 id="polars"><a class="header" href="#polars">Polars</a></h2>
<p>Polars is a fast DataFrames library implemented in Rust, designed for high performance and efficiency. It's capable of handling large datasets with ease and speed, focusing on lazy computations for optimal performance. Polars leverages Rust's memory safety and speed, bringing efficient data processing capabilities to Python. It's especially good for tasks involving large datasets that require high-speed manipulation, filtering, and aggregation.</p>
<p>Polars operates with both eager and lazy evaluation. The lazy evaluation allows for more optimized computations by building a computation graph and optimizing it before execution, which can lead to significant performance improvements.</p>
<p>It's largely recreated the Pandas API, for ease-of-transition:</p>
<pre><code class="language-python">import polars as pl

# Read a CSV file into a Polars DataFrame
df = pl.read_csv("your_data.csv")

# Select columns and compute the mean of a column
mean_value = df.select([
    pl.col("your_column_name").mean()
])

print(mean_value)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-async-methods"><a class="header" href="#-async-methods">‚è≥ Async Methods</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-multiprocessing"><a class="header" href="#-multiprocessing">üè≠ Multiprocessing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-compiling-python"><a class="header" href="#-compiling-python">‚öôÔ∏è Compiling Python</a></h1>
<p>When you run a Python script, the Python interpreter automatically compiles the code into Python bytecode, which is a lower-level, platform-independent representation of your source code. This bytecode is then executed by the Python virtual machine (PVM). The msot widely used Python implementation is CPython (not to be confused with Cython, which we'll cover in the next chapter).</p>
<p>But this interpretation layer adds overhead and can slow down your code, especially if lots of the following are happening:</p>
<ul>
<li>mathmatical operations</li>
<li>loops</li>
<li>temporary object creation</li>
</ul>
<p>Compiling our Python code down into machine code means that the interpretation level can be skipped out entirely.</p>
<p>Having said this, compiling Python can add complexity, potentially slowing you down from a development perspective. Refer to the earlier section, <a href="./when_to_optimise.html">When to Optimise</a>.</p>
<p>Also, if your code is slow because of other factors, such as lots of I/O, network, disk, database, or external library calls, then compiling might not add much. Also, if you've already used some of the libraries previously discussed (NumPy etc.), then you're already probably tapping into under-the-hood optimisations.</p>
<h2 id="ahead-of-time-aot-vs-just-in-time-jit"><a class="header" href="#ahead-of-time-aot-vs-just-in-time-jit">Ahead of Time (AOT) vs Just in Time (JIT)</a></h2>
<p>There are two approaches to compilation: AOT vs JIT.</p>
<p>An AOT method like Cython compiles your code down to a static set of files, which will only run on your machine (or equivalent). It's a "once and done" approach, that you can then re-use over and over again, without suffering from "cold starts". However, the end result isn't portable, and if you change your code, you'll have to perform a recompilation step before you can use it again.</p>
<p>A JIT approach is typically easier and requires less manual effort. The computer handles the compilation step, which occurs before the actual running of the code. This means you can rapidly iterate your code and make changes, without having to specify a manual recompilation. However, if you have some code that is frequently spun up, you might start suffering from a "cold start" problem, incurring a time penalty (the "just in time" compilation step) whenever the code is run.</p>
<h2 id="thinking-in-types"><a class="header" href="#thinking-in-types">Thinking in Types</a></h2>
<p>Why is Python "slow"? One reason is that it's dynamically-typed and interpreted. The virtual machine incurs overhead, because it has to be prepared for the datatypes to potentially change. It also has to wrap up low-level types with higher-level functions like hashing and printing. This is why compilation can help for code that has lots of loops making lots of calls.</p>
<p>In order to take advantage of compilation, you need to be more careful with specifying your datatypes (ints, floats, strings), so that you can remove the need for the flexibility of the interpreter.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cython-pure-c-based-compiling"><a class="header" href="#cython-pure-c-based-compiling">Cython: Pure C-based Compiling</a></h1>
<p>Cython serves as an extension of Python, designed to give C-like performance with code that is written mostly in Python. It is a powerful tool for optimizing and speeding up Python code, especially in areas where performance is critical, such as scientific computing, data analysis, and machine learning. Cython achieves this by allowing the inclusion of static type declarations, which can then be compiled into C or C++ code. This compiled code can be executed much faster than the equivalent Python code. Additionally, Cython can be used to wrap C and C++ libraries, making it possible to use them from Python code.</p>
<p>Here's a simple overview of how Cython works:</p>
<p>Type Annotations: Unlike Python, Cython allows you to add C-style static type definitions to your Python code. These type definitions are optional but can significantly increase the speed of your code by reducing the overhead of Python's dynamic typing.
Compilation: The Cython compiler translates the annotated Cython code into C or C++ code. This code is then compiled by a C/C++ compiler to produce a shared library that can be imported into Python.
Integration: The compiled Cython module can now be used from Python, just like any Python module. This allows for seamless integration of performance-critical code with higher-level Python scripts.</p>
<h2 id="using-cython"><a class="header" href="#using-cython">Using Cython</a></h2>
<p>Cython, as an AOT compilation method, requires a bit of fiddly set-up.</p>
<p>You need to <code>pip install cython</code> to begin.</p>
<p>Cython files end in <code>.pyx</code>. Here's <code>sum_squares.pyx</code> as an example function:</p>
<pre><code class="language-python"># Defining a function with Cython type annotations for C-level speed.
def sum_of_squares(int n):
    # Declaring a C int type for the loop variable.
    cdef int i
    cdef int sum = 0
    for i in range(n):
        sum += i * i
    return sum
</code></pre>
<p>Next, you need to create a <code>setup.py</code> file to compile the Cython module:</p>
<pre><code class="language-python">from setuptools import setup
from Cython.Build import cythonize

setup(
    ext_modules=cythonize("sum_squares.pyx", annotate=True)
)
</code></pre>
<p>Compile the Cython module by running:</p>
<pre><code class="language-shell">python setup.py build_ext --in-place
</code></pre>
<p>Finally, you can use the compiled module in Python:</p>
<pre><code class="language-python"># Import the compiled Cython module
import sum_squares

# Call the Cython function
print(sum_squares.sum_of_squares(10))  # Output: 285
</code></pre>
<p>The key takeaway is that you need to add type annotations!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="numba-llvm-based-compiling"><a class="header" href="#numba-llvm-based-compiling">Numba: LLVM-based Compiling</a></h1>
<p>Numba is an open-source just-in-time (JIT) compiler that translates a subset of Python and NumPy code into fast machine code. Numba is designed to help you speed up your Python applications with minimal effort and to achieve significant performance improvements with simple decorators and a few code modifications. It works by decorating Python functions to indicate that they should be JIT-compiled. This makes Numba particularly useful for numerical and scientific computing.</p>
<p>Here's a quick overview of its key features:</p>
<ul>
<li>Easy to Use: You can accelerate your functions by simply adding a decorator to your Python code.</li>
<li>Performance Boost: Numba translates your Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. This can lead to massive speed improvements.</li>
<li>Python and NumPy Support: Numba understands Python loops, Python typing, and NumPy functions, allowing you to use and compile a wide range of Python code.</li>
<li>Cross-platform: Numba works on Windows, macOS, and Linux, and can generate code for x86, x86_64, and ARM CPUs.</li>
</ul>
<h2 id="using-numba-1"><a class="header" href="#using-numba-1">Using Numba</a></h2>
<p>Let's say you want to speed up a simple function that calculates the sum of squares of each element in an array. Here's how you could use Numba to do this:</p>
<pre><code class="language-python">from numba import jit
import numpy as np

# Define a function to compute the sum of squares.
@jit(nopython=True)  # The decorator tells Numba to compile this function using the "nopython" mode for best performance.
def sum_of_squares(arr):
    total = 0
    for i in range(arr.size):
        total += arr[i] ** 2
    return total

# Generate a large array of random numbers.
arr = np.random.rand(1000000)

# Call the JIT-compiled function.
result = sum_of_squares(arr)

print(result)
</code></pre>
<p>In this example, the <code>@jit(nopython=True)</code> decorator tells Numba to compile the sum_of_squares function into machine code that does not rely on the Python runtime for execution. The nopython mode requires that the function can be fully compiled (so that it doesn't have to call back into the Python runtime), which is generally the mode that gives the best performance.</p>
<p>When you run this code, Numba compiles the sum_of_squares function the first time it's called, transforming it into fast machine code. Subsequent calls to this function are much faster, as they bypass the Python interpreter and execute the compiled machine code directly. This can lead to significant performance improvements, especially for computational-heavy tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pypy-replacement-virtual-machine"><a class="header" href="#pypy-replacement-virtual-machine">PyPy: Replacement Virtual Machine</a></h1>
<p>PyPy is an alternative implementation of Python, designed to be faster and more efficient than the default CPython interpreter. It accomplishes this primarily through the use of a Just-In-Time (JIT) compiler, which translates Python code into machine code at runtime. The JIT compiler can significantly speed up the execution of Python code, especially for long-running applications where the overhead of JIT compilation can be amortized over time.</p>
<p>Key features of PyPy:</p>
<ul>
<li>Performance: PyPy often runs faster than CPython due to its JIT compiler. The performance gain varies by task but can be substantial for CPU-intensive tasks.</li>
<li>Compatibility: PyPy aims to be fully compatible with CPython, meaning it should run any Python code written for CPython. However, there might be edge cases or reliance on CPython-specific extensions that do not work out of the box.</li>
<li>Memory Usage: PyPy can use less memory than CPython, thanks to its more efficient garbage collector.</li>
<li>Stackless Python Support: PyPy supports Stackless Python, an enhanced version of Python aimed at concurrency and micro-threads.</li>
</ul>
<p>However, PyPy is not always the best choice. For instance, code that relies heavily on C extensions not specifically optimized for PyPy may not see performance improvements and could even run slower. Moreover, the initial JIT compilation adds overhead, making PyPy less suited for scripts that run quickly and terminate.</p>
<h2 id="using-pypy"><a class="header" href="#using-pypy">Using PyPy</a></h2>
<p>You can download PyPy here: <a href="https://www.pypy.org/index.html">https://www.pypy.org/index.html</a></p>
<p>Let's consider a simple example to demonstrate Python code that can benefit from PyPy's JIT compilation. We'll calculate the sum of the squares of numbers from 1 to 1,000,000. This is a CPU-bound task that should run faster on PyPy for large calculations.</p>
<pre><code class="language-python">def sum_of_squares(n):
    return sum(x*x for x in range(1, n+1))

print(sum_of_squares(1000000))
</code></pre>
<p>To run this code in PyPy, save it to a file (e.g., sum_squares.py) and then execute it using the PyPy interpreter instead of the standard Python interpreter:</p>
<pre><code class="language-shell">pypy sum_squares.py
</code></pre>
<p>For more complex applications, especially those involving heavy computation, PyPy can often provide a significant speedup.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="foreign-function-interfaces"><a class="header" href="#foreign-function-interfaces">Foreign Function Interfaces</a></h1>
<p>Foreign Function Interfaces (FFIs) in Python allow Python code to call C libraries directly. This capability is essential for situations where Python developers need to access and use legacy C code, optimize performance-critical sections of an application, or use hardware-accelerated or system-level functionality not directly available in Python. The most common tools for working with FFIs in Python are <code>ctypes</code> and <code>cffi</code>.</p>
<h2 id="ctypes"><a class="header" href="#ctypes">ctypes</a></h2>
<p><code>ctypes</code> is a foreign function library for Python that provides C compatible data types and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.</p>
<p>Here's a simple example that uses <code>ctypes</code> to call the time function from the C standard library, which returns the current time in seconds since the Epoch (1970-01-01 00:00:00 +0000 (UTC)).</p>
<pre><code class="language-python">import ctypes
# Load the C standard library
libc = ctypes.CDLL(None)
# Define the return type of the function we are going to call
libc.time.argtypes = [ctypes.POINTER(ctypes.c_long)]
libc.time.restype = ctypes.c_long

t = libc.time(None)
print(f"The current time is {t} seconds since the Epoch.")
</code></pre>
<h2 id="cffi"><a class="header" href="#cffi">cffi</a></h2>
<p><code>cffi</code> (C Foreign Function Interface) is another library for calling C code from Python. Compared to ctypes, cffi provides more advanced features like out-of-line API mode, which allows for better error checking and integration with existing C code.</p>
<p>Below is an example of using <code>cffi</code> to achieve the same functionality as the <code>ctypes</code> example, calling the time function from the C library.</p>
<p>First, you need to install cffi: <code>pip install cffi</code></p>
<p>Then, you can use <code>cffi</code> like this:</p>
<pre><code class="language-python">from cffi import FFI
ffi = FFI()
# Define the external C function
ffi.cdef("long time(long *t);")
# Load the C standard library
C = ffi.dlopen(None)
t = ffi.new("long *")
print(f"The current time is {C.time(t)} seconds since the Epoch.")
</code></pre>
<h2 id="when-to-use-which"><a class="header" href="#when-to-use-which">When to Use Which</a></h2>
<p><code>ctypes</code> is part of the standard Python library, so it doesn't require any additional installations. It's straightforward for simple use cases but can become cumbersome for complex C libraries or where callback functions are involved.</p>
<p><code>cffi</code> requires installation but offers a more flexible and powerful interface for working with C code. It supports both ABI (Application Binary Interface) level and API (Application Programming Interface) level interfaces, making it suitable for more complex integration scenarios.</p>
<p>Both <code>ctypes</code> and <code>cffi</code> are powerful tools for integrating C libraries with Python, each with its own strengths. The choice between them depends on the specific requirements of the project, such as the complexity of the C code being interfaced and the performance requirements.</p>
<h2 id="what-about-fortran"><a class="header" href="#what-about-fortran">What About Fortran?</a></h2>
<p>If you have some legacy scientific code, the most common way to bridge Python and Fortran is through the use of <code>f2py</code> and <code>numpy</code>'s Fortran integration facilities.</p>
<p><code>f2py</code> is one of the easiest and most efficient ways to call Fortran code from Python, especially for numerical computations. f2py generates Python wrapper modules automatically, allowing Fortran routines to be called as if they were Python functions.</p>
<p>Suppose you have a simple Fortran subroutine that calculates the sum of two arrays:</p>
<pre><code class="language-fortran">subroutine add_arrays(a, b, c, n)
    integer, intent(in) :: n
    double precision, intent(in) :: a(n), b(n)
    double precision, intent(out) :: c(n)
    integer :: i
    do i = 1, n
        c(i) = a(i) + b(i)
    end do
end subroutine add_arrays
</code></pre>
<p>You can compile this Fortran code into a Python module using f2py:</p>
<pre><code class="language-shell">f2py -c -m addarrays addarrays.f90
</code></pre>
<p>This command creates a Python module named addarrays. You can then import this module in Python and call the add_arrays function:</p>
<pre><code class="language-python">import addarrays
import numpy as np

a = np.array([1.0, 2.0, 3.0], dtype=np.float64)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
c = addarrays.add_arrays(a, b)

print(c)
</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
