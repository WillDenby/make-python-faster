<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Make Python Faster: a Practical Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item "><a href="table_of_contents.html"><strong aria-hidden="true">1.</strong> üìñ Table of Contents</a></li><li class="chapter-item "><a href="when_to_optimise.html"><strong aria-hidden="true">2.</strong> ‚ùì When to Optimise</a></li><li class="chapter-item "><a href="pythons_execution_model.html"><strong aria-hidden="true">3.</strong> üêç Python's Execution Model</a></li><li class="chapter-item "><a href="algorithm_complexity.html"><strong aria-hidden="true">4.</strong> üß† Algorithm Complexity</a></li><li class="chapter-item "><a href="profiling_tools.html"><strong aria-hidden="true">5.</strong> üîé Profiling Tools</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="profiling_tools/time_time.html"><strong aria-hidden="true">5.1.</strong> üïí time.time()</a></li><li class="chapter-item "><a href="profiling_tools/timeit.html"><strong aria-hidden="true">5.2.</strong> ‚è±Ô∏è timeit</a></li><li class="chapter-item "><a href="profiling_tools/unixs_time_command.html"><strong aria-hidden="true">5.3.</strong> ‚è≤Ô∏è Unix's time Command</a></li><li class="chapter-item "><a href="profiling_tools/cprofile.html"><strong aria-hidden="true">5.4.</strong> üìä cProfile</a></li><li class="chapter-item "><a href="profiling_tools/snakeviz.html"><strong aria-hidden="true">5.5.</strong> üê≤ Snakeviz</a></li><li class="chapter-item "><a href="profiling_tools/line_profiler.html"><strong aria-hidden="true">5.6.</strong> üìà line_profiler</a></li><li class="chapter-item "><a href="profiling_tools/memory_profiler.html"><strong aria-hidden="true">5.7.</strong> üß† memory_profiler</a></li><li class="chapter-item "><a href="profiling_tools/py_spy.html"><strong aria-hidden="true">5.8.</strong> üïµÔ∏è‚Äç‚ôÇÔ∏è Py-Spy</a></li></ol></li><li class="chapter-item "><a href="built_in_data_structures.html"><strong aria-hidden="true">6.</strong> üèóÔ∏è Built-in Data Structures</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="built_in_data_structures/lists.html"><strong aria-hidden="true">6.1.</strong> üìù Lists</a></li><li class="chapter-item "><a href="built_in_data_structures/tuples.html"><strong aria-hidden="true">6.2.</strong> üéÅ Tuples</a></li><li class="chapter-item "><a href="built_in_data_structures/dictionaries.html"><strong aria-hidden="true">6.3.</strong> üóùÔ∏è Dictionaries</a></li><li class="chapter-item "><a href="built_in_data_structures/sets.html"><strong aria-hidden="true">6.4.</strong> üéØ Sets</a></li><li class="chapter-item "><a href="built_in_data_structures/the_collections_module.html"><strong aria-hidden="true">6.5.</strong> üì¶ The Collections Module</a></li></ol></li><li class="chapter-item "><a href="numpy_pandas_and_more.html"><strong aria-hidden="true">7.</strong> ‚öôÔ∏è NumPy, Pandas, and More</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="numpy_pandas_and_more/efficient_numerical_computations.html"><strong aria-hidden="true">7.1.</strong> ‚ö° Efficient Numerical Computations</a></li><li class="chapter-item "><a href="numpy_pandas_and_more/using_numexpr.html"><strong aria-hidden="true">7.2.</strong> üî¢ Using numexpr</a></li><li class="chapter-item "><a href="numpy_pandas_and_more/optimising_data_manipulation.html"><strong aria-hidden="true">7.3.</strong> üêº Optimising Data Manipulation</a></li><li class="chapter-item "><a href="numpy_pandas_and_more/dask_and_polars.html"><strong aria-hidden="true">7.4.</strong> üêª‚Äç‚ùÑÔ∏è Dask and Polars</a></li></ol></li><li class="chapter-item "><a href="non_linear_execution.html"><strong aria-hidden="true">8.</strong> üòµ‚Äçüí´ Non-Linear Execution</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="non_linear_execution/the_global_interpreter_lock.html"><strong aria-hidden="true">8.1.</strong> üîí The Global Interpreter Lock</a></li><li class="chapter-item "><a href="non_linear_execution/threading_and_multiprocessing.html"><strong aria-hidden="true">8.2.</strong> üßµ Threading and Multiprocessing</a></li><li class="chapter-item "><a href="non_linear_execution/asynchronous_programming.html"><strong aria-hidden="true">8.3.</strong> ‚è∞ Asynchronous Programming</a></li><li class="chapter-item "><a href="non_linear_execution/concurrent_futures.html"><strong aria-hidden="true">8.4.</strong> ü™¢ concurrent.futures</a></li></ol></li><li class="chapter-item "><a href="alternative_python_interpreters.html"><strong aria-hidden="true">9.</strong> üöÄ Alternative Python Interpreters</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="alternative_python_interpreters/pypy.html"><strong aria-hidden="true">9.1.</strong> üèéÔ∏è PyPy</a></li><li class="chapter-item "><a href="alternative_python_interpreters/ironpython.html"><strong aria-hidden="true">9.2.</strong> üîó IronPython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/jython.html"><strong aria-hidden="true">9.3.</strong> ‚òï Jython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/graalpython.html"><strong aria-hidden="true">9.4.</strong> üåå GraalPython</a></li><li class="chapter-item "><a href="alternative_python_interpreters/cinder.html"><strong aria-hidden="true">9.5.</strong> üî• Cinder</a></li><li class="chapter-item "><a href="alternative_python_interpreters/micropython.html"><strong aria-hidden="true">9.6.</strong> ü§ñ MicroPython</a></li></ol></li><li class="chapter-item "><a href="leaving_interpretation.html"><strong aria-hidden="true">10.</strong> üèÉ Leaving Interpretation</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="leaving_interpretation/c_extensions_for_python.html"><strong aria-hidden="true">10.1.</strong> üá® C Extensions for Python</a></li><li class="chapter-item "><a href="leaving_interpretation/compiling_with_cython.html"><strong aria-hidden="true">10.2.</strong> üí® Compiling with Cython</a></li><li class="chapter-item "><a href="leaving_interpretation/just_in_time_with_numba.html"><strong aria-hidden="true">10.3.</strong> ‚ö° Just-in-Time with Numba</a></li><li class="chapter-item "><a href="leaving_interpretation/foreign_function_interfaces.html"><strong aria-hidden="true">10.4.</strong> üåâ Foreign Function Interfaces</a></li></ol></li><li class="chapter-item "><a href="memory_management.html"><strong aria-hidden="true">11.</strong> üß© Memory Management</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="memory_management/reducing_footprint_and_avoiding_leaks.html"><strong aria-hidden="true">11.1.</strong> ‚ôªÔ∏è Reducing Footprint and Avoiding Leaks</a></li><li class="chapter-item "><a href="memory_management/caching_strategies.html"><strong aria-hidden="true">11.2.</strong> üíæ Caching Strategies</a></li></ol></li><li class="chapter-item "><a href="performant_web_applications.html"><strong aria-hidden="true">12.</strong> üåê Performant Web Applications</a></li><li class="chapter-item "><a href="final_checklist.html"><strong aria-hidden="true">13.</strong> ‚úÖ Final Checklist</a><a class="toggle"><div>‚ù±</div></a></li><li><ol class="section"><li class="chapter-item "><a href="final_checklist/easy_wins.html"><strong aria-hidden="true">13.1.</strong> üíØ Easy Wins</a></li><li class="chapter-item "><a href="final_checklist/easy_losses.html"><strong aria-hidden="true">13.2.</strong> üßπ Easy Losses</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Make Python Faster: a Practical Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="-table-of-contents"><a class="header" href="#-table-of-contents">üìñ Table of Contents:</a></h1>
<ul>
<li><a href="./when_to_optimise.html">‚ùì When to Optimise</a></li>
<li><a href="./pythons_execution_model.html">üêç Python's Execution Model</a></li>
<li><a href="./algorithm_complexity.html">üß† Algorithm Complexity</a></li>
<li><a href="./profiling_tools.html">üîé Profiling Tools</a></li>
<li><a href="./built_in_data_structures.html">üèóÔ∏è Built-in Data Structures</a></li>
<li><a href="./numpy_pandas_and_more.html">‚öôÔ∏è NumPy, Pandas, and More</a></li>
<li><a href="./non_linear_execution.html">üòµ‚Äçüí´ Non-Linear Execution</a></li>
<li><a href="./alternative_python_interpreters.html">üöÄ Alternative Python Interpreters</a></li>
<li><a href="./leaving_interpretation.html">üèÉ Leaving Interpretation</a></li>
<li><a href="./memory_management.html">üß© Memory Management</a></li>
<li><a href="./performant_web_applications.html">üåê Performant Web Applications</a></li>
<li><a href="./final_checklist.html">Final Checklist</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-when-to-optimise-your-code"><a class="header" href="#-when-to-optimise-your-code">‚ùì When to Optimise your Code</a></h1>
<p>In most organisations, programmers live under the paradigm: <code>overall team velocity &gt; individual code optimisation</code></p>
<p>Some of the tips in this book are general best practice (e.g. avoid pointless iterations in your loops); others require more of a time investment. So before considering any implementation of the latter, ask yourself the following three questions;</p>
<ol>
<li><strong>Does the code achieve its objectives?</strong></li>
</ol>
<p>Can you build a prototype to demonstrate proof-of-concept? Is the code useful? Does it do what it's meant to do? If not, figure this bit out first!</p>
<ol start="2">
<li><strong>Is the code robust?</strong></li>
</ol>
<p>Have you documented what you're doing? Does your code conform to organisational standards? Can other developers easily build on top of it?</p>
<ol start="3">
<li><strong>Is it worth further development?</strong></li>
</ol>
<p>Is this a mission-critical piece of code? Or is it only being run occasionally? Remember the famous quote from Tony Hoare/Donald Knuth: <code>Premature optimisation is the root of all evil</code></p>
<p>We code to make the world more efficient. But consult this xkcd graphic if in doubt:</p>
<p><img src="https://imgs.xkcd.com/comics/is_it_worth_the_time.png" alt="A matrix about time saved vs time invested" /></p>
<p>Some problems just aren't worth worrying about!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-pythons-execution-model"><a class="header" href="#-pythons-execution-model">üêç Python's Execution Model</a></h1>
<p>Before we dive into how to make our code faster, it's worth recapping how Python works.</p>
<p>CPython is the default and most widely used implementation of the Python programming language. It is written in C and Python, providing a foundation for executing Python programs. CPython compiles Python code into bytecode, which is then executed by the Python virtual machine. The execution model of CPython involves several key components and processes, including parsing, compilation, and interpretation.</p>
<h2 id="parsing"><a class="header" href="#parsing">Parsing</a></h2>
<p>The Python interpreter first reads your Python code. This stage involves parsing the code into an Abstract Syntax Tree (AST). The AST represents the code's structure in a tree-like form, making it easier for the compiler to understand and manipulate.</p>
<p>Try running the following code, which uses the ast module to parse a simple function into an AST, illustrating what happens at the beginning of the Python code execution process:</p>
<pre><code class="language-python">import ast

code = """
def greet(name):
    return f'Hello, {name}!'
"""

tree = ast.parse(code)
print(ast.dump(tree, indent=4))
</code></pre>
<h2 id="compilation"><a class="header" href="#compilation">Compilation</a></h2>
<p>After parsing, the AST is compiled into bytecode. Bytecode is a low-level, platform-independent representation of your code that can be executed by the Python virtual machine (PVM).</p>
<p>The following code compiles a string of Python code into bytecode and then disassembles it to inspect the bytecode instructions that the Python virtual machine will execute:</p>
<pre><code class="language-python">import dis

def greet(name):
    return f'Hello, {name}!'

# Compile the function into bytecode
compiled_code = compile('greet("World")', '&lt;string&gt;', 'exec')

# Disassemble to see the bytecode
dis.dis(compiled_code)
</code></pre>
<h2 id="python-virtual-machine-pvm"><a class="header" href="#python-virtual-machine-pvm">Python Virtual Machine (PVM)</a></h2>
<p>The Python virtual machine is the runtime engine of CPython. It executes the bytecode produced during the compilation stage. The PVM is an interpreter for the bytecode, going through the instructions one by one and performing the specified operations.</p>
<h3 id="in-conclusion"><a class="header" href="#in-conclusion">In Conclusion</a></h3>
<p>The execution model of CPython can be summarized as follows:</p>
<ol>
<li>Source Code: The Python source code (.py files) is written by the programmer.</li>
<li>AST: The source code is parsed into an Abstract Syntax Tree, representing the syntactical structure.</li>
<li>Bytecode Compilation: The AST is compiled into bytecode, a lower-level, platform-independent code.</li>
<li>PVM Execution: The Python virtual machine executes the bytecode, performing operations as specified.</li>
</ol>
<p>The efficiency and performance of CPython can be affected by factors such as the complexity of the Python code, the use of built-in functions (which are typically optimized C functions), and the interaction with external modules and libraries.</p>
<p>It's also worth noting that while CPython is the standard and most commonly used Python interpreter, other implementations exist, such as PyPy (which focuses on performance through Just-In-Time compilation) and Jython (which runs on the Java platform). Each implementation has its execution model, optimized for different use cases and performance characteristics. We'll see these later on.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-algorithm-complexity"><a class="header" href="#-algorithm-complexity">üß† Algorithm Complexity</a></h1>
<p>We should also briefly recap algorithm complexity.</p>
<p>Algorithm complexity encompasses both time complexity and space complexity. I.e. it measures the efficiency of an algorithm in terms of the time it takes to run (time complexity) and the amount of memory it consumes (space complexity) as a function of the size of the input data.</p>
<p>Understanding algorithm complexity is essential for writing efficient code, especially in languages like Python, where execution speed and resource management can be vital considerations in the development of scalable and performant applications.</p>
<h2 id="time-complexity"><a class="header" href="#time-complexity">Time Complexity</a></h2>
<p>Time complexity measures how the runtime of an algorithm changes as the size of the input data increases. It's often expressed using Big O notation, which provides an upper bound on the growth rate of the algorithm's runtime, helping to understand its worst-case scenario.</p>
<p><strong>Constant Time (O(1))</strong>: The execution time remains constant regardless of the input size. For example, accessing any element in an array by index.</p>
<p><strong>Linear Time (O(n))</strong>: The execution time increases linearly with the size of the input. For example, searching for an element in an unsorted list.</p>
<pre><code class="language-python">def find_element(lst, key):
    for item in lst:
        if item == key:
            return True
    return False
</code></pre>
<p><strong>Logarithmic Time (O(log n))</strong>: The execution time grows logarithmically with the size of the input. Binary search is a classic example.</p>
<pre><code class="language-python">def binary_search(arr, low, high, key):
    if high &gt;= low:
        mid = (high + low) // 2
        if arr[mid] == key:
            return mid
        elif arr[mid] &gt; key:
            return binary_search(arr, low, mid - 1, key)
        else:
            return binary_search(arr, mid + 1, high, key)
    else:
        return -1
</code></pre>
<p><strong>Quadratic Time (O(n^2))</strong>: The execution time grows quadratically with the input size. A common example is the bubble sort algorithm.</p>
<pre><code class="language-python">def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] &gt; arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
</code></pre>
<h2 id="space-complexity"><a class="header" href="#space-complexity">Space Complexity</a></h2>
<p>Space complexity measures the total amount of memory that an algorithm needs to run as a function of the size of the input data. Like time complexity, it's often expressed in Big O notation.</p>
<p><strong>Constant Space (O(1))</strong>: The algorithm requires a fixed amount of memory space regardless of the input size.</p>
<p><strong>Linear Space (O(n))</strong>: The space required by the algorithm increases linearly with the size of the input data.</p>
<p>For example, merging two lists requires additional space proportional to the sum of both lists' sizes, making its space complexity O(n).</p>
<pre><code class="language-python">def merge_lists(lst1, lst2):
    return lst1 + lst2  # Creates a new list with elements of lst1 followed by elements of lst2
</code></pre>
<p>Understanding and optimizing the complexity of algorithms can significantly impact the performance of Python applications. Lower complexity often translates to faster execution times and lower resource consumption, which is particularly important in resource-constrained environments or applications that handle large volumes of data.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-profiling-tools"><a class="header" href="#-profiling-tools">üîé Profiling Tools</a></h1>
<p>It's tempting to just dive into your code and start refactoring at once! But you might end up spending lots of time eeking out tiny performance gains on parts of your code that are already pretty efficient - and completely missing the real bottlenecks. That's why we start with profiling.</p>
<p>Figure out <strong>where</strong> the problems are - and <strong>how bad</strong> they are. That way, you can make an evidence-based request for more time from management to spend on improving the code. Here are resources you might want to profile in terms of usage:</p>
<ul>
<li>CPU</li>
<li>Memory</li>
<li>Network bandwidth</li>
<li>Disk IO</li>
</ul>
<p>Here are some best practices:</p>
<ol>
<li><strong>Isolate the Code You Want to Benchmark</strong> - Make sure that the code you're benchmarking is isolated from setup and teardown operations that you don't intend to measure.</li>
<li><strong>Choose the Right Tool for the Job</strong> - As we'll see, <code>timeit</code> is great for micro-benchmarks, while <code>cProfile</code> and <code>line_profiler</code> can help with more detailed profiling.</li>
<li><strong>Warm-up the Python Runtime</strong> - Before running your benchmarks, "warm up" the Python interpreter by running your code a few times without measuring it. This process can help mitigate the impact of caching and other optimizations that the interpreter might perform.</li>
<li><strong>Run Benchmarks Multiple Times</strong> - To get a more accurate measure, run your benchmarks multiple times and consider using the average time. This approach helps smooth out any irregularities caused by background processes or other anomalies.</li>
<li><strong>Consider Systematic Variations</strong> - Be aware of external factors that can affect benchmark results, such as other running processes, system load, and hardware differences. Try to minimize these variations when benchmarking.</li>
<li><strong>Benchmark with Realistic Data</strong> - Test your code with data that closely resembles what you expect in production. The performance can greatly differ based on the type, size, and complexity of the input data.</li>
</ol>
<p>One thing to always remember is that profiling can add to the computer workload, and slow things down.</p>
<p>There are several tools and techniques available for profiling, each with its own strengths and use cases. In the next few pages we'll check out some of the most commonly used profiling tools and methods, along with code examples.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-timetime"><a class="header" href="#-timetime">üïí time.time()</a></h1>
<p>The <code>time.time()</code> function from the Python standard library is a straightforward way to measure the elapsed time during code execution. It returns the current time in seconds since the Epoch (January 1, 1970, 00:00:00 UTC). You can use it to calculate how long a piece of code takes to execute by recording the time before and after the execution and then finding the difference.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import time

start_time = time.time()

# Place the code you want to time here
time.sleep(2)  # Example: simple sleep for 2 seconds

end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time: {elapsed_time} seconds")
</code></pre>
<p>This method is very basic and is useful for quick-and-dirty timing without needing to install or use more complex profiling tools.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-timeit"><a class="header" href="#-timeit">‚è±Ô∏è timeit</a></h1>
<p><code>timeit</code> is a Python module designed to allow Python developers to time small bits of Python code with a minimal influence from the timing mechanism itself. It provides a more accurate timing mechanism than time.time() for small code snippets by taking into account setup code and running the code multiple times to calculate an average time.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import timeit

code_to_test = """
a = [1, 2, 3]
b = [4, 5, 6]
c = a + b
"""

elapsed_time = timeit.timeit(stmt=code_to_test, number=100000)
print(f"Elapsed time: {elapsed_time} seconds")
</code></pre>
<p>The function runs the code snippet specified by the stmt parameter number times and returns the total time taken.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-unixs-time-command"><a class="header" href="#-unixs-time-command">‚è≤Ô∏è Unix's time Command</a></h1>
<p>The Unix <code>time</code> command is used to measure the time taken by a program to execute, providing a simple way to time the execution duration of command-line programs and scripts. This is not a Python-specific tool but can be used with Python scripts or any other executable program.</p>
<p><strong>Usage Example:</strong></p>
<p>To time a Python script named script.py, you would use the time command like so:</p>
<pre><code class="language-shell">time python script.py
</code></pre>
<p>This will output something similar to:</p>
<pre><code class="language-shell">real    0m0.123s
user    0m0.084s
sys     0m0.036s
</code></pre>
<ul>
<li><code>real</code> indicates the total elapsed time (wall clock).</li>
<li><code>user</code> shows the total time spent in user mode.</li>
<li><code>sys</code> represents the total time spent in kernel mode.</li>
</ul>
<p>Using the time command is beneficial for getting a quick overview of the time taken by an entire program or script, including any Python scripts you might be running.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-cprofile"><a class="header" href="#-cprofile">üìä cProfile</a></h1>
<p>cProfile is a built-in profiler that provides a detailed breakdown of how much time your program spends in each function. It's great for getting an overview of which functions are the most time-consuming.</p>
<p><strong>Usage Example:</strong></p>
<pre><code class="language-python">import cProfile
import re

def example_function():
    return re.compile("foo|bar")

if __name__ == "__main__":
    cProfile.run('example_function()')
</code></pre>
<p>This will output statistics about the time spent in each function, allowing you to identify which parts of your code are the slowest.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-snakeviz"><a class="header" href="#-snakeviz">üê≤ Snakeviz</a></h1>
<p>Snakeviz is a browser-based graphical viewer for the output of Python‚Äôs cProfile module.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install snakeviz</code></p>
<p>First, generate a profile file using cProfile:</p>
<pre><code class="language-python">import cProfile
cProfile.run('example_function()', 'profile_output')
</code></pre>
<p>Then, visualize it with Snakeviz:</p>
<pre><code class="language-shell">snakeviz profile_output
</code></pre>
<p>This will open up a browser tab with an interactive visualization of your profiling data.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-line_profiler"><a class="header" href="#-line_profiler">üìà line_profiler</a></h1>
<p><code>line_profiler</code> is an external tool that goes into more detail than cProfile by showing how much time is spent on each line of your code. This is especially useful for fine-tuning performance by identifying slow lines in functions.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install line_profiler</code></p>
<pre><code class="language-python">from line_profiler import LineProfiler

def do_some_operations():
    [x**2 for x in range(10000)]  # Example operation

if __name__ == '__main__':
    lp = LineProfiler()
    lp_wrapper = lp(do_some_operations)
    lp_wrapper()
    lp.print_stats()
</code></pre>
<p>You need to wrap the function you want to profile and then print the statistics.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-memory_profiler"><a class="header" href="#-memory_profiler">üß† memory_profiler</a></h1>
<p><code>memory_profiler</code> monitors the memory usage of your application, which can be crucial for identifying memory leaks or functions that use more memory than expected.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install memory_profiler</code></p>
<pre><code class="language-python">from memory_profiler import profile

@profile
def my_func():
    a = [1] * (10**6)
    b = [2] * (2 * 10**7)
    del b
    return a

if __name__ == '__main__':
    my_func()
</code></pre>
<p>This decorates a function to profile its memory usage line by line.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-py-spy"><a class="header" href="#-py-spy">üïµÔ∏è‚Äç‚ôÇÔ∏è Py-Spy</a></h1>
<p>Py-Spy is a sampling profiler for Python programs that can profile running Python processes without modifying them or needing program restarts.</p>
<p><strong>Usage Example:</strong></p>
<p>Install it with: <code>pip install py-spy</code></p>
<p>Run it in your terminal:</p>
<pre><code class="language-shell">py-spy top --pid &lt;pid of your python program&gt;
</code></pre>
<p>Or to generate a flame graph:</p>
<pre><code class="language-shell">py-spy record --pid &lt;pid of your python program&gt; --output profile.svg
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-built-in-data-structures"><a class="header" href="#-built-in-data-structures">üèóÔ∏è Built-in Data Structures</a></h1>
<p>Python provides several built-in data structures that are highly versatile and powerful, enabling developers to store and manipulate data efficiently. These data structures include lists, tuples, dictionaries, and sets, each with its unique features and performance characteristics. They are, in general, lower-level and hence faster than custom data structures.</p>
<p>Choosing the right data structure depends on the specific requirements of your application, including the types of operations you need to perform and their frequency. A rule of thumb to remember throughout is that "generic" structures containing multiple data types will tend to incur greater overhead when being manipulated than those of a single type.</p>
<p>Before we dive into them, here are some other useful tips:</p>
<h2 id="general-tips"><a class="header" href="#general-tips">General Tips</a></h2>
<ul>
<li>Lists and Tuples: Good for ordered collections of items. Lists are mutable, whereas tuples are immutable.</li>
<li>Dictionaries: Ideal for associative arrays where key-value pair mappings are needed.</li>
<li>Sets: Useful for storing unique elements and performing set operations.</li>
</ul>
<h2 id="memory-efficiency-tips"><a class="header" href="#memory-efficiency-tips">Memory Efficiency Tips</a></h2>
<ul>
<li>Use tuples instead of lists for fixed-size collections because tuples have a smaller memory overhead.</li>
<li>Consider using <strong>slots</strong> for classes if you're creating many instances of a class. This can significantly reduce the memory overhead by preventing the creation of a <strong>dict</strong> for each instance, at the cost of flexibility.</li>
<li>Be mindful of container size and cleanup. Large collections can consume a lot of memory. Removing references to unneeded objects or using data structures with smaller overheads can help manage memory usage.</li>
<li>Consider using specialized libraries or data structures for large datasets, such as arrays from the array module for homogeneous data or third-party libraries like NumPy, which can be more memory-efficient for certain tasks.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-lists"><a class="header" href="#-lists">üìù Lists</a></h1>
<p>Lists are ordered collections of items (elements) that can be of different types. They are mutable, meaning that their elements can be changed, added, or removed.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Accessing an element by index is O(1).</li>
<li>Adding/removing elements at the end is O(1), but inserting/removing elements elsewhere can be O(n) because it may require shifting elements.</li>
<li>Searching for an element is O(n) because it requires a linear search.</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Lists are dynamic arrays and thus have some overhead for memory allocation to support their mutability and variable size. Each item in a list holds a reference to an object (which could be anything), and there's additional memory overhead for maintaining the size of the list and the pointers to each item.</p>
<p>So the memory usage of a list grows with the number of elements. However, because Python preallocates memory in chunks (to avoid frequent resizing), a list might use more memory than the actual data it stores.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_list = [1, 2, 3]
my_list.append(4)  # Add an element
my_list[1] = 20    # Modify an element
del my_list[0]     # Remove an element
print(my_list)     # Output: [20, 3, 4]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-tuples"><a class="header" href="#-tuples">üéÅ Tuples</a></h1>
<p>Tuples are similar to lists but are immutable, meaning that once a tuple is created, it cannot be modified. This makes tuples a good choice for representing fixed collections of items.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Accessing an element by index is O(1).</li>
<li>Since tuples are immutable, operations like adding or removing elements are not applicable.</li>
<li>Searching for an element is O(n).</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Tuples are immutable and hence can be optimized by Python's runtime. Since they cannot change in size, Python knows exactly how much memory to allocate at creation time.</p>
<p>Therefore, generally, tuples are more memory-efficient than lists with the same elements because of their immutability and the absence of overhead associated with variable size. However, like lists, each element is a reference to another object, so the overall memory usage depends on what is stored in the tuple.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_tuple = (1, 2, 3)
print(my_tuple[1])  # Access an element, Output: 2
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-dictionaries"><a class="header" href="#-dictionaries">üóùÔ∏è Dictionaries</a></h1>
<p>Dictionaries are unordered collections of key-value pairs. They allow for fast data lookup by key and are mutable.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Access, insertion, and deletion operations are O(1) on average because dictionaries are implemented using hash tables.</li>
<li>However, in the worst-case scenario (e.g., many key collisions), these operations can degrade to O(n).</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Dictionaries in Python are implemented using hash tables. This means they use a sparse array to provide fast access paths to values based on unique keys. Each entry in the hash table holds the key, the value, and a hash of the key for fast comparison.</p>
<p>This overhead allows for fast access but means that, byte for byte, a dictionary will use more memory than a list or tuple storing the same data. The memory usage becomes more efficient as the dictionary grows larger, but sparse usage (many empty entries) can lead to wasted space.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_dict = {'apple': 5, 'banana': 3}
my_dict['cherry'] = 7  # Add a new key-value pair
my_dict['apple'] = 10  # Update an existing key-value pair
del my_dict['banana']  # Remove a key-value pair
print(my_dict)         # Output: {'apple': 10, 'cherry': 7}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-sets"><a class="header" href="#-sets">üéØ Sets</a></h1>
<p>Sets are unordered collections of unique elements. They are mutable and provide efficient ways to perform common set operations like union, intersection, and difference.</p>
<p><strong>Performance Characteristics:</strong></p>
<ul>
<li>Like dictionaries, set operations such as adding, removing, and checking for membership are O(1) on average.</li>
</ul>
<p><strong>Memory Implications</strong></p>
<p>Sets are conceptually similar to dictionaries with only keys and no values. They are also backed by a hash table, providing fast operations for checking membership, adding, and removing elements. They are memory-efficient for operations involving large numbers of elements but might use more memory than lists or tuples for the same number of elements, due to the hash table mechanism.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">my_set = {1, 2, 3}
my_set.add(4)    # Add an element
my_set.remove(2) # Remove an element
print(my_set)    # Output: {1, 3, 4}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-the-collections-module"><a class="header" href="#-the-collections-module">üì¶ The Collections Module</a></h1>
<p>The collections module in Python provides specialized container datatypes that are alternatives to Python's general-purpose built-in containers like dict, list, set, and tuple. These specialized containers are designed to provide additional functionality and can often lead to more efficient and optimized code, both in terms of speed and memory usage. Here's an overview of some of the most useful collections, when to use them, and examples of their usage:</p>
<h2 id="namedtuple"><a class="header" href="#namedtuple">namedtuple()</a></h2>
<p>Use case: When you need to access elements by name to make the code more readable and self-documenting.</p>
<p>Efficiency: Access is as fast as a tuple because namedtuple instances are just as lightweight as regular tuples.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import namedtuple

Point = namedtuple('Point', ['x', 'y'])
pt = Point(1, 2)
print(pt.x, pt.y)  # Output: 1 2
</code></pre>
<h2 id="deque"><a class="header" href="#deque">deque</a></h2>
<p>Use case: When you need to add or pop elements from both ends of a collection efficiently.</p>
<p>Efficiency: Provides faster appends and pops from the left end but is slower than lists in random access.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import deque

dq = deque([1, 2, 3])
dq.appendleft(0)  # Add to the left
dq.pop()  # Remove from the right
print(dq)  # Output: deque([0, 1, 2])
</code></pre>
<h2 id="counter"><a class="header" href="#counter">Counter</a></h2>
<p>Use case: When you need to count the occurrence of items in an iterable.</p>
<p>Efficiency: Makes counting elements and element-wise operations more efficient than manually using a dictionary.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import Counter

cnt = Counter('abracadabra')
print(cnt)  # Output: Counter({'a': 5, 'b': 2, 'r': 2, 'c': 1, 'd': 1})
</code></pre>
<h2 id="ordereddict"><a class="header" href="#ordereddict">OrderedDict</a></h2>
<p>Use case: Prior to Python 3.7, when you needed to keep the order of keys in a dictionary. From Python 3.7 onwards, the built-in dict maintains insertion order, making OrderedDict less critical.</p>
<p>Efficiency: Useful in earlier Python versions for ordered operations.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import OrderedDict

od = OrderedDict([('a', 1), ('b', 2), ('c', 3)])
od['d'] = 4
print(od)  # Output: OrderedDict([('a', 1), ('b', 2), ('c', 3), ('d', 4)])
</code></pre>
<h2 id="defaultdict"><a class="header" href="#defaultdict">defaultdict</a></h2>
<p>Use case: When you need a dictionary that automatically initializes non-existent keys with a default value.</p>
<p>Efficiency: Simplifies code by eliminating the need for manual checks and assignments for missing keys.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import defaultdict

dd = defaultdict(list)
for i in ['a', 'b', 'a']:
    dd[i].append(1)
print(dd)  # Output: defaultdict(&lt;class 'list'&gt;, {'a': [1, 1], 'b': [1]})
</code></pre>
<h2 id="chainmap"><a class="header" href="#chainmap">ChainMap</a></h2>
<p>Use case: To combine multiple dictionaries or mappings into a single view.</p>
<p>Efficiency: Convenient for scoping contexts like variable scopes in programming languages. Avoids merging dictionaries, which can be costly.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from collections import ChainMap

defaults = {'color': 'red', 'user': 'guest'}
environment = {'user': 'admin', 'path': 'bin'}
cm = ChainMap(environment, defaults)
print(cm['color'])  # Output: red
print(cm['user'])  # Output: admin
</code></pre>
<p>Each of these specialized containers can lead to more elegant, readable, and efficient code by leveraging their unique properties for specific use cases.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-numpy-pandas-and-more"><a class="header" href="#-numpy-pandas-and-more">‚öôÔ∏è NumPy, Pandas, and More</a></h1>
<p>Numpy and Pandas are two widely used libraries in Python for data manipulation and analysis. They are Compared to built-in data structures, they offer more efficient storage and data operations, especially when dealing with large datasets or complex numerical computations. Later in this section, we'll also explore related libraries like Dask and Polars, which further extend these capabilities.</p>
<h2 id="numpy"><a class="header" href="#numpy">Numpy</a></h2>
<p>Python's native list structure has a problem with vector operations and matrix manipulations. Lists don't actually hold the data - instead, they hold pointers to the data location. The advantage of this is that the contents of a list can be heterogenous. However, it introduces a fetch/look-up overhead, and if you are doing lots of operations, this adds up.</p>
<p>What does this look like in hardware terms? In brief, there's a cost in moving data from memory to the CPU. Modern computers can have tiered architectures, with DRAM, SRAM, external caches, and on-chip caches. CPUs can also do things like branch prediction, speculation, overlapped instruction fetching, pipelining, and superscalar execution. If you want to get into the weeds, you can use the Linux perf tool to do some profiling, looking for <code>cache-misses</code> (memory bound) and <code>page-faults</code> (disk/network bound). But we might as well be sensible with how we do things.</p>
<h3 id="the-array-module"><a class="header" href="#the-array-module">The Array Module?</a></h3>
<p>Python offers an array module, that overcomes the memory fragementation issue by storing items sequentially. Iterating through an array therefore doesn't require multiple look-ups, as data can be cached (i.e. closer in terms of spatial and temporal locality to the CPU). But then we run into a different issue: Python, as a high-level interpreted language, isn't optimised for vector operations, and isn't good at dealing with the low-level implementation of array.</p>
<h3 id="enter-numpy"><a class="header" href="#enter-numpy">Enter NumPy</a></h3>
<p>Numpy is a library for numerical computing in Python. It introduces an array object (ndarray), which is a multidimensional container of items of the same type and size. Here's why you might use Numpy arrays over Python lists:</p>
<p><strong>Efficiency</strong>: Numpy arrays are stored at one continuous place in memory unlike lists, so processes can access and manipulate them very efficiently. This is particularly beneficial for operations involving large data sets.</p>
<p><strong>Convenience</strong>: Numpy offers comprehensive mathematical functions that can be applied on arrays without writing loops. This makes code cleaner and faster.</p>
<p><strong>Functionality</strong>: Numpy supports an extensive range of operations including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation, and much more.</p>
<p><strong>Example: Summing Elements</strong></p>
<pre><code class="language-python">import numpy as np

# With Numpy
numpy_array = np.array([1, 2, 3, 4])
print(np.sum(numpy_array))  # Output: 10

# With built-in Python list
python_list = [1, 2, 3, 4]
print(sum(python_list))  # Output: 10
</code></pre>
<p>The Numpy version is much faster and more efficient, especially with larger arrays.</p>
<h2 id="pandas"><a class="header" href="#pandas">Pandas</a></h2>
<p>Pandas is built on top of Numpy and is crucial for data manipulation and analysis. It introduces two new data structures to Python: Series and DataFrame, which are built on Numpy arrays.</p>
<p><strong>Data Representation</strong>: Pandas provides a fast, flexible, and expressive data structure designed to make working with "relational" or "labeled" data both easy and intuitive.</p>
<p><strong>Functionality</strong>: It offers powerful, expressive, and flexible data operations for cleaning, transforming, merging, reshaping, aggregation, and selection of data.</p>
<p><strong>Handling Missing Data</strong>: Pandas is designed to handle missing data using numpy.nan, making it incredibly versatile for data analysis tasks where missing data is a common issue.</p>
<p><strong>Example: Handling CSV Files</strong></p>
<pre><code class="language-python">import pandas as pd

# Using Pandas
data = pd.read_csv('file.csv')
print(data.head())  # Displays the first 5 rows of the file

# The equivalent using built-in Python would require manually parsing the CSV file into lists or dictionaries,
# handling the header separately, and not having the convenient data manipulation functions that Pandas offers.
</code></pre>
<p>Pandas abstracts away much of the complexity of handling tabular data, making it invaluable for data analysis.</p>
<p>While Python's built-in data structures are incredibly powerful and useful for a wide range of programming tasks, Numpy and Pandas offer specialized features that are better suited for numerical computing and data analysis.</p>
<p>Let's explore them further!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-efficient-numerical-computations"><a class="header" href="#-efficient-numerical-computations">‚ö° Efficient Numerical Computations</a></h1>
<p>Here's an overview of how NumPy can be leveraged for efficient numerical computations, along with some code examples.</p>
<h2 id="basic-array-operations"><a class="header" href="#basic-array-operations">Basic Array Operations</a></h2>
<p>NumPy arrays support a wide range of mathematical operations that can be performed efficiently and with concise syntax.</p>
<p><strong>Creating Arrays:</strong></p>
<pre><code class="language-python">import numpy as np

# Create a 1D array
a = np.array([1, 2, 3])

# Create a 2D array (matrix)
b = np.array([[1, 2, 3], [4, 5, 6]])

# Create an array filled with zeros
c = np.zeros((2, 3))

# Create an array filled with ones
d = np.ones((3, 2))

# Create an identity matrix
e = np.eye(3)
</code></pre>
<p><strong>Arithmetic Operations:</strong></p>
<p>Operations are element-wise and can be used to efficiently perform computations across arrays.</p>
<pre><code class="language-python">x = np.array([1, 2, 3])
y = np.array([4, 5, 6])

# Addition
print(x + y)

# Subtraction
print(x - y)

# Multiplication
print(x * y)

# Division
print(x / y)
</code></pre>
<h2 id="advanced-array-operations"><a class="header" href="#advanced-array-operations">Advanced Array Operations</a></h2>
<p>NumPy provides advanced functionalities, including broadcasting, vectorized operations, and complex slicing.</p>
<p><strong>Broadcasting:</strong></p>
<p>Broadcasting allows NumPy to work with arrays of different shapes during arithmetic operations.</p>
<pre><code class="language-python">a = np.array([1, 2, 3])
b = np.array([[0], [10], [20], [30]])

# Broadcasting allows these to be added even though they're different shapes
print(a + b)
</code></pre>
<p><strong>Vectorized Operations:</strong></p>
<p>Vectorized operations enable operations to be performed on arrays without explicit loops, which can significantly improve performance.</p>
<pre><code class="language-python"># Calculate the sine of each element
angles = np.array([0, np.pi/2, np.pi])
print(np.sin(angles))
</code></pre>
<h2 id="linear-algebra"><a class="header" href="#linear-algebra">Linear Algebra</a></h2>
<p>NumPy provides a set of functions for linear algebra operations, making it simple to perform tasks like matrix multiplication, finding determinants, solving linear systems, and more.</p>
<pre><code class="language-python"># Matrix multiplication
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
print(np.dot(A, B))

# Determinant
print(np.linalg.det(A))

# Solve linear system Ax = b
b = np.array([1, 2])
x = np.linalg.solve(A, b)
print(x)
</code></pre>
<h2 id="statistical-functions"><a class="header" href="#statistical-functions">Statistical Functions</a></h2>
<p>NumPy also includes functions for performing statistical operations on arrays, such as finding the mean, median, standard deviation, etc.</p>
<pre><code class="language-python">data = np.array([1, 2, 3, 4, 5])

# Mean
print(np.mean(data))

# Median
print(np.median(data))

# Standard deviation
print(np.std(data))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-using-numexpr"><a class="header" href="#-using-numexpr">üî¢ Using numexpr</a></h1>
<p>Numexpr is a Python library that provides a way to evaluate numerical expressions on arrays at a faster rate than NumPy can, in some cases. It achieves this speedup by using a faster evaluator that utilizes the capabilities of modern CPUs more efficiently, including their vector units and multiple cores. Numexpr compiles the array expressions you give it into optimized, intermediate code that it then executes more efficiently than NumPy's default methods. This can lead to significant performance improvements, especially on operations that involve large arrays.</p>
<h2 id="how-numexpr-works"><a class="header" href="#how-numexpr-works">How Numexpr Works</a></h2>
<p>Numexpr takes a string expression describing a numerical operation and evaluates it much faster than if you were to directly use NumPy for two main reasons:</p>
<ul>
<li><strong>Efficient Use of Memory</strong>: It avoids allocating full temporary arrays that result from intermediate calculations in complex expressions. This means it can handle larger arrays and use less memory, which is particularly useful when working with large datasets that don't fit into your computer's RAM.</li>
<li><strong>Utilization of CPU Features</strong>: Numexpr is designed to make efficient use of CPU features, such as multiple cores and vector operations (SIMD - Single Instruction, Multiple Data). This allows it to perform calculations faster than NumPy, which does not parallelize operations at the level Numexpr does.</li>
</ul>
<p><strong>Why Use Numexpr:</strong></p>
<ul>
<li><strong>Performance</strong>: For complex expressions, especially those involving large arrays, Numexpr can significantly outperform NumPy in terms of speed by utilizing CPU features more efficiently and reducing memory usage.</li>
<li><strong>Handling Large Data</strong>: By minimizing memory consumption through avoiding temporary arrays, Numexpr can work with larger datasets more effectively than NumPy alone.</li>
</ul>
<p><strong>Example Usage</strong></p>
<p>Let's compare a NumPy operation with its Numexpr counterpart to illustrate the usage and performance difference:</p>
<p>NumPy:</p>
<pre><code class="language-python">import numpy as np

a = np.random.rand(1000000)
b = np.random.rand(1000000)

# Using NumPy
c = 2*a + 3*b + 1
</code></pre>
<p>Numexpr:</p>
<pre><code class="language-python">import numexpr as ne
import numpy as np

a = np.random.rand(1000000)
b = np.random.rand(1000000)

# Using Numexpr
c = ne.evaluate('2*a + 3*b + 1')
</code></pre>
<p>In the example above, both snippets achieve the same result, but numexpr can execute the operation faster on large arrays due to its efficient memory use and parallel computation capabilities.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-optimising-data-manipulation"><a class="header" href="#-optimising-data-manipulation">üêº Optimising Data Manipulation</a></h1>
<p>Optimizing data manipulation and analysis with Pandas is crucial for handling large datasets efficiently and effectively. Pandas, being one of the most popular libraries in Python for data analysis, provides numerous ways to optimize performance. Below, I'll highlight some key strategies and provide code examples where relevant.</p>
<h2 id="choosing-the-right-data-types"><a class="header" href="#choosing-the-right-data-types">Choosing the Right Data Types</a></h2>
<p>Pandas automatically assigns data types, but they might not always be the most memory-efficient. By converting columns to more appropriate data types, you can significantly reduce memory usage.</p>
<p><strong>Example: Converting data types</strong></p>
<pre><code class="language-python">import pandas as pd

# Assuming df is your DataFrame
# Convert to 'category' if the number of unique values is small
df['column_name'] = df['column_name'].astype('category')

# Use smaller numeric types
df['int_column'] = df['int_column'].astype('int32')
</code></pre>
<h2 id="using-chunking-for-large-datasets"><a class="header" href="#using-chunking-for-large-datasets">Using Chunking for Large Datasets</a></h2>
<p>When dealing with very large datasets that do not fit into memory, you can read and process the data in chunks.</p>
<p><strong>Example: Reading in chunks</strong></p>
<pre><code class="language-python">chunk_size = 10000  # size of the chunk
for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):
    # process each chunk
    pass
</code></pre>
<h2 id="avoiding-loops-with-vectorization"><a class="header" href="#avoiding-loops-with-vectorization">Avoiding Loops with Vectorization</a></h2>
<p>Loops in Python are slow. Pandas operations are optimized to use vectorized operations that are much faster than iterating through rows.</p>
<p><strong>Example: Vectorized operations</strong></p>
<pre><code class="language-python"># Bad: Looping through DataFrame rows
for index, row in df.iterrows():
    df.at[index, 'new_column'] = row['column'] * 2

# Good: Vectorized operation
df['new_column'] = df['column'] * 2
</code></pre>
<h2 id="using-efficient-functions"><a class="header" href="#using-efficient-functions">Using Efficient Functions</a></h2>
<p>Pandas provides functions that are optimized for performance. For example, apply() is versatile but can be slower than using vectorized operations or specific functions like agg(), transform(), or groupby().</p>
<p><strong>Example: Efficient aggregation</strong></p>
<pre><code class="language-python"># Using `apply()` - Less efficient
df.groupby('group_column').apply(lambda x: x['column'].sum())

# Using `agg()` - More efficient
df.groupby('group_column')['column'].agg('sum')
</code></pre>
<h2 id="filtering-data-early"><a class="header" href="#filtering-data-early">Filtering Data Early</a></h2>
<p>Filter out unnecessary data as early as possible in your pipeline to reduce the size of your DataFrame and speed up subsequent operations.</p>
<p><strong>Example: Early filtering</strong></p>
<pre><code class="language-python"># Filter before performing other operations
df_filtered = df[df['column'] &gt; 100]
# Now work with df_filtered
</code></pre>
<h2 id="using-eval-and-query-for-compound-expressions"><a class="header" href="#using-eval-and-query-for-compound-expressions">Using eval() and query() for Compound Expressions</a></h2>
<p>For complex operations, eval() and query() can be faster because they operate at a lower level and can avoid intermediate data structures.</p>
<p><strong>Example: Using query()</strong></p>
<pre><code class="language-python">result = df.query('column1 &gt; 200 &amp; column2 &lt; 300')
</code></pre>
<h2 id="caching-intermediate-results"><a class="header" href="#caching-intermediate-results">Caching Intermediate Results</a></h2>
<p>If certain results are used multiple times, it might be beneficial to cache them instead of recalculating.</p>
<p><strong>Example: Caching results</strong></p>
<pre><code class="language-python"># Assuming calculating `expensive_operation()` is resource-intensive
df['expensive_column'] = expensive_operation(df['column'])
df['new_column'] = df['expensive_column'] + 100
</code></pre>
<h2 id="utilize-multiindex-for-complex-data-structures"><a class="header" href="#utilize-multiindex-for-complex-data-structures">Utilize MultiIndex for Complex Data Structures</a></h2>
<p>For data with hierarchical structures, using MultiIndex can significantly improve both performance and memory usage. It's particularly useful for grouping and unstacking operations.</p>
<p><strong>Example: Creating and using MultiIndex</strong></p>
<pre><code class="language-python"># Setting a MultiIndex from columns
df.set_index(['level_0', 'level_1'], inplace=True)

# Accessing a slice
df.loc[('index_level_0_value', 'index_level_1_value')]
</code></pre>
<h2 id="optimize-io-operations"><a class="header" href="#optimize-io-operations">Optimize I/O Operations</a></h2>
<p>Reading and writing data efficiently can also be optimized. For example, using the Parquet format can significantly reduce both disk space usage and read/write times compared to CSV or Excel.</p>
<p><strong>Example: Reading and writing Parquet files</strong></p>
<pre><code class="language-python"># Writing to Parquet
df.to_parquet('data.parquet')

# Reading from Parquet
df = pd.read_parquet('data.parquet')
</code></pre>
<h2 id="minimizing-memory-use-with-sparse-data-structures"><a class="header" href="#minimizing-memory-use-with-sparse-data-structures">Minimizing Memory Use with Sparse Data Structures</a></h2>
<p>If your dataset contains a lot of missing values or zeros, converting applicable columns to sparse data types can reduce memory usage.</p>
<p><strong>Example: Using sparse data type</strong></p>
<pre><code class="language-python">df['sparse_column'] = pd.arrays.SparseArray(df['sparse_column'])
</code></pre>
<h2 id="enhancing-performance-with-swifter"><a class="header" href="#enhancing-performance-with-swifter">Enhancing Performance with Swifter</a></h2>
<p>For operations that are difficult to vectorize or when you're stuck with apply() for complex functions, consider using the swifter library. It automatically decides whether to use the original .apply() method, to vectorize the operation if possible, or to use parallel processing to speed up the computation.</p>
<p><strong>Example: Using swifter</strong></p>
<pre><code class="language-python">import swifter

# Apply a function more efficiently
df['result'] = df['column'].swifter.apply(lambda x: complex_function(x))
</code></pre>
<p>Optimizing data manipulation and analysis in Pandas involves a combination of choosing the right data types, leveraging Pandas' powerful vectorized operations, and avoiding common pitfalls like unnecessary looping. By following these strategies, you can make your data processing pipelines more efficient and scalable.</p>
<p>You can also use techniques like parallel processing explicitly - we'll cover these later, as they apply more broadly.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-dask-and-polars"><a class="header" href="#-dask-and-polars">üêª‚Äç‚ùÑÔ∏è Dask and Polars</a></h1>
<p>These are two libraries designed to address the shortcomings of Pandas.</p>
<h2 id="dask"><a class="header" href="#dask">Dask</a></h2>
<p>Dask is a flexible parallel computing library for analytics, enabling you to scale up to clusters or down to your laptop. It's particularly well-suited for working with large datasets that don't fit into memory, as it breaks down complex computations into manageable tasks, which are executed in parallel. Dask provides dynamic task scheduling optimized for computation. It's designed to integrate seamlessly with existing Python libraries like NumPy, Pandas, and Scikit-Learn, allowing you to scale those libraries' functionality across multiple cores or machines.</p>
<p>Here's a simple example that demonstrates how to use Dask Array to perform a computation that is automatically parallelized:</p>
<pre><code class="language-python">import dask.array as da

# Create a large random dask array
x = da.random.random((10000, 10000), chunks=(1000, 1000))

# Compute the mean of the array
mean_result = x.mean().compute()

print(mean_result)
</code></pre>
<h2 id="polars"><a class="header" href="#polars">Polars</a></h2>
<p>Polars is a fast DataFrames library implemented in Rust, designed for high performance and efficiency. It's capable of handling large datasets with ease and speed, focusing on lazy computations for optimal performance. Polars leverages Rust's memory safety and speed, bringing efficient data processing capabilities to Python. It's especially good for tasks involving large datasets that require high-speed manipulation, filtering, and aggregation.</p>
<p>Polars operates with both eager and lazy evaluation. The lazy evaluation allows for more optimized computations by building a computation graph and optimizing it before execution, which can lead to significant performance improvements.</p>
<p>It's largely recreated the Pandas API, for ease-of-transition:</p>
<pre><code class="language-python">import polars as pl

# Read a CSV file into a Polars DataFrame
df = pl.read_csv("your_data.csv")

# Select columns and compute the mean of a column
mean_value = df.select([
    pl.col("your_column_name").mean()
])

print(mean_value)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-non-linear-execution"><a class="header" href="#-non-linear-execution">üòµ‚Äçüí´ Non-Linear Execution</a></h1>
<p>A strictly sequential, or linear, flow of execution in Python (or any programming language) can have several drawbacks in terms of code speed, especially as the complexity and scale of the tasks increase. Here are some of the key issues.</p>
<ul>
<li><strong>Single-Core Utilization</strong>: Modern computers have multiple cores (or CPUs), which allow them to perform several operations in parallel. Sequential code only occupies one core at a time, leaving other cores idle and underutilized. This means that the program is not taking full advantage of the available hardware resources to speed up execution.</li>
<li><strong>I/O Bound Processes</strong>: In a strictly sequential execution, the program waits for I/O operations (like reading from a file or network requests) to complete before proceeding to the next step. This waiting time can significantly slow down the overall execution, as the CPU remains idle during these periods, wasting precious computation time.</li>
<li><strong>Inefficient Handling of Large Data Sets</strong>: When dealing with large data sets, sequential processing can be very time-consuming because each item is processed one after the other. Parallel processing techniques, like those provided by libraries such as multiprocessing or concurrent.futures in Python, can distribute the data and computation across multiple cores, significantly reducing processing time.</li>
<li><strong>Lack of Scalability</strong>: Sequential code is hard to scale. As the amount of data or the complexity of the problem increases, the execution time increases linearly (or sometimes even more than linearly). In contrast, parallel or concurrent execution models can scale more effectively by distributing work across available resources.</li>
</ul>
<p>Python's Global Interpreter Lock, which we'll explore in the next chapter, is often behind this. To mitigate it, developers use parallelism, concurrency, or asynchronous programming models. These approaches allow the program to make better use of system resources, handle I/O more efficiently, and improve overall execution speed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-the-global-interpreter-lock"><a class="header" href="#-the-global-interpreter-lock">üîí The Global Interpreter Lock</a></h1>
<p>The Global Interpreter Lock (GIL) is a mechanism used in computer languages that have memory management, notably in CPython, the standard Python implementation. The GIL is a mutex that protects access to Python objects, preventing multiple threads from executing Python bytecodes at once. This lock is necessary because CPython's memory management is not thread-safe. The GIL ensures that only one thread runs in the interpreter at any given time, which simplifies the implementation of CPython and prevents potential conflicts between threads. But we'll see how this can have performance implications.</p>
<h2 id="implications-for-multi-threaded-applications"><a class="header" href="#implications-for-multi-threaded-applications">Implications for Multi-threaded Applications</a></h2>
<p>The existence of the GIL has significant implications for developers writing multi-threaded applications in Python:</p>
<ol>
<li>Concurrency vs. Parallelism: While the GIL allows for concurrency (multiple threads can be created and managed), it does not allow for true parallelism (multiple threads executing simultaneously) in a single Python process. This means that multi-threaded programs that are CPU-bound may not see a performance improvement; in fact, they might run slower than if they were executed in a single thread due to overhead.</li>
<li>I/O-bound Applications: For I/O-bound applications (waiting for input/output operations to complete), the GIL has less impact. The GIL is released while waiting for I/O, allowing other threads to run. Therefore, Python's threading module can be an excellent choice for I/O-bound applications.</li>
<li>Alternative Approaches: To achieve true parallelism, Python developers often use multiprocessing instead of multithreading. The multiprocessing module creates separate Python processes for each task, each with its own Python interpreter and, by extension, its own GIL. This allows tasks to run in parallel on multiple cores.</li>
</ol>
<h2 id="some-examples-to-try"><a class="header" href="#some-examples-to-try">Some Examples to Try</a></h2>
<p><strong>Example 1: Demonstrating the GIL's Impact on CPU-bound Operations</strong></p>
<p>This example demonstrates how the GIL can limit the performance of CPU-bound multi-threaded programs.</p>
<pre><code class="language-python">import threading
import time

# A simple CPU-bound function
def cpu_bound_task():
    count = 0
    while count &lt; 10000000:
        count += 1

start_time = time.time()
threads = []
for _ in range(2):  # Create 2 threads
    thread = threading.Thread(target=cpu_bound_task)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end_time = time.time()
print(f"Duration with threads: {end_time - start_time} seconds")
</code></pre>
<p><strong>Example 2: I/O-bound Multi-threading</strong></p>
<p>The next example demonstrates how multi-threading can be beneficial for I/O-bound tasks, as the GIL is released during I/O operations, allowing other threads to run.</p>
<pre><code class="language-python">import threading
import time

# A simple I/O-bound function that waits for some time, simulating an I/O operation
def io_bound_task():
    print("Task start")
    time.sleep(2)  # Simulate an I/O operation
    print("Task complete")

start_time = time.time()
threads = []
for _ in range(2):  # Create 2 threads
    thread = threading.Thread(target=io_bound_task)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end_time = time.time()
print(f"Duration with threads: {end_time - start_time} seconds")
</code></pre>
<p>In the CPU-bound example, you might not notice a significant performance improvement from using threads due to the GIL. In contrast, the I/O-bound example can benefit from multi-threading, as threads waiting for I/O allow others to run, potentially improving overall efficiency.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-threading-and-multiprocessing"><a class="header" href="#-threading-and-multiprocessing">üßµ Threading and Multiprocessing</a></h1>
<p>Threading and multiprocessing are two key approaches for achieving parallelism and concurrency in Python, enabling your program to perform multiple operations at once, which can significantly improve performance for I/O-bound and CPU-bound tasks. Here's an overview of both, including when and how to use them, along with some code examples.</p>
<h2 id="threading"><a class="header" href="#threading">Threading</a></h2>
<p>Threading is the practice of running multiple threads (lighter-weight processes) concurrently in a single process. It's particularly useful for I/O-bound tasks, where the program spends a lot of time waiting for external events (e.g., file I/O, network operations).</p>
<p><strong>When to Use:</strong></p>
<ul>
<li>I/O-Bound Processes: When your program spends a significant amount of time waiting for external events.</li>
<li>UI Applications: To keep the UI responsive while performing background tasks.</li>
</ul>
<p><strong>How to Use:</strong></p>
<p>Python provides the threading module to implement threading in your programs. Here's a simple example of using threading to perform a task in the background:</p>
<pre><code class="language-python">import threading
import time

def print_numbers():
    for i in range(5):
        time.sleep(1)
        print(i)

# Create a thread
thread = threading.Thread(target=print_numbers)

# Start the thread
thread.start()

# Wait for the thread to complete
thread.join()

print("Done")
</code></pre>
<h2 id="multiprocessing"><a class="header" href="#multiprocessing">Multiprocessing</a></h2>
<p>Multiprocessing involves running multiple processes concurrently, each with its own Python interpreter and memory space. It's ideal for CPU-bound tasks, where the program needs to perform extensive computations.</p>
<p><strong>When to Use:</strong></p>
<ul>
<li>CPU-Bound Processes: When your program requires heavy computation and can be parallelized to take advantage of multiple CPUs.</li>
<li>Parallel Processing of Data: To perform operations on large datasets concurrently.</li>
</ul>
<p><strong>How to Use:</strong></p>
<p>The multiprocessing module in Python allows you to create and manage separate processes. Here's an example of using multiprocessing to perform a computationally heavy task:</p>
<pre><code class="language-python">from multiprocessing import Process, current_process
import time

def compute_heavy_task(name):
    print(f"Process {name}: starting")
    # Simulate a heavy computation
    time.sleep(2)
    print(f"Process {name}: finishing")

if __name__ == '__main__':
    # Create processes
    processes = [Process(target=compute_heavy_task, args=(f'Process {i}',)) for i in range(5)]

    # Start all processes
    for process in processes:
        process.start()

    # Wait for all processes to complete
    for process in processes:
        process.join()

    print("Done")
</code></pre>
<h2 id="key-differences-and-considerations"><a class="header" href="#key-differences-and-considerations">Key Differences and Considerations</a></h2>
<ul>
<li>Global Interpreter Lock (GIL): Python's GIL means that even if you're using multiple threads, only one thread can execute Python bytecode at a time. This limitation doesn't apply to I/O operations but does limit the effectiveness of threads for CPU-bound tasks.</li>
<li>Memory Usage: Multiprocessing can lead to significant memory overhead, as each process has its own Python interpreter and memory space.</li>
<li>Overhead: Creating and managing processes is heavier than threads. Therefore, multiprocessing is usually not beneficial for light tasks or tasks that are too quick to execute.</li>
</ul>
<p><strong>Choosing Between Threading and Multiprocessing:</strong></p>
<ul>
<li>Use threading for I/O-bound tasks or when needing to maintain a responsive UI.</li>
<li>Use multiprocessing for CPU-bound tasks to leverage multiple CPUs for parallel processing.</li>
</ul>
<p>Both approaches are powerful tools in Python for achieving parallelism and improving program performance, but choosing the right one depends on the nature of your tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-asynchronous-programming"><a class="header" href="#-asynchronous-programming">‚è∞ Asynchronous Programming</a></h1>
<p>AsyncIO is a Python library used for writing concurrent code using the async/await syntax. It's designed to handle asynchronous IO tasks, such as network and file I/O, which can significantly improve the performance of your program by making better use of I/O wait times. Here's an overview covering the key concepts, how the event loop works, and some practical applications:</p>
<h2 id="key-concepts"><a class="header" href="#key-concepts">Key Concepts</a></h2>
<ul>
<li>
<p><strong>Asynchronous Programming</strong>: Asynchronous programming allows tasks to run concurrently, improving the performance of applications that perform a lot of I/O operations. It enables the program to start a task and move on to another one before the first one finishes.</p>
</li>
<li>
<p><strong>Coroutines</strong>: Coroutines are special functions that work asynchronously. They are defined with async def and are used to pause and resume execution using the await keyword.</p>
</li>
<li>
<p><strong>Task</strong>: A Task is used to schedule coroutines concurrently. When a coroutine is wrapped into a Task with functions like asyncio.create_task(), it's scheduled to run on the event loop.</p>
</li>
<li>
<p><strong>Event Loop</strong>: The event loop is the core of every asyncio application. It runs in a loop, executing asynchronous tasks and callbacks. It handles all the switching between tasks, essentially enabling the concurrency in your application.</p>
</li>
</ul>
<h2 id="event-loop-workflow"><a class="header" href="#event-loop-workflow">Event Loop Workflow</a></h2>
<ol>
<li>Start the Event Loop: The event loop is started using asyncio.run() or manually with loop = asyncio.get_event_loop() and loop.run_until_complete().</li>
<li>Schedule Tasks: Tasks are scheduled to run on the event loop. They can be coroutines or other future-like objects.</li>
<li>Execute Tasks: The event loop runs the tasks. While a task waits for an I/O operation (like a network operation), the event loop can switch and execute other tasks.</li>
<li>Close the Event Loop: Once all tasks are completed, the event loop is stopped and closed.</li>
</ol>
<h2 id="practical-applications"><a class="header" href="#practical-applications">Practical Applications</a></h2>
<p>AsyncIO is ideal for I/O-bound and high-level structured network code. Examples include:</p>
<ul>
<li>Web scraping</li>
<li>Web servers and client applications</li>
<li>Database query handling</li>
<li>Network servers and clients</li>
<li>File I/O operations</li>
</ul>
<h2 id="code-examples"><a class="header" href="#code-examples">Code Examples</a></h2>
<p><strong>Basic Coroutine</strong></p>
<pre><code class="language-python">import asyncio

async def hello_world():
    print("Hello")
    await asyncio.sleep(1)  # Simulate I/O operation
    print("World")

asyncio.run(hello_world())
</code></pre>
<p><strong>Running Multiple Coroutines</strong></p>
<pre><code class="language-python">import asyncio

async def fetch_data():
    print("Fetching data...")
    await asyncio.sleep(2)
    print("Data fetched")
    return {'data': 1}

async def print_numbers():
    for i in range(10):
        print(i)
        await asyncio.sleep(0.5)

async def main():
    task1 = asyncio.create_task(fetch_data())
    task2 = asyncio.create_task(print_numbers())

    # Wait for all tasks to complete
    await task1
    await task2

asyncio.run(main())
</code></pre>
<p><strong>Using Event Loop Directly (less common in modern asyncio code)</strong></p>
<pre><code class="language-python">import asyncio

async def say_after(delay, what):
    await asyncio.sleep(delay)
    print(what)

async def main():
    loop = asyncio.get_event_loop()
    task = loop.create_task(say_after(1, 'hello'))
    await task

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
loop.close()
</code></pre>
<p>These examples illustrate the basic usage of AsyncIO for concurrent programming in Python. AsyncIO's power lies in its ability to handle many tasks simultaneously, making it a valuable tool for any I/O-bound application.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-concurrentfutures"><a class="header" href="#-concurrentfutures">ü™¢ concurrent.futures</a></h1>
<p>The concurrent.futures module in Python provides a high-level interface for asynchronously executing callables. It simplifies the execution of tasks in parallel, abstracting away many of the lower-level details of thread or process management. The module offers two main executor classes - ThreadPoolExecutor and ProcessPoolExecutor. The former is used for executing tasks in separate threads, making it ideal for I/O-bound tasks. The latter runs each task in a separate process, circumventing the Global Interpreter Lock (GIL) and is more suitable for CPU-bound tasks.</p>
<h2 id="basic-usage"><a class="header" href="#basic-usage">Basic Usage</a></h2>
<p><strong>Submitting Tasks</strong></p>
<p>Tasks can be submitted for execution using the submit() method, which schedules the callable to be executed and returns a Future object. A Future represents the eventual result of a computation.</p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor

def task(n):
    return n ** 2

# Create a ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=3) as executor:
    future = executor.submit(task, 5)
    print(future.result())  # Prints: 25
</code></pre>
<p><strong>Using map()</strong></p>
<p>The map() function is similar to the built-in map, but it executes the function across multiple threads or processes in parallel. It returns an iterator that yields the results of the function calls.</p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor

def task(n):
    return n ** 2

# Using ThreadPoolExecutor
with ThreadPoolExecutor(max_workers=3) as executor:
    results = executor.map(task, [1, 2, 3, 4, 5])
    for result in results:
        print(result)  # Prints the squares of the numbers
</code></pre>
<h2 id="handling-future-results"><a class="header" href="#handling-future-results">Handling Future Results</a></h2>
<p>The Future object allows you to check if the task has completed (done()), wait for its completion (result()), and even cancel the task (cancel()). It encapsulates the asynchronous execution of a callable and provides methods to check its status and retrieve its result.</p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor
import time

def task():
    time.sleep(1)
    return "Task completed"

with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(task)
    print(future.done())  # False, the task is not completed yet.
    time.sleep(1.5)
    print(future.done())  # True, the task is now completed.
    print(future.result())  # "Task completed"
</code></pre>
<h2 id="exception-handling"><a class="header" href="#exception-handling">Exception Handling</a></h2>
<p>When the callable raises an exception, the Future object catches it. The exception will be re-raised when you call result().</p>
<pre><code class="language-python">from concurrent.futures import ThreadPoolExecutor

def task():
    raise Exception("Task error")

with ThreadPoolExecutor(max_workers=1) as executor:
    future = executor.submit(task)
    try:
        result = future.result()  # This will raise the exception thrown by task()
    except Exception as e:
        print(e)  # Prints: Task error
</code></pre>
<h2 id="choosing-between-threadpoolexecutor-and-processpoolexecutor"><a class="header" href="#choosing-between-threadpoolexecutor-and-processpoolexecutor">Choosing Between ThreadPoolExecutor and ProcessPoolExecutor</a></h2>
<ul>
<li>Use ThreadPoolExecutor for I/O-bound tasks or when executing a large number of small tasks.</li>
<li>Use ProcessPoolExecutor for CPU-bound tasks to take advantage of multiple CPU cores.</li>
</ul>
<p>The concurrent.futures module abstracts the complexity of thread and process management, providing an easy-to-use interface for executing tasks concurrently. This allows you to write more efficient, readable code, especially for I/O-bound or CPU-intensive tasks.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-alternative-python-interpreters"><a class="header" href="#-alternative-python-interpreters">üöÄ Alternative Python Interpreters</a></h1>
<p>The standard Python interpreter (CPython) sometimes faces criticism for performance issues, especially in comparison to compiled languages. This has led to the development of alternative Python interpreters, each aiming to address specific performance bottlenecks or to integrate Python more seamlessly with different environments. Let's check some of them out!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-pypy"><a class="header" href="#-pypy">üèéÔ∏è PyPy</a></h1>
<p>PyPy stands out as the most popular alternative Python interpreter, known for its impressive speed improvements over CPython. PyPy achieves its performance through Just-In-Time (JIT) compilation, which compiles Python code into machine code at runtime. This can lead to significant performance gains, especially in long-running applications. However, if you have short but frequently run scripts, the compilation overhead can be a concern.</p>
<p><strong>Features:</strong></p>
<ul>
<li>JIT compilation for faster execution.</li>
<li>Compatibility with Python 2.7 and 3.10 (at time of writing).</li>
<li>Supports most of the Python standard library and many third-party modules.</li>
<li>Memory Usage: PyPy can use less memory than CPython, thanks to its more efficient garbage collector.</li>
<li>Stackless Python Support: PyPy supports Stackless Python, an enhanced version of Python aimed at concurrency and micro-threads.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>PyPy is best suited for long-running applications where the overhead of JIT compilation can be amortized over time. It's particularly beneficial for applications with heavy numerical computations or extensive use of loops (but you may not need it if using NumPy!).</p>
<p><strong>Code Example:</strong></p>
<p>The usage of PyPy is straightforward because it's a drop-in replacement for CPython. First, download it from <a href="https://www.pypy.org/">https://www.pypy.org/</a></p>
<p>You can run your Python script using PyPy just by using the <code>pypy</code> command instead of <code>python</code>:</p>
<pre><code class="language-shell">pypy script.py
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-ironpython"><a class="header" href="#-ironpython">üîó IronPython</a></h1>
<p>IronPython is an open-source implementation of Python that runs on the .NET Framework and Mono. It is designed to seamlessly integrate with .NET, allowing Python developers to make use of .NET libraries and frameworks. IronPython aims to be a true implementation of Python, while also providing the additional performance and integration capabilities of .NET.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Full integration with the .NET Framework, enabling access to a vast library of .NET functionality.</li>
<li>Allows Python code to interoperate with .NET languages like C# and VB.NET.</li>
<li>Supports dynamic compilation to .NET bytecode, potentially offering performance benefits on the .NET runtime.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>IronPython is ideal for Python developers working in a .NET environment or needing to integrate Python code with .NET applications. It's particularly useful for projects that can benefit from the .NET framework's features, such as Windows-based desktop applications or web services.</p>
<p><strong>Code Example:</strong></p>
<p>Download IronPython first: <a href="https://ironpython.net">https://ironpython.net</a></p>
<p>Running a Python script with IronPython is similar to using the standard Python interpreter, but you use the <code>ipy</code> command instead.</p>
<pre><code class="language-shell">ipy script.py
</code></pre>
<p>To integrate Python code within a C# application using IronPython, you can do something like the following:</p>
<pre><code class="language-csharp">var engine = Python.CreateEngine();
var scope = engine.CreateScope();
engine.ExecuteFile("script.py", scope);
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-jython"><a class="header" href="#-jython">‚òï Jython</a></h1>
<p>Jython is an implementation of Python designed to run on the Java platform. It compiles Python code to Java bytecode, allowing Python programs to seamlessly integrate with Java modules and libraries. This offers a performance boost in environments where the Java Virtual Machine (JVM) is optimized.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Runs on the JVM, allowing integration with Java libraries.</li>
<li>Access to Java's concurrency features and large ecosystem.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>Jython is a great choice when you need to integrate Python code with Java applications or take advantage of Java's rich ecosystem of libraries.</p>
<p><strong>Code Example:</strong></p>
<p>Start here: <a href="https://www.jython.org">https://www.jython.org</a></p>
<p>Using Jython typically involves invoking the Jython interpreter to run Python scripts or to integrate Python with Java code.</p>
<pre><code class="language-shell">jython script.py
</code></pre>
<p>In Java, you can embed Jython as follows:</p>
<pre><code class="language-java">PythonInterpreter interpreter = new PythonInterpreter();
interpreter.exec("print('Hello from Jython')");
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-graalpython"><a class="header" href="#-graalpython">üåå GraalPython</a></h1>
<p>GraalPython is part of the GraalVM ecosystem, offering a high-performance Python 3 interpreter. It's designed to execute Python code efficiently by leveraging the GraalVM's advanced JIT compiler. GraalPython aims to support Python 3.8 language features and a wide range of Python libraries, making it an attractive option for running existing Python code at higher speeds.</p>
<p><strong>Features:</strong></p>
<ul>
<li>High-performance execution through GraalVM's JIT compilation.</li>
<li>Compatibility with Python 3.8 features and a broad set of third-party libraries.</li>
<li>Interoperability with other languages supported by GraalVM, such as JavaScript, Ruby, and R, enabling polyglot applications.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>GraalPython is a good choice for projects that require high performance and are running in environments where GraalVM can be used. It's also beneficial for applications that need to interoperate with other programming languages supported by GraalVM, making it ideal for complex, multi-language systems.</p>
<p><strong>Code Example:</strong></p>
<p>Get started here: <a href="https://github.com/oracle/graalpython">https://github.com/oracle/graalpython</a></p>
<p>To run a Python script using GraalPython, you typically use the <code>graalpython</code> command provided by GraalVM.</p>
<pre><code class="language-shell">graalpython script.py
</code></pre>
<p>For polyglot applications, you can access Python code from Java like this:</p>
<pre><code class="language-java">Context context = Context.newBuilder().allowAllAccess(true).build();
context.eval("python", "print('Hello from GraalPython')");
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-cinder"><a class="header" href="#-cinder">üî• Cinder</a></h1>
<p>Cinder is Meta's (formerly Facebook's) performance-oriented fork of CPython 3.8. It incorporates several enhancements aimed at improving the performance of Python code, notably in highly concurrent server environments. Cinder is not a standalone interpreter but has contributed some of its features back to the main Python branch, showcasing its influence on the Python ecosystem.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Static Python: An experimental, opt-in type system that allows for compiling Python to more efficient code, improving performance.</li>
<li>Strict Modules: Provides a way to declare modules with stricter performance characteristics, allowing Cinder to optimize these modules more aggressively.</li>
<li>Performance Enhancements: Includes various optimizations for faster execution of Python code, particularly in the context of web and server applications.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>Cinder is tailored for large-scale, performance-sensitive Python applications, especially those running in server environments. It's most beneficial for organizations that can invest in managing a custom Python interpreter to gain execution speed.</p>
<p><strong>Code Example:</strong></p>
<p>You can find installation instructions here: <a href="https://github.com/facebookincubator/cinder">https://github.com/facebookincubator/cinder</a></p>
<p>While Cinder's usage is similar to standard Python, benefiting from its features often requires adopting specific patterns or annotations in your code, such as using static types:</p>
<pre><code class="language-python">from static import int64

def fib(n: int64) -&gt; int64:
    if n &lt;= 1:
        return n
    return fib(n-1) + fib(n-2)
</code></pre>
<p>Cinder is used similarly to CPython, but with an emphasis on its performance-enhancing features.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-micropython"><a class="header" href="#-micropython">ü§ñ MicroPython</a></h1>
<p>MicroPython is a lean and efficient implementation of Python 3, designed to run on microcontrollers and in constrained environments. Its goal is to be as compatible with standard Python as possible given the hardware limitations, making Python programming accessible for embedded systems development.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Compact: Requires minimal resources, running in as little as 256KB of code space and 16KB of RAM.</li>
<li>Peripheral Access: Includes libraries to access low-level hardware, such as digital and analog I/O, SPI, I2C, and more.</li>
<li>Interactive Prompt: Offers a Python command line (REPL) on the device for interactive development and debugging.</li>
</ul>
<p><strong>When to Use:</strong></p>
<p>MicroPython shines in embedded systems and IoT applications where resources are limited. It's ideal for developers looking to leverage Python's ease of use and readability in hardware projects, from hobbyist level to professional embedded systems.</p>
<p><strong>Code Example:</strong></p>
<p>Find download instructions here: <a href="https://micropython.org">https://micropython.org</a></p>
<p>A simple MicroPython script to blink an LED might look like this:</p>
<pre><code class="language-python">from machine import Pin
import time

led = Pin(2, Pin.OUT) # Pin 2 is an LED on many boards

while True:
    led.on()
    time.sleep(0.5)
    led.off()
    time.sleep(0.5)
</code></pre>
<p>This script toggles an LED on and off on a board like the ESP8266 or ESP32, showcasing the simplicity of using Python for hardware programming.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-leaving-interpretation"><a class="header" href="#-leaving-interpretation">üèÉ Leaving Interpretation</a></h1>
<p>Moving away from interpretation involves exploring alternative approaches to executing Python code, transcending the traditional interpreter-based execution model. The interpretation layer adds overhead and can slow down your code, especially if lots of the following are happening:</p>
<ul>
<li>mathmatical operations</li>
<li>loops</li>
<li>temporary object creation</li>
</ul>
<p>We've already seen how PyPy can help. But in this section, we'll be more explicit. This involves delving into the integration of C within Python environments and the compilation of Python code using other Ahead-Of-Time (AOT) and Just-In-Time (JIT) compilation methods. These techniques aim to enhance performance, efficiency, and the potential for Python applications by leveraging the strengths of compiled languages and dynamic compilation strategies.</p>
<p>Having said this, compiling Python can add complexity, potentially slowing you down from a development perspective. Refer to the earlier section, <a href="./when_to_optimise.html">When to Optimise</a>. Also, if your code is slow because of other factors, such as lots of I/O, network, disk, database, or external library calls, then compiling might not add much. Also, if you've already used some of the libraries previously discussed (NumPy etc.), then you're already probably tapping into under-the-hood optimisations.</p>
<h2 id="thinking-in-types"><a class="header" href="#thinking-in-types">Thinking in Types</a></h2>
<p>Why is Python "slow"? One reason is that it's dynamically-typed and interpreted. The virtual machine incurs overhead, because it has to be prepared for the datatypes to potentially change. It also has to wrap up low-level types with higher-level functions like hashing and printing. This is why compilation can help for code that has lots of loops making lots of calls.</p>
<p>In order to take advantage of compilation, you need to be more careful with specifying your datatypes (ints, floats, strings), so that you can remove the need for the flexibility of the interpreter.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-c-extensions-for-python"><a class="header" href="#-c-extensions-for-python">üá® C Extensions for Python</a></h1>
<p>You may know that lots of Python libraries, especially in AI/ML, call C under the hood, with Python just serving as a lightweight interface. But how does this work?</p>
<p>Writing C extensions for Python is a powerful technique for optimizing critical performance paths in Python applications. By moving computationally intensive operations from Python to C, you can achieve significant speedups because C code runs much faster than Python code, especially for tasks involving heavy computation or processing large data sets. Here's an overview of how to write C extensions for Python, including setting up your environment, writing the C code, and integrating it with Python.</p>
<p><strong>Setting Up Your Environment:</strong></p>
<p>Before you start, you'll need a C compiler (like GCC on Linux or MSVC on Windows) and the Python headers. The Python headers are usually included with your Python installation, or they can be installed via a package like python-dev on Ubuntu or python-devel on Fedora.</p>
<h2 id="writing-the-c-extension"><a class="header" href="#writing-the-c-extension">Writing the C Extension</a></h2>
<p>A Python C extension module is just like any other Python module, except that it's written in C. It needs to implement a set of initialization functions that the Python interpreter calls when the module is imported.</p>
<p>Here‚Äôs a simple example of a C extension that defines a function fast_add, which adds two numbers with type checking for performance.</p>
<pre><code class="language-c">#include &lt;Python.h&gt;

/* The C function to add two numbers */
static PyObject* fast_add(PyObject* self, PyObject* args) {
    long a, b;
    if (!PyArg_ParseTuple(args, "ll", &amp;a, &amp;b)) {
        return NULL; // If the input isn't two longs, return NULL
    }
    return PyLong_FromLong(a + b);
}

/* Method definition object */
static PyMethodDef FastAddMethods[] = {
    {"fast_add", fast_add, METH_VARARGS, "Add two numbers quickly."},
    {NULL, NULL, 0, NULL} // Sentinel
};

/* Module definition */
static struct PyModuleDef fastaddmodule = {
    PyModuleDef_HEAD_INIT,
    "fastadd", // name of module
    "A module that adds two numbers quickly.", // module documentation
    -1,       // size of per-interpreter state of the module, or -1 if the module keeps state in global variables.
    FastAddMethods
};

/* Module initialization function */
PyMODINIT_FUNC PyInit_fastadd(void) {
    return PyModule_Create(&amp;fastaddmodule);
}
</code></pre>
<h2 id="compiling-the-extension"><a class="header" href="#compiling-the-extension">Compiling the Extension</a></h2>
<p>To compile the extension, you'll need to create a setup.py file for your module. This file uses setuptools to compile your C code into a Python extension module.</p>
<pre><code class="language-python">from setuptools import setup, Extension

module1 = Extension('fastadd',
                    sources = ['fastaddmodule.c'])

setup(name = 'FastAddPackage',
      version = '1.0',
      description = 'This is a demo package for adding two numbers.',
      ext_modules = [module1])
</code></pre>
<p>After creating the setup.py file, you can build the module by running the following command in your terminal:</p>
<pre><code class="language-shell">python setup.py build_ext --inplace
</code></pre>
<h2 id="using-the-extension-in-python"><a class="header" href="#using-the-extension-in-python">Using the Extension in Python</a></h2>
<p>Once the module is compiled, you can import and use it in Python like any other module:</p>
<pre><code class="language-python">import fastadd

result = fastadd.fast_add(5, 10)
print(result)  # Output: 15
</code></pre>
<h2 id="key-points-to-remember"><a class="header" href="#key-points-to-remember">Key Points to Remember</a></h2>
<ul>
<li><strong>Error Handling</strong>: Proper error handling in C is crucial. Make sure to check for errors and return NULL in your C functions if any occur to avoid crashing the interpreter.</li>
<li><strong>Reference Counting</strong>: Pay attention to reference counting when working with Python objects in C to prevent memory leaks or crashes.</li>
<li><strong>Compatibility</strong>: Keep in mind Python version compatibility, especially between Python 2 and Python 3, as there are differences in the API.</li>
</ul>
<p>Writing C extensions can greatly improve the performance of your Python programs, but it also introduces complexity and the potential for hard-to-debug errors. Make sure to thoroughly test your C code and handle all possible error conditions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-compiling-with-cython"><a class="header" href="#-compiling-with-cython">üí® Compiling with Cython</a></h1>
<p>Cython is a powerful tool that allows Python code to be compiled to C, offering significant performance enhancements, especially for computational heavy tasks or when integrating with C libraries. It's particularly useful for optimizing bottlenecks in Python code. Here‚Äôs an overview of how you can use Cython to achieve these performance gains:</p>
<h2 id="basic-example"><a class="header" href="#basic-example">Basic Example</a></h2>
<p>First, you need to install Cython. It can usually be installed via pip: <code>pip install cython</code></p>
<p>Let's start with a basic example of a Python function that sums the squares of numbers up to a given value. Here‚Äôs how you might write it in pure Python:</p>
<pre><code class="language-python">def sum_of_squares(n):
    total = 0
    for i in range(n):
        total += i ** 2
    return total
</code></pre>
<p>To optimize this with Cython, you'd create a .pyx file, say example_cy.pyx, and type the variables for C-level performance:</p>
<pre><code class="language-cython">def sum_of_squares(int n):
    cdef int total = 0
    cdef int i
    for i in range(n):
        total += i ** 2
    return total
</code></pre>
<h2 id="compilation-1"><a class="header" href="#compilation-1">Compilation</a></h2>
<p>To compile the Cython code, you need a setup script, setup.py, containing:</p>
<pre><code class="language-python">from distutils.core import setup
from Cython.Build import cythonize

setup(ext_modules = cythonize("example_cy.pyx"))
</code></pre>
<p>Run the compilation process with:</p>
<pre><code class="language-shell">python setup.py build_ext --inplace
</code></pre>
<p>This generates a shared object file (.so or .pyd, depending on the OS) that you can import in Python.</p>
<h2 id="using-cython-in-jupyter-notebooks"><a class="header" href="#using-cython-in-jupyter-notebooks">Using Cython in Jupyter Notebooks</a></h2>
<p>For quick experiments, Cython can be used directly in Jupyter notebooks with the %load_ext Cython magic command. Here‚Äôs how you can use it:</p>
<pre><code class="language-python">%load_ext Cython

%%cython
def sum_of_squares(int n):
    cdef int total = 0
    cdef int i
    for i in range(n):
        total += i ** 2
    return total
</code></pre>
<h2 id="profiling-and-annotating"><a class="header" href="#profiling-and-annotating">Profiling and Annotating</a></h2>
<p>Cython can generate an HTML file showing which lines of code are converted to pure C and which ones invoke the Python C-API (potentially slowing down execution). Compile with the -a option in your setup script or use the %%cython -a magic command in Jupyter notebooks to generate this annotation. Lines highlighted in yellow indicate Python interactions, urging a closer look for potential optimization.</p>
<h2 id="tips-for-optimization"><a class="header" href="#tips-for-optimization">Tips for Optimization</a></h2>
<ul>
<li><strong>Typing Variables</strong>: Use cdef to declare C static types for variables.</li>
<li><strong>Function Definitions</strong>: Use cpdef for functions you want to call both from Python and Cython-compiled C code. It generates both a C-callable function and a Python wrapper.</li>
<li><strong>Avoiding Python Objects</strong>: Whenever possible, operate on C types rather than Python objects to avoid overhead.</li>
<li><strong>Loop Unrolling</strong>: In some cases, manually unrolling loops can lead to performance gains.</li>
</ul>
<p>Cython is a vast topic, and these examples just scratch the surface. For deeper computational tasks, integrating with C libraries, or further optimizations, the Cython documentation is an excellent resource.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-just-in-time-with-numba"><a class="header" href="#-just-in-time-with-numba">‚ö° Just-in-Time with Numba</a></h1>
<p>Just-In-Time (JIT) compilation is a way to enhance the performance of Python, especially with libraries such as NumPy, by dynamically compiling Python bytecode into machine code at runtime. This process is particularly beneficial for numerical computations where execution speed is critical. The JIT compiler translates Python code into a form that can be executed more quickly, bypassing some of the overhead that comes with Python's dynamic nature.</p>
<p>One of the most popular JIT compilers for Python is Numba. Numba allows Python functions to be decorated with a special marker that indicates they should be JIT compiled. When a decorated function is called, Numba compiles it to machine code "just in time" for execution. This compiled code is directly executed by the CPU, leading to significant performance improvements for computational tasks.</p>
<p>Numba is particularly effective for functions that perform operations on NumPy arrays or for loops that can't be easily vectorized.</p>
<h2 id="using-numba-for-jit-compilation"><a class="header" href="#using-numba-for-jit-compilation">Using Numba for JIT Compilation</a></h2>
<p>Here's a basic example comparing the performance of standard Python vs. JIT-compiled Python with Numba for a numerical operation:</p>
<p><strong>Standard Python Function:</strong></p>
<pre><code class="language-python">import numpy as np
from time import time

def sum_array(arr):
    result = 0
    for x in arr:
        result += x
    return result

arr = np.arange(1000000)
start_time = time()
sum_array(arr)
print("Standard Python:", time() - start_time, "seconds")
</code></pre>
<p><strong>JIT-Compiled Function with Numba:</strong></p>
<p>First, install Numba with pip: <code>pip install numba</code></p>
<p>Then, modify the function to use Numba's JIT decorator:</p>
<pre><code class="language-python">from numba import jit
import numpy as np
from time import time

@jit(nopython=True)
def sum_array_numba(arr):
    result = 0
    for x in arr:
        result += x
    return result

arr = np.arange(1000000)
start_time = time()
sum_array_numba(arr)
print("Numba JIT:", time() - start_time, "seconds")
</code></pre>
<p>In this example, @jit(nopython=True) is used to ensure that the function runs in "nopython mode," which avoids the use of Python objects and functions, leading to faster execution. The performance gain with JIT compilation can be significant, especially for larger datasets or more complex numerical computations.</p>
<h2 id="advantages-of-jit-compilation"><a class="header" href="#advantages-of-jit-compilation">Advantages of JIT Compilation</a></h2>
<ul>
<li><strong>Performance</strong>: JIT compilation can significantly speed up the execution time of Python code, especially for numerical and array operations.</li>
<li><strong>Ease of Use</strong>: With libraries like Numba, adding JIT compilation to Python code can be as simple as adding a decorator to functions.</li>
<li><strong>Flexibility</strong>: JIT compilation can be selectively applied to performance-critical parts of the code, allowing a mix of rapid development and high execution speed.</li>
<li><strong>Cross-platform:</strong> Numba works on Windows, macOS, and Linux, and can generate code for x86, x86_64, and ARM CPUs.</li>
</ul>
<h2 id="considerations"><a class="header" href="#considerations">Considerations:</a></h2>
<ul>
<li><strong>Initial Overhead</strong>: JIT compilation introduces a one-time overhead for compiling the code, which might not be beneficial for functions that are called only once or for very small datasets.</li>
<li><strong>Compatibility</strong>: Numba does not support all Python features and libraries, so some code might need to be adapted to be compatible with Numba's requirements.</li>
</ul>
<p>In summary, JIT compilation with tools like Numba offers a practical way to enhance the performance of Python, particularly for numerical and scientific computing tasks. By compiling critical sections of the code to machine code, it achieves execution speeds that can rival or exceed those of code written in more traditionally performant languages like C or Fortran, without sacrificing the ease of use and flexibility of Python.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-foreign-function-interfaces"><a class="header" href="#-foreign-function-interfaces">üåâ Foreign Function Interfaces</a></h1>
<p>Foreign Function Interfaces (FFIs) in Python allow Python code to call C libraries directly. This capability is essential for situations where Python developers need to access and use legacy C code, optimize performance-critical sections of an application, or use hardware-accelerated or system-level functionality not directly available in Python. The most common tools for working with FFIs in Python are <code>ctypes</code> and <code>cffi</code>.</p>
<h2 id="ctypes"><a class="header" href="#ctypes">ctypes</a></h2>
<p><code>ctypes</code> is a foreign function library for Python that provides C compatible data types and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.</p>
<p>Here's a simple example that uses <code>ctypes</code> to call the time function from the C standard library, which returns the current time in seconds since the Epoch (1970-01-01 00:00:00 +0000 (UTC)).</p>
<pre><code class="language-python">import ctypes
# Load the C standard library
libc = ctypes.CDLL(None)
# Define the return type of the function we are going to call
libc.time.argtypes = [ctypes.POINTER(ctypes.c_long)]
libc.time.restype = ctypes.c_long

t = libc.time(None)
print(f"The current time is {t} seconds since the Epoch.")
</code></pre>
<h2 id="cffi"><a class="header" href="#cffi">cffi</a></h2>
<p><code>cffi</code> (C Foreign Function Interface) is another library for calling C code from Python. Compared to ctypes, cffi provides more advanced features like out-of-line API mode, which allows for better error checking and integration with existing C code.</p>
<p>Below is an example of using <code>cffi</code> to achieve the same functionality as the <code>ctypes</code> example, calling the time function from the C library.</p>
<p>First, you need to install cffi: <code>pip install cffi</code></p>
<p>Then, you can use <code>cffi</code> like this:</p>
<pre><code class="language-python">from cffi import FFI
ffi = FFI()
# Define the external C function
ffi.cdef("long time(long *t);")
# Load the C standard library
C = ffi.dlopen(None)
t = ffi.new("long *")
print(f"The current time is {C.time(t)} seconds since the Epoch.")
</code></pre>
<h2 id="when-to-use-which"><a class="header" href="#when-to-use-which">When to Use Which</a></h2>
<p><code>ctypes</code> is part of the standard Python library, so it doesn't require any additional installations. It's straightforward for simple use cases but can become cumbersome for complex C libraries or where callback functions are involved.</p>
<p><code>cffi</code> requires installation but offers a more flexible and powerful interface for working with C code. It supports both ABI (Application Binary Interface) level and API (Application Programming Interface) level interfaces, making it suitable for more complex integration scenarios.</p>
<p>Both <code>ctypes</code> and <code>cffi</code> are powerful tools for integrating C libraries with Python, each with its own strengths. The choice between them depends on the specific requirements of the project, such as the complexity of the C code being interfaced and the performance requirements.</p>
<h2 id="what-about-fortran"><a class="header" href="#what-about-fortran">What About Fortran?</a></h2>
<p>If you have some legacy scientific code, the most common way to bridge Python and Fortran is through the use of <code>f2py</code> and <code>numpy</code>'s Fortran integration facilities.</p>
<p><code>f2py</code> is one of the easiest and most efficient ways to call Fortran code from Python, especially for numerical computations. f2py generates Python wrapper modules automatically, allowing Fortran routines to be called as if they were Python functions.</p>
<p>Suppose you have a simple Fortran subroutine that calculates the sum of two arrays:</p>
<pre><code class="language-fortran">subroutine add_arrays(a, b, c, n)
    integer, intent(in) :: n
    double precision, intent(in) :: a(n), b(n)
    double precision, intent(out) :: c(n)
    integer :: i
    do i = 1, n
        c(i) = a(i) + b(i)
    end do
end subroutine add_arrays
</code></pre>
<p>You can compile this Fortran code into a Python module using f2py:</p>
<pre><code class="language-shell">f2py -c -m addarrays addarrays.f90
</code></pre>
<p>This command creates a Python module named addarrays. You can then import this module in Python and call the add_arrays function:</p>
<pre><code class="language-python">import addarrays
import numpy as np

a = np.array([1.0, 2.0, 3.0], dtype=np.float64)
b = np.array([4.0, 5.0, 6.0], dtype=np.float64)
c = addarrays.add_arrays(a, b)

print(c)
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-memory-management"><a class="header" href="#-memory-management">üß© Memory Management</a></h1>
<p>Python's memory management system is designed to be automatic and handle the allocation/deallocation of memory for objects in your programs, so you can focus more on coding than on managing memory. The system uses a combination of reference counting and garbage collection to efficiently manage memory.</p>
<p>In the next chapter, we'll consider some potential issues and solutions. But for now, a quick overview of how it works.</p>
<h2 id="reference-counting"><a class="header" href="#reference-counting">Reference Counting</a></h2>
<p>The primary memory management mechanism in Python is reference counting. Each object in Python has a reference count variable that keeps track of the number of references that point to it. When the reference count drops to zero, meaning no references to the object exist, Python automatically reclaims the memory, freeing up space.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">import sys

# Creating an object
a = []

# Initially, the reference count is 1
print(sys.getrefcount(a) - 1)  # subtract 1 to exclude the reference count by getrefcount() itself

# Creating another reference, b, pointing to the same list
b = a

# Now, the reference count is 2
print(sys.getrefcount(a) - 1)

# Deleting one reference
del b

# Reference count back to 1
print(sys.getrefcount(a) - 1)
</code></pre>
<p>In this example, we create a list a, and the reference count increases as we create more references to it (e.g., b = a). When a reference is removed (e.g., del b), the reference count decreases. The sys.getrefcount() function returns the current reference count for the object.</p>
<h2 id="garbage-collection"><a class="header" href="#garbage-collection">Garbage Collection</a></h2>
<p>While reference counting handles most memory management, it can't deal with reference cycles directly (where two or more objects reference each other, creating a loop). To solve this, Python has a garbage collector that can detect these cycles and collect objects that are unreachable.</p>
<p>Python's garbage collection is based on the "generation" concept, where objects are classified into three generations depending on how many collection sweeps they have survived. New objects are placed in the first generation. Objects that survive the garbage collection process are moved into the next generation. Older generations are collected less frequently, making the garbage collection process efficient.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">import gc

class MyClass:
    def __del__(self):
        print("MyClass instance deleted")

# Create a reference cycle
a = MyClass()
b = MyClass()
a.partner = b
b.partner = a

# Delete references
del a
del b

# At this point, the reference cycle exists, and the objects are not deleted
# Force a garbage collection to clean up the cycle
gc.collect()  # This triggers the deletion of both MyClass instances
</code></pre>
<p>In this example, a and b reference each other, creating a cycle. Deleting the variables a and b doesn't actually free the memory because of the cycle. Calling gc.collect() forces the garbage collector to run, detecting and cleaning up the cycle, which leads to the deletion of MyClass instances.</p>
<p>Python‚Äôs memory management uses reference counting for immediate deallocation of objects when they are no longer needed, complemented by a garbage collector to handle reference cycles. This dual approach helps automate memory management, but understanding it can be beneficial for optimizing your Python programs, especially for long-running or memory-intensive applications.</p>
<h2 id="potential-issues"><a class="header" href="#potential-issues">Potential Issues</a></h2>
<p><strong>Overhead of Reference Counting</strong></p>
<ul>
<li>Increment/Decrement Operations: Each time an object is referenced or dereferenced, Python performs increment and decrement operations on the reference count. In performance-critical sections of code, this can introduce a non-negligible overhead.</li>
<li>Delayed Deallocation: Reference counting immediately deallocates memory once an object's reference count drops to zero. While generally efficient, this process can cause unpredictable delays in a program's execution, especially if the object being destroyed has a complex <strong>del</strong>() method.</li>
</ul>
<p><strong>Garbage Collection Pauses</strong></p>
<ul>
<li>Stop-the-world: The garbage collection process can introduce latency, as it may pause the execution of a program to inspect and collect unreachable objects. This is particularly problematic in real-time systems where consistent performance is critical.</li>
<li>Unpredictable Execution Timing: Garbage collection runs at intervals that may not be predictable. This can lead to sporadic performance degradation, especially in long-running applications where accumulated garbage suddenly triggers a collection.</li>
</ul>
<p><strong>Memory Fragmentation</strong></p>
<ul>
<li>Dynamic Allocation: Python's dynamic nature means objects are frequently allocated and deallocated. This can lead to memory fragmentation, where free memory is split into small, non-contiguous blocks, making it difficult to allocate large objects and potentially leading to inefficient use of memory.</li>
</ul>
<p><strong>Reference Cycles</strong></p>
<ul>
<li>Leaked Memory: While Python's garbage collector can detect objects in a reference cycle that are no longer reachable, the presence of reference cycles can still lead to memory leaks if the collector doesn't run frequently or if objects modify their <strong>del</strong> methods in ways that prevent garbage collection.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-reducing-footprint-and-avoiding-leaks"><a class="header" href="#-reducing-footprint-and-avoiding-leaks">‚ôªÔ∏è Reducing Footprint and Avoiding Leaks</a></h1>
<p>Reducing the memory footprint and avoiding memory leaks in Python are crucial for optimizing the performance and scalability of applications, especially when dealing with large datasets or running on limited-resource environments. Python, being a high-level language, abstracts away many memory management details, but there are still best practices and techniques you can apply to manage memory usage effectively.</p>
<h2 id="efficient-data-types"><a class="header" href="#efficient-data-types">Efficient Data Types</a></h2>
<p>Choosing the right data types can significantly reduce memory usage. For instance, using array.array for large arrays of numbers instead of lists, or <strong>slots</strong> to limit the attributes in a class.</p>
<p><strong>Example with array.array:</strong></p>
<pre><code class="language-python">import array

# For a large array of integers
int_array = array.array('i', range(1000000))
</code></pre>
<p><strong>Example with <strong>slots</strong>:</strong></p>
<pre><code class="language-python">class SlotsBased:
    __slots__ = ['name', 'age']

    def __init__(self, name, age):
        self.name = name
        self.age = age

# Compared to a regular class without __slots__
class RegularClass:
    def __init__(self, name, age):
        self.name = name
        self.age = age
</code></pre>
<h2 id="using-generators"><a class="header" href="#using-generators">Using Generators</a></h2>
<p>Generators yield items one at a time, consuming less memory when processing large datasets.</p>
<p><strong>Example of using a generator:</strong></p>
<pre><code class="language-python">def large_file_reader(file_name):
    for row in open(file_name, "r"):
        yield row

# Use the generator
for row in large_file_reader("large_file.txt"):
    process(row)
</code></pre>
<h2 id="explicitly-releasing-resources"><a class="header" href="#explicitly-releasing-resources">Explicitly Releasing Resources</a></h2>
<p>Garbage collection in Python usually handles memory management, but in some cases, you might need to manually release resources. This is especially true for external resources like file handles or network connections.</p>
<p><strong>Example of using context managers to ensure resources are released:</strong></p>
<pre><code class="language-python">with open("file.txt", "r") as file:
    data = file.read()
# File is automatically closed after the block, releasing its resources.
</code></pre>
<h2 id="weak-references"><a class="header" href="#weak-references">Weak References</a></h2>
<p>Weak references allow the Python garbage collector to collect an object even if it has references, as long as they are weak. This is useful for caching or mapping large objects that you don't want to keep in memory indefinitely.</p>
<p><strong>Example with weakref:</strong></p>
<pre><code class="language-python">import weakref

class BigObject:
    pass

obj = BigObject()
obj_weakref = weakref.ref(obj)

# obj can be garbage collected even if obj_weakref exists
</code></pre>
<h2 id="monitoring-and-profiling-memory-usage"><a class="header" href="#monitoring-and-profiling-memory-usage">Monitoring and Profiling Memory Usage</a></h2>
<p>Identifying and diagnosing memory issues requires monitoring and profiling. Tools like memory_profiler and objgraph can help identify memory leaks and high memory usage.</p>
<p><strong>Example with memory_profiler:</strong></p>
<pre><code class="language-python"># You need to install memory_profiler first (`pip install memory_profiler`)
from memory_profiler import profile

@profile
def my_function():
    a = [1] * (10 ** 6)
    b = [2] * (2 * 10 ** 7)
    del b
    return a
</code></pre>
<h2 id="garbage-collection-tuning"><a class="header" href="#garbage-collection-tuning">Garbage Collection Tuning</a></h2>
<p>Python‚Äôs garbage collector can be tuned or manually invoked to manage memory more aggressively. However, be cautious as forcing garbage collection can impact performance.</p>
<p><strong>Example of invoking garbage collection:</strong></p>
<pre><code class="language-python">import gc

gc.collect()  # Force a garbage collection
</code></pre>
<h2 id="avoiding-circular-references"><a class="header" href="#avoiding-circular-references">Avoiding Circular References</a></h2>
<p>Circular references can prevent Python's garbage collector from reclaiming memory. Use weak references or redesign your data structures to avoid them.</p>
<p><strong>Detecting circular references:</strong></p>
<pre><code class="language-python">import gc

gc.collect()  # Collect all objects if possible
for obj in gc.garbage:
    print(obj)
</code></pre>
<p>These techniques, combined with a thoughtful design, can help manage memory effectively in Python applications, improving performance and reducing the risk of memory-related issues.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-caching-strategies"><a class="header" href="#-caching-strategies">üíæ Caching Strategies</a></h1>
<p>Caching in Python is a technique used to store data in a temporary storage area (cache) so that future requests for that data can be served faster. This is particularly useful in situations where data retrieval or computation is resource-intensive. Python provides various strategies and tools for caching, which can be applied depending on the specific requirements of your application.</p>
<h2 id="lru-least-recently-used-cache"><a class="header" href="#lru-least-recently-used-cache">LRU (Least Recently Used) Cache</a></h2>
<p>The LRU caching strategy removes the least recently used items first. This is useful in applications where you want to cache a limited number of items and the likelihood of accessing recently used items is high.</p>
<p>Python's functools module provides an @lru_cache decorator to implement LRU caching easily. It can be applied to any function whose output you want to cache.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from functools import lru_cache

@lru_cache(maxsize=100)
def expensive_function(param):
    # Simulate an expensive operation
    return some_expensive_computation(param)
</code></pre>
<h2 id="ttl-time-to-live-cache"><a class="header" href="#ttl-time-to-live-cache">TTL (Time To Live) Cache</a></h2>
<p>TTL caching invalidates cache entries after a set period. This strategy is suitable when the data changes over time or if you want to ensure that data doesn't become stale.</p>
<p>The cachetools library offers TTL caching capabilities.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from cachetools import TTLCache
from cachetools.decorators import cached

ttl_cache = TTLCache(maxsize=100, ttl=300) # Cache up to 100 items, each for 300 seconds

@cached(ttl_cache)
def get_data(key):
    # Fetch data that changes over time
    return fetch_some_data(key)

## Memoization
</code></pre>
<p>Memoization is a specific form of caching that involves storing the results of expensive function calls and returning the cached result when the same inputs occur again. Memoization is a form of LRU caching but is specifically applied to function calls.</p>
<p>Python's functools module @lru_cache can also be used for memoization.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from functools import lru_cache

@lru_cache(maxsize=None)  # No limit on cache size
def fibonacci(n):
    if n &lt; 2:
        return n
    return fibonacci(n-1) + fibonacci(n-2)
</code></pre>
<h2 id="disk-based-caching"><a class="header" href="#disk-based-caching">Disk-based Caching</a></h2>
<p>When dealing with large amounts of data or needing to persist cache across program restarts, disk-based caching can be used. Libraries such as joblib provide mechanisms for storing cache data on disk.</p>
<p><strong>Example:</strong></p>
<pre><code class="language-python">from joblib import Memory
memory = Memory("./cachedir", verbose=0)

@memory.cache
def expensive_function(param):
    # Operations that take a lot of resources
    return some_expensive_computation(param)
</code></pre>
<h2 id="choosing-the-right-strategy"><a class="header" href="#choosing-the-right-strategy">Choosing the Right Strategy</a></h2>
<ul>
<li><strong>LRU Cache</strong>: Use when working with a fixed-size cache and accessing recently used items is more probable.</li>
<li><strong>TTL Cache</strong>: Suitable for data that changes over time or where freshness is critical.</li>
<li><strong>Memoization</strong>: Best for optimizing expensive, deterministic functions with a limited set of inputs.</li>
<li><strong>Disk-based Caching</strong>: Ideal for very large datasets or when cache persistence is necessary.</li>
</ul>
<p>Implementing caching can significantly improve the performance of Python applications by reducing the need to recompute results or re-fetch data. However, it's important to choose the right caching strategy based on the application's requirements and the nature of the data being cached.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-performant-web-applications"><a class="header" href="#-performant-web-applications">üåê Performant Web Applications</a></h1>
<p>Optimizing Django, Flask, and FastAPI applications for better performance involves a multifaceted approach, including optimizing database queries, caching, asynchronous processing, and more. Here's a concise overview of optimization strategies for each framework, along with relevant code examples where applicable.</p>
<h2 id="django-optimization"><a class="header" href="#django-optimization">Django Optimization</a></h2>
<p><strong>Database Queries Optimization and Indexing:</strong></p>
<ul>
<li>Use select_related() for forward ForeignKey relationships to reduce the number of database queries.</li>
<li>Use prefetch_related() for many-to-many and reverse ForeignKey relationships.</li>
</ul>
<pre><code class="language-python"># Without optimization
books = Book.objects.all()
for book in books:
    print(book.author.name)  # Each iteration hits the database

# With select_related
books = Book.objects.select_related('author').all()
for book in books:
    print(book.author.name)  # No additional database queries
</code></pre>
<ul>
<li>Add indexes to your models for fields that are frequently queried.</li>
</ul>
<pre><code class="language-python">class MyModel(models.Model):
    name = models.CharField(max_length=100, db_index=True)
</code></pre>
<p><strong>Caching:</strong></p>
<ul>
<li>Use Django‚Äôs cache framework to cache views, querysets, or template fragments.</li>
</ul>
<pre><code class="language-python">from django.core.cache import cache

def my_view(request):
    if cache.get('my_key'):
        return cache.get('my_key')
    else:
        result = expensive_query()
        cache.set('my_key', result, 60)  # Cache for 60 seconds
        return result
</code></pre>
<ul>
<li>Use template fragment caching: Cache parts of templates that are expensive to render.</li>
</ul>
<pre><code class="language-python">{% load cache %}
{% cache 600 sidebar %}
    ... expensive calculations here ...
{% endcache %}
</code></pre>
<p><strong>Middleware, Query, and Static File Optimization:</strong></p>
<ul>
<li>Reduce middleware classes that are not necessary for your project.</li>
<li>Use Django Debug Toolbar to identify and optimize slow queries.</li>
<li>Use GZipMiddleware: Compresses responses for browsers that support gzip.</li>
<li>Serve static files efficiently: Use WhiteNoise or a similar tool to serve static files directly from WSGI and apply cache control headers.</li>
</ul>
<h2 id="flask-optimization"><a class="header" href="#flask-optimization">Flask Optimization</a></h2>
<p><strong>Database Queries Optimization:</strong></p>
<ul>
<li>Use SQLAlchemy or another ORM and apply similar strategies as Django for query optimization.</li>
<li>Optimize queries by joining tables only when necessary and filtering queries as much as possible.</li>
</ul>
<pre><code class="language-python">from myapp import db
from myapp.models import User, Post

posts = Post.query.join(User).filter(Post.user_id == User.id).all()
</code></pre>
<p><strong>Caching:</strong></p>
<ul>
<li>Use Flask-Caching to cache views or data.</li>
</ul>
<pre><code class="language-python">from flask_caching import Cache

cache = Cache(config={'CACHE_TYPE': 'simple'})
app = Flask(__name__)
cache.init_app(app)

@app.route('/')
@cache.cached(timeout=50)
def index():
    return expensive_function()
</code></pre>
<p><strong>Asynchronous Views:</strong></p>
<ul>
<li>Use Flask‚Äôs support for async views to handle I/O-bound operations efficiently.</li>
</ul>
<pre><code class="language-python">@app.route('/async')
async def async_view():
    await asyncio.sleep(1)  # Simulate async I/O operation
    return 'This is an async view'
</code></pre>
<p><strong>Profiling and Monitoring</strong></p>
<ul>
<li>Use Flask extensions like Flask-DebugToolbar to monitor performance bottlenecks.</li>
</ul>
<p><strong>Use Efficient WSGI Servers</strong></p>
<ul>
<li>Deploy Flask applications using efficient WSGI servers like Gunicorn or uWSGI instead of the built-in Flask server for production environments.</li>
</ul>
<h2 id="fastapi-optimization"><a class="header" href="#fastapi-optimization">FastAPI Optimization</a></h2>
<p><strong>Asynchronous and Concurrent Handling:</strong></p>
<ul>
<li>FastAPI is built to be asynchronous. Use async and await for I/O-bound operations, including database operations.</li>
</ul>
<pre><code class="language-python">from fastapi import FastAPI
import httpx

app = FastAPI()

@app.get("/async")
async def read_async():
    async with httpx.AsyncClient() as client:
        response = await client.get('https://example.com')
        return response.text
</code></pre>
<p><strong>Database Queries Optimization:</strong></p>
<ul>
<li>Use databases like Tortoise ORM or SQLAlchemy with async support to optimize database interactions.</li>
<li>Implement strategies similar to Django for prefetching and selecting related data asynchronously.</li>
</ul>
<p><strong>Dependency Injection for Caching:</strong></p>
<ul>
<li>Use FastAPI's dependency injection system for efficient caching mechanisms across your app.</li>
<li>Implement background tasks for operations that can be processed asynchronously.</li>
</ul>
<h2 id="general-optimization-tips"><a class="header" href="#general-optimization-tips">General Optimization Tips</a></h2>
<ul>
<li>Profile your application to identify bottlenecks using tools like cProfile for Python.</li>
<li>Serve static files efficiently using a web server like Nginx or a CDN.</li>
<li>Optimize front-end assets (minify CSS/JS, image compression) to reduce load times.</li>
<li>Implement HTTP/2 where possible.</li>
<li>Asynchronous Tasks: Use Celery for background tasks to prevent blocking web requests for operations like sending emails or processing large data.</li>
<li>Database Connections: Use persistent database connections and connection pooling.</li>
</ul>
<p>By employing these optimization techniques, you can significantly improve the performance of Django, Flask, and FastAPI applications, enhancing user experience and resource utilization.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-final-checklist"><a class="header" href="#-final-checklist">‚úÖ Final Checklist</a></h1>
<p>In summary, optimization in Python often revolves around improving time complexity, reducing memory usage, and leveraging Python's built-in features or external libraries that are implemented in C for speed. In the next section, we'll cover a brief checklist. If you only remember these things, you'll probably make good steps towards more performant code.</p>
<p>Like any other language, Python also has its own set of performance pitfalls, often referred to as "anti-patterns". These anti-patterns can significantly degrade the performance of a Python application if not addressed. So we'll finish with an overview of some common performance anti-patterns in Python and suggestions on how to avoid them!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-easy-wins"><a class="header" href="#-easy-wins">üíØ Easy Wins</a></h1>
<h2 id="loop-optimization"><a class="header" href="#loop-optimization">Loop Optimization</a></h2>
<p>Loops, especially nested loops, can significantly affect performance. Avoiding or minimizing the use of nested loops can lead to substantial improvements. Use list comprehensions and generator expressions for more compact and faster code.</p>
<p><strong>Example: Using List Comprehension</strong></p>
<p>Instead of:</p>
<pre><code class="language-python">result = []
for i in range(10):
    result.append(i * i)
</code></pre>
<p>Use:</p>
<pre><code class="language-python">result = [i * i for i in range(10)]
</code></pre>
<h2 id="data-structures"><a class="header" href="#data-structures">Data Structures</a></h2>
<p>Choosing the right data structure can drastically improve performance. Python's built-in data structures like lists, sets, and dictionaries are highly optimized.</p>
<p><strong>Dictionaries and Sets:</strong> Use dictionaries for quick lookups by key and sets for fast membership testing.
<strong>Tuples:</strong> Use tuples instead of lists for immutable sequences to save memory.</p>
<p><strong>Example: Using Set for Membership Testing</strong></p>
<p>Instead of:</p>
<pre><code class="language-python">my_list = [1, 2, 3, 4, 5]
if 3 in my_list:  # O(n) time complexity
    print("Found!")
</code></pre>
<p>Use:</p>
<pre><code class="language-python">my_set = {1, 2, 3, 4, 5}
if 3 in my_set:  # O(1) time complexity
    print("Found!")
</code></pre>
<h2 id="algorithmic-improvements"><a class="header" href="#algorithmic-improvements">Algorithmic Improvements</a></h2>
<p>Improving the algorithm itself is often the most effective way to optimize. This could mean choosing a more efficient sorting algorithm, using dynamic programming to avoid recalculating results, or employing algorithms with better time complexity.</p>
<p><strong>Example: Memoization in Fibonacci Sequence Calculation</strong></p>
<p>Instead of a simple recursive solution, use memoization:</p>
<pre><code class="language-python">def fibonacci(n, memo={}):
    if n in memo:
        return memo[n]
    if n &lt;= 2:
        return 1
    memo[n] = fibonacci(n - 1, memo) + fibonacci(n - 2, memo)
    return memo[n]
</code></pre>
<h2 id="built-in-functions-and-libraries-or-not"><a class="header" href="#built-in-functions-and-libraries-or-not">Built-in Functions and Libraries (Or Not)</a></h2>
<p>Python's standard library and third-party libraries like NumPy and Pandas are written in C, making them much faster than custom, pure Python code for certain operations, especially numerical computations and data processing.</p>
<p><strong>Example: Using NumPy for Array Operations</strong></p>
<p>Instead of:</p>
<pre><code class="language-python">result = []
for i in range(10000):
    result.append(i * 2)
</code></pre>
<p>Use NumPy:</p>
<pre><code class="language-python">import numpy as np
arr = np.arange(10000) * 2
</code></pre>
<h2 id="profiling-and-identifying-bottlenecks"><a class="header" href="#profiling-and-identifying-bottlenecks">Profiling and Identifying Bottlenecks</a></h2>
<p>Before optimizing, it's crucial to identify where the bottlenecks are. Python provides profiling tools like cProfile to analyze the performance of your code.</p>
<pre><code class="language-python">import cProfile
def my_function():
    result = [i * i for i in range(10000)]
    return result

cProfile.run('my_function()')
</code></pre>
<h2 id="avoiding-global-variables"><a class="header" href="#avoiding-global-variables">Avoiding Global Variables</a></h2>
<p>Access to global variables is slower than local variable access. If you're using global variables within a loop or a frequently called function, consider passing them as arguments or using them as local variables.</p>
<h2 id="string-concatenation"><a class="header" href="#string-concatenation">String Concatenation</a></h2>
<p>Strings in Python are immutable, so every time you concatenate strings, a new string is created. For efficient string concatenation, especially in loops, use .join() or string formatting.</p>
<p><strong>Example: Using .join() for Efficient String Concatenation</strong></p>
<p>Instead of:</p>
<pre><code class="language-python">result = ""
for i in range(100):
    result += str(i)
</code></pre>
<p>Use:</p>
<pre><code class="language-python">result = "".join(str(i) for i in range(100))
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="-easy-losses"><a class="header" href="#-easy-losses">üßπ Easy Losses</a></h1>
<h2 id="using-lists-where-other-data-structures-are-more-efficient"><a class="header" href="#using-lists-where-other-data-structures-are-more-efficient">Using Lists Where Other Data Structures Are More Efficient</a></h2>
<p>Anti-pattern: Using lists for operations that require frequent lookups, insertions, and deletions.</p>
<pre><code class="language-python"># Inefficient for frequent lookups
my_list = [1, 2, 3, 4, 5]
if 4 in my_list:
    print("Found!")
</code></pre>
<p>Solution: Use set or dict for frequent lookups and insertions.</p>
<pre><code class="language-python">my_set = {1, 2, 3, 4, 5}
if 4 in my_set:  # Much faster lookup
    print("Found!")
</code></pre>
<h2 id="not-using-list-comprehensions-or-generator-expressions"><a class="header" href="#not-using-list-comprehensions-or-generator-expressions">Not Using List Comprehensions or Generator Expressions</a></h2>
<p>Anti-pattern: Using loops to generate lists or to iterate over collections when a list comprehension or generator expression would be more efficient and concise.</p>
<pre><code class="language-python">result = []
for i in range(100):
    if i % 2 == 0:
        result.append(i*i)
</code></pre>
<p>Solution: Use list comprehensions for more concise and faster code.</p>
<pre><code class="language-python">result = [i*i for i in range(100) if i % 2 == 0]
</code></pre>
<p>For large datasets or when the result list is not needed all at once, use generator expressions to save memory.</p>
<pre><code class="language-python">result = (i*i for i in range(100) if i % 2 == 0)
</code></pre>
<h2 id="misusing-the-global-interpreter-lock-gil"><a class="header" href="#misusing-the-global-interpreter-lock-gil">Misusing the Global Interpreter Lock (GIL)</a></h2>
<p>Anti-pattern: Relying solely on threads for concurrency in CPU-bound tasks, which can be inefficient due to the Global Interpreter Lock (GIL) in CPython.</p>
<pre><code class="language-python">from threading import Thread

# This might not speed up due to GIL in CPU-bound tasks
def compute_heavy():
    # Some heavy computation
    pass

threads = [Thread(target=compute_heavy) for _ in range(4)]
for thread in threads:
    thread.start()
for thread in threads:
    thread.join()
</code></pre>
<p>Solution: Use multiprocessing or libraries like concurrent.futures.ProcessPoolExecutor for CPU-bound tasks.</p>
<pre><code class="language-python">from multiprocessing import Pool

def compute_heavy():
    # Some heavy computation
    pass

with Pool(4) as p:
    p.map(compute_heavy, [1, 2, 3, 4])
</code></pre>
<h2 id="not-utilizing-built-in-functions-and-libraries"><a class="header" href="#not-utilizing-built-in-functions-and-libraries">Not Utilizing Built-in Functions and Libraries</a></h2>
<p>Anti-pattern: Reimplementing functionality that is already provided by Python's built-in functions or standard libraries.</p>
<pre><code class="language-python"># Custom implementation of a feature that exists in standard library
def manual_sort(my_list):
    return sorted(my_list)  # Inefficient and reinventing the wheel

# Correct approach
my_list = [3, 1, 4, 1, 5]
print(sorted(my_list))
</code></pre>
<p>Solution: Always check the Python standard library and built-in functions before implementing common algorithms and data structures.</p>
<h2 id="deeply-nested-functions"><a class="header" href="#deeply-nested-functions">Deeply Nested Functions</a></h2>
<p>Anti-pattern: Writing deeply nested functions or loops, which can make code hard to read, debug, and optimize.</p>
<pre><code class="language-python">def deeply_nested(data):
    for i in data:
        for j in i:
            # ... more nested loops or conditions
            pass
</code></pre>
<p>Solution: Refactor deeply nested loops into separate functions or use more efficient data structures or algorithms to simplify the logic.</p>
<p>By being aware of these common anti-patterns and applying the suggested solutions, you can significantly improve the performance and maintainability of your Python code.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
